{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51fad5c-ddc0-414b-8ec6-016b75b0f970",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import nltk\n",
    "import torch\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from transformers import EarlyStoppingCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Load evaluation metrics\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "\n",
    "# Model and tokenizer\n",
    "model_name = \"google/flan-t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Dataset configurations\n",
    "dataset_configs = [\n",
    "    {\"name\": \"Zero-Shot\", \"path\": \"Zero-Shot_qa_dataset.json\"},\n",
    "    {\"name\": \"One-Shot\", \"path\": \"One-Shot_qa_dataset.json\"},\n",
    "    {\"name\": \"Few-Shot\", \"path\": \"Few-Shot_qa_dataset.json\"},\n",
    "]\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(example):\n",
    "    model_input = tokenizer(\n",
    "        example[\"input\"], max_length=256, padding=\"max_length\", truncation=True\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        example[\"output\"], max_length=128, padding=\"max_length\", truncation=True\n",
    "    )\n",
    "    model_input[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_input\n",
    "\n",
    "# Compute metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.clip(predictions, 0, tokenizer.vocab_size - 1)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_preds = [p.strip() for p in decoded_preds]\n",
    "    decoded_labels = [l.strip() for l in decoded_labels]\n",
    "\n",
    "    rouge_scores = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    meteor_score = meteor.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    bleu_score = bleu.compute(predictions=decoded_preds, references=[[l] for l in decoded_labels])\n",
    "\n",
    "    return {\n",
    "        \"rouge1\": rouge_scores[\"rouge1\"],\n",
    "        \"rouge2\": rouge_scores[\"rouge2\"],\n",
    "        \"rougeL\": rouge_scores[\"rougeL\"],\n",
    "        \"meteor\": meteor_score[\"meteor\"],\n",
    "        \"bleu\": bleu_score[\"bleu\"]\n",
    "    }\n",
    "\n",
    "# Compute category-wise metrics\n",
    "def compute_category_metrics(predictions, references, categories):\n",
    "    category_data = defaultdict(lambda: {'preds': [], 'refs': []})\n",
    "    for pred, ref, cat in zip(predictions, references, categories):\n",
    "        category_data[cat]['preds'].append(pred)\n",
    "        category_data[cat]['refs'].append(ref)\n",
    "    category_results = {}\n",
    "    for cat, data in category_data.items():\n",
    "        try:\n",
    "            rouge_scores = rouge.compute(predictions=data['preds'], references=data['refs'])\n",
    "            meteor_score = meteor.compute(predictions=data['preds'], references=data['refs'])\n",
    "            bleu_score = bleu.compute(predictions=data['preds'], references=[[r] for r in data['refs']])\n",
    "            category_results[cat] = {\n",
    "                'count': len(data['preds']),\n",
    "                'rouge1': rouge_scores[\"rouge1\"],\n",
    "                'rouge2': rouge_scores[\"rouge2\"],\n",
    "                'rougeL': rouge_scores[\"rougeL\"],\n",
    "                'meteor': meteor_score[\"meteor\"],\n",
    "                'bleu': bleu_score[\"bleu\"]\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error in category {cat}: {e}\")\n",
    "    return category_results\n",
    "\n",
    "# Full training loop\n",
    "def run_finetuning(dataset_name, dataset_path):\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"\\U0001F680 Processing {dataset_name}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    examples = []\n",
    "    for item in data[\"queries\"]:\n",
    "        question = item[\"question\"]\n",
    "        answer = item[\"answer\"]\n",
    "        context = item.get(\"source_context\", \"\")\n",
    "        question_type = item.get(\"question_type\", \"unknown\")\n",
    "        triples = item.get(\"ground_truth\", {}).get(\"source_triples\", [])\n",
    "        kg_text = \"; \".join([\n",
    "            f\"{t['subject']} - {t['predicate']} - {t['object']}\" for t in triples\n",
    "        ]) if triples else \"\"\n",
    "        input_text = (\n",
    "            f\"Answer the following {question_type} question:\\n\"\n",
    "            f\"Question: {question}\\n\"\n",
    "            f\"Context: {context}\\n\"\n",
    "            f\"Knowledge: {kg_text}\"\n",
    "        )\n",
    "        examples.append({\n",
    "            \"input\": input_text,\n",
    "            \"output\": answer,\n",
    "            \"question\": question,\n",
    "            \"category\": question_type.lower()\n",
    "        })\n",
    "\n",
    "    train_data, test_data = train_test_split(examples, test_size=0.3, random_state=42)\n",
    "    val_data, test_data = train_test_split(test_data, test_size=1/3, random_state=42)\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": Dataset.from_list(train_data),\n",
    "        \"validation\": Dataset.from_list(val_data),\n",
    "        \"test\": Dataset.from_list(test_data),\n",
    "    })\n",
    "\n",
    "    tokenized_dataset = dataset.map(preprocess, batched=False)\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=f\"./flant5-large-finetuned-{dataset_name.lower()}\",\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=25,\n",
    "        learning_rate=3e-4,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=20,\n",
    "        save_steps=200,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=200,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"rouge1\",\n",
    "        greater_is_better=True,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=128,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    print(f\"\\nEvaluating {dataset_name} test set...\")\n",
    "    test_results = trainer.evaluate(tokenized_dataset[\"test\"])\n",
    "    for key, value in test_results.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key.upper()}: {value:.4f}\")\n",
    "\n",
    "    print(f\"\\nCategory-wise Results for {dataset_name}...\")\n",
    "    model.eval()\n",
    "    device = model.device\n",
    "    all_predictions, all_references, all_categories = [], [], []\n",
    "\n",
    "    for example in tqdm(test_data):\n",
    "        inputs = tokenizer(\n",
    "            example[\"input\"], return_tensors=\"pt\", truncation=True, padding=True, max_length=256\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        outputs = model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True)\n",
    "        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        all_predictions.append(prediction.strip())\n",
    "        all_references.append(example[\"output\"].strip())\n",
    "        all_categories.append(example[\"category\"])\n",
    "\n",
    "    category_metrics = compute_category_metrics(all_predictions, all_references, all_categories)\n",
    "    df = pd.DataFrame.from_dict(category_metrics, orient=\"index\")\n",
    "    df.to_csv(f\"category_metrics_{dataset_name.lower()}.csv\")\n",
    "    print(df.to_string())\n",
    "\n",
    "    trainer.save_model(f\"./flan-t5-large-finetuned-{dataset_name.lower()}\")\n",
    "    tokenizer.save_pretrained(f\"./flan-t5-large-finetuned-{dataset_name.lower()}\")\n",
    "    print(f\"\\n {dataset_name} model and tokenizer saved successfully.\\n\")\n",
    "\n",
    "\n",
    "# Run all datasets\n",
    "for config in dataset_configs:\n",
    "    run_finetuning(config[\"name\"], config[\"path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e7455-7127-4de7-b2f7-479c29fba303",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
