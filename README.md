# ðŸ§  NeuReg: Neuro-Symbolic QA Generation from Regulatory Compliance

NeuReg is a neuro-symbolic QA generation framework that transforms complex regulatory documents into intelligent, explainable QA systems. It integrates ontology-guided knowledge graphs (KGs) with large language models (LLMs) to generate high-quality, semantically grounded questionâ€“answer (QA) pairs.

---

## ðŸ”„ Pipeline Overview

1. **Ontology-Guided Triple Extraction**  
   Structured triples are extracted from Education Funding Regulatory (EFR) documents using a domain-specific ontology.

2. **KGâ€“Text Pair Formation**  
   Each triple is aligned with its corresponding text segment to provide contextual grounding.

3. **Multi-Strategy Prompting**  
   QA pairs are generated using Zero-Shot (ZS), One-Shot (OS), and Few-Shot (FS) prompting strategies.

4. **Quality Validation**  
   QA quality is assessed through expert human annotation and automatic evaluation using state-of-the-art LLMs.

---

## ðŸš€ Motivation

Access to education funding is governed by complex and evolving policies. These are often communicated in lengthy documents that are difficult for students and staff to interpret. NeuReg addresses this challenge by converting these regulatory documents into structured and queryable QA datasetsâ€”bridging the gap between unstructured policy language and structured decision support.

---

## âœ¨ Key Contributions

### ðŸ§  A Novel Neuro-Symbolic Framework

NeuReg combines LLM-based generative reasoning with symbolic structure from ontology-guided KGs, enabling semantically rich and accurate QA generation in high-stakes regulatory domains.

### ðŸ“Š First-of-Its-Kind Regulatory QA Dataset

We present a domain-specific QA dataset covering four distinct question types. It is validated via expert human judgments and automatic scoring by multiple SOTA LLMs.

### ðŸ”¬ Empirical Validation & Ablation Studies

Our evaluations isolate the unique contributions of KG and text inputs, demonstrate the effectiveness of different prompting strategies, and quantify model performance through fine-tuned T5/Flan-T5 variants.

---

## ðŸŽ¯ QA Generation Types

| Type      | Description |
|-----------|-------------|
| **FactQ**   | Direct factual retrieval (e.g., definitions, thresholds, dates) |
| **RelQ**    | Relational reasoning over KG (e.g., entity-to-entity links) |
| **CompQ**   | Comparison of policies, programs, or eligibility rules |
| **InferQ**  | Multi-hop reasoning and inference across KG and text |

---

NeuReg offers a reproducible, ontology-grounded QA pipeline designed to improve transparency and interpretability in complex regulatory environments. For code, datasets, evaluation results, and ablation studies, explore the full repository.

## ðŸ“‚ Repository Structure

<pre> ```text NeuReg/ â”œâ”€â”€ README.md # Overview of the project, pipeline, and structure â”œâ”€â”€ LICENSE # License (e.g., MIT, Apache 2.0) â”œâ”€â”€ requirements.txt # Python dependencies for reproducibility â”œâ”€â”€ data/ # Input data and KG construction â”‚ â”œâ”€â”€ chunks/ # Text chunk extraction â”‚ â”‚ â”œâ”€â”€ chunks.csv # Cleaned chunks from regulatory documents â”‚ â”‚ â””â”€â”€ chunks.ipynb # Chunking pipeline notebook â”‚ â”œâ”€â”€ ontology/ # Ontology schema and triple generation â”‚ â”‚ â”œâ”€â”€ ontology_schema.json # Domain ontology (JSON format) â”‚ â”‚ â”œâ”€â”€ Ontology_Guided_Triples.csv # KG triples aligned to ontology â”‚ â”‚ â”œâ”€â”€ Ontology_Guided_Triples_statistics.json # Triple statistics (e.g., counts) â”‚ â”‚ â”œâ”€â”€ EFRO_Schema_Extraction.ipynb # Extract schema from documents â”‚ â”‚ â””â”€â”€ KG_Extraction.ipynb # Triple generation notebook â”œâ”€â”€ qa_generation/ # QA dataset generation using LLMs â”‚ â”œâ”€â”€ Zero-shot.ipynb # Zero-shot prompting â”‚ â”œâ”€â”€ One-shot.ipynb # One-shot prompting â”‚ â”œâ”€â”€ Few-shot.ipynb # Few-shot prompting â”‚ â”œâ”€â”€ Zero-Shot_qa_dataset.json # QA dataset (zero-shot) â”‚ â”œâ”€â”€ One-Shot_qa_dataset.json # QA dataset (one-shot) â”‚ â”œâ”€â”€ Few-Shot_qa_dataset.json # QA dataset (few-shot) â”‚ â”œâ”€â”€ Zero_Shot_QA_analysis_report.json # Evaluation report (zero-shot) â”‚ â”œâ”€â”€ One_Shot_QA_analysis_report.json # Evaluation report (one-shot) â”‚ â””â”€â”€ Few_Shot_QA_analysis_report.json # Evaluation report (few-shot) â”œâ”€â”€ evaluation/ # Evaluation of QA quality â”‚ â”œâ”€â”€ ontology_guided/ # Triple validation via ontology rules â”‚ â”‚ â”œâ”€â”€ Evaluation.ipynb â”‚ â”‚ â”œâ”€â”€ evaluation_results.csv â”‚ â”‚ â””â”€â”€ evaluation_report.json â”‚ â”œâ”€â”€ llm_judges/ # LLM-based automatic QA evaluation â”‚ â”‚ â”œâ”€â”€ DeepSeek-R1-Distill-Llama-70B/ â”‚ â”‚ â”‚ â”œâ”€â”€ DeepSeek-R1-Distill-Llama-70B.ipynb â”‚ â”‚ â”‚ â”œâ”€â”€ *_zeroshot_*.csv # Zero-shot results â”‚ â”‚ â”‚ â””â”€â”€ *_fewshot_*.csv # Few-shot results â”‚ â”‚ â”œâ”€â”€ Gemma-2-27B/ â”‚ â”‚ â”œâ”€â”€ LLaMA-3.3-70B/ â”‚ â”‚ â”œâ”€â”€ Mixtral-8x22B/ â”‚ â”‚ â””â”€â”€ Qwen3-32B/ â”‚ â”œâ”€â”€ humans/ # Human evaluation files â”‚ â”‚ â”œâ”€â”€ Evaluation_Template.pdf # Scoring form (PDF/CSV) â”‚ â”‚ â”œâ”€â”€ Human_based_results_analysis.ipynb â”‚ â”‚ â””â”€â”€ humans_Analysis_report.csv â”‚ â””â”€â”€ llm_vs_human/ # Correlation between LLM and human scores â”‚ â”œâ”€â”€ llm_vs_human_Analysis_results_analysis.ipynb â”‚ â””â”€â”€ Correlation_llm_vs_human.csv â”œâ”€â”€ analysis/ # Dataset-level statistics â”‚ â”œâ”€â”€ Statistical_Analysis.ipynb â”‚ â”œâ”€â”€ Readability_Analysis.csv # FKGL, Flesch, etc. â”‚ â”œâ”€â”€ Vocabulary_Diversity_Analysis.csv â”‚ â”œâ”€â”€ Length_Distribution_Analysis.csv â”‚ â”œâ”€â”€ LLMs_based_results_analysis.ipynb â”‚ â””â”€â”€ LLMs_Analysis_report.csv â”œâ”€â”€ ablations/ # QA generation using partial inputs only â”‚ â”œâ”€â”€ chunks_only/ # QA from text chunks only (no KG) â”‚ â”‚ â”œâ”€â”€ chunks_only_qa_dataset.ipynb â”‚ â”‚ â”œâ”€â”€ evaluation/ â”‚ â”‚ â”‚ â”œâ”€â”€ chunks_only_evaluation_DeepSeekR1.ipynb â”‚ â”‚ â”‚ â”œâ”€â”€ *.csv â”‚ â”œâ”€â”€ kg_only/ # QA from KG triples only (no text) â”‚ â”‚ â”œâ”€â”€ KG_only_qa_dataset.ipynb â”‚ â”‚ â”œâ”€â”€ evaluation/ â”‚ â”‚ â”‚ â”œâ”€â”€ KG_only_evaluation_DeepSeekR1.ipynb â”‚ â”‚ â”‚ â”œâ”€â”€ *.csv â”‚ â”œâ”€â”€ Ablation_1_chunks_only_qa_dataset.json â”‚ â”œâ”€â”€ Ablation_1_chunks_only_analysis_report.json â”‚ â”œâ”€â”€ Ablation_2_kg_only_qa_dataset.json â”‚ â”œâ”€â”€ Ablation_2_kg_only_analysis_report.json â”œâ”€â”€ fine_tuning/ # Fine-tuning experiments on QA datasets â”‚ â”œâ”€â”€ t5_small/ â”‚ â”‚ â”œâ”€â”€ t5_small_zero.ipynb â”‚ â”‚ â”œâ”€â”€ t5_small_one.ipynb â”‚ â”‚ â””â”€â”€ t5_small_few.ipynb â”‚ â”œâ”€â”€ t5_base/ â”‚ â”œâ”€â”€ t5_large/ â”‚ â”œâ”€â”€ flan_t5_small/ â”‚ â”œâ”€â”€ fl

