# ðŸ§  NeuReg: Neuro-Symbolic QA Generation from Regulatory Compliance

NeuReg is a neuro-symbolic QA generation framework that transforms complex regulatory documents into intelligent, explainable QA systems. It integrates ontology-guided knowledge graphs (KGs) with large language models (LLMs) to generate high-quality, semantically grounded questionâ€“answer (QA) pairs.

---

## ðŸ”„ Pipeline Overview

1. **Ontology-Guided Triple Extraction**  
   Structured triples are extracted from Education Funding Regulatory (EFR) documents using a domain-specific ontology.

2. **KGâ€“Text Pair Formation**  
   Each triple is aligned with its corresponding text segment to provide contextual grounding.

3. **Multi-Strategy Prompting**  
   QA pairs are generated using Zero-Shot (ZS), One-Shot (OS), and Few-Shot (FS) prompting strategies.

4. **Quality Validation**  
   QA quality is assessed through expert human annotation and automatic evaluation using state-of-the-art LLMs.

---

## ðŸš€ Motivation

Access to education funding is governed by complex and evolving policies. These are often communicated in lengthy documents that are difficult for students and staff to interpret. NeuReg addresses this challenge by converting these regulatory documents into structured and queryable QA datasetsâ€”bridging the gap between unstructured policy language and structured decision support.

---

## âœ¨ Key Contributions

### ðŸ§  A Novel Neuro-Symbolic Framework

NeuReg combines LLM-based generative reasoning with symbolic structure from ontology-guided KGs, enabling semantically rich and accurate QA generation in high-stakes regulatory domains.

### ðŸ“Š First-of-Its-Kind Regulatory QA Dataset

We present a domain-specific QA dataset covering four distinct question types. It is validated via expert human judgments and automatic scoring by multiple SOTA LLMs.

### ðŸ”¬ Empirical Validation & Ablation Studies

Our evaluations isolate the unique contributions of KG and text inputs, demonstrate the effectiveness of different prompting strategies, and quantify model performance through fine-tuned T5/Flan-T5 variants.

---

## ðŸŽ¯ QA Generation Types

| Type      | Description |
|-----------|-------------|
| **FactQ**   | Direct factual retrieval (e.g., definitions, thresholds, dates) |
| **RelQ**    | Relational reasoning over KG (e.g., entity-to-entity links) |
| **CompQ**   | Comparison of policies, programs, or eligibility rules |
| **InferQ**  | Multi-hop reasoning and inference across KG and text |

---

NeuReg offers a reproducible, ontology-grounded QA pipeline designed to improve transparency and interpretability in complex regulatory environments. For code, datasets, evaluation results, and ablation studies, explore the full repository.

## ðŸ“‚ Repository Structure

```text
NeuReg/
â”œâ”€â”€ README.md                          # Overview of the project, contributions, pipeline, and structure
â”œâ”€â”€ LICENSE                            # Project license (e.g., MIT, Apache 2.0)
â”œâ”€â”€ requirements.txt                   # Python dependencies for reproducing the results

â”œâ”€â”€ data/                              # Preprocessing and knowledge graph construction
â”‚   â”œâ”€â”€ chunks/                        # Extracting regulatory text chunks
â”‚   â”‚   â”œâ”€â”€ chunks.csv                 # Final cleaned chunk dataset
â”‚   â”‚   â””â”€â”€ chunks.ipynb               # Chunk extraction notebook
â”‚   â”œâ”€â”€ ontology/                      # Ontology schema and KG triples
â”‚   â”‚   â”œâ”€â”€ ontology_schema.json       # Extracted domain ontology in JSON
â”‚   â”‚   â”œâ”€â”€ Ontology_Guided_Triples.csv           # Ontology-guided KG triples
â”‚   â”‚   â”œâ”€â”€ Ontology_Guided_Triples_statistics.json  # Stats on generated triples
â”‚   â”‚   â”œâ”€â”€ EFRO_Schema_Extraction.ipynb           # Extract ontology schema from guidance
â”‚   â”‚   â””â”€â”€ KG_Extraction.ipynb                    # Generate KG using ontology + chunks

â”œâ”€â”€ qa_generation/                     # QA dataset generation using prompting
â”‚   â”œâ”€â”€ Zero-shot.ipynb                # Zero-shot QA generation
â”‚   â”œâ”€â”€ One-shot.ipynb                 # One-shot QA generation
â”‚   â”œâ”€â”€ Few-shot.ipynb                 # Few-shot QA generation
â”‚   â”œâ”€â”€ Zero-Shot_qa_dataset.json      # Output QA dataset (zero-shot)
â”‚   â”œâ”€â”€ One-Shot_qa_dataset.json       # Output QA dataset (one-shot)
â”‚   â”œâ”€â”€ Few-Shot_qa_dataset.json       # Output QA dataset (few-shot)
â”‚   â”œâ”€â”€ Zero_Shot_QA_analysis_report.json  # Analysis report (zero-shot)
â”‚   â”œâ”€â”€ One_Shot_QA_analysis_report.json   # Analysis report (one-shot)
â”‚   â””â”€â”€ Few_Shot_QA_analysis_report.json   # Analysis report (few-shot)

â”œâ”€â”€ evaluation/                        # Evaluation modules for QA datasets
â”‚   â”œâ”€â”€ ontology_guided/               # KG triples validation
â”‚   â”‚   â”œâ”€â”€ Evaluation.ipynb
â”‚   â”‚   â”œâ”€â”€ evaluation_results.csv
â”‚   â”‚   â””â”€â”€ evaluation_report.json
â”‚   â”œâ”€â”€ llm_judges/                    # LLM-based QA scoring (5 models)
â”‚   â”‚   â”œâ”€â”€ DeepSeek-R1-Distill-Llama-70B/
â”‚   â”‚   â”‚   â”œâ”€â”€ DeepSeek-R1-Distill-Llama-70B.ipynb
â”‚   â”‚   â”‚   â”œâ”€â”€ *_zeroshot_*.csv
â”‚   â”‚   â”‚   â””â”€â”€ *_fewshot_*.csv
â”‚   â”‚   â”œâ”€â”€ Gemma-2-27B/
â”‚   â”‚   â”œâ”€â”€ LLaMA-3.3-70B/
â”‚   â”‚   â”œâ”€â”€ Mixtral-8x22B/
â”‚   â”‚   â””â”€â”€ Qwen3-32B/
â”‚   â”œâ”€â”€ humans/                        # Human-based QA evaluation
â”‚   â”‚   â”œâ”€â”€ Evaluation_Template.pdf (or .csv)      # Annotation template
â”‚   â”‚   â”œâ”€â”€ Human_based_results_analysis.ipynb
â”‚   â”‚   â””â”€â”€ humans_Analysis_report.csv
â”‚   â””â”€â”€ llm_vs_human/                 # Correlation between LLM and human scores
â”‚       â”œâ”€â”€ llm_vs_human_Analysis_results_analysis.ipynb
â”‚       â””â”€â”€ Correlation_llm_vs_human.csv

â”œâ”€â”€ analysis/                          # Statistical analysis & insights
â”‚   â”œâ”€â”€ Statistical_Analysis.ipynb
â”‚   â”œâ”€â”€ Readability_Analysis.csv       # FKGL, Flesch, etc.
â”‚   â”œâ”€â”€ Vocabulary_Diversity_Analysis.csv
â”‚   â”œâ”€â”€ Length_Distribution_Analysis.csv
â”‚   â”œâ”€â”€ LLMs_based_results_analysis.ipynb
â”‚   â””â”€â”€ LLMs_Analysis_report.csv

â”œâ”€â”€ ablations/                         # Ablation studies (chunks_only/kg_only)
â”‚   â”œâ”€â”€ chunks_only/                   # QA from chunks only (no KG)
â”‚   â”‚   â”œâ”€â”€ chunks_only_qa_dataset.ipynb
â”‚   â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”‚   â”œâ”€â”€ chunks_only_evaluation_DeepSeekR1.ipynb
â”‚   â”‚   â”‚   â”œâ”€â”€ *.csv
â”‚   â”œâ”€â”€ kg_only/                       # QA from KG only (no chunks)
â”‚   â”‚   â”œâ”€â”€ KG_only_qa_dataset.ipynb
â”‚   â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”‚   â”œâ”€â”€ KG_only_evaluation_DeepSeekR1.ipynb
â”‚   â”‚   â”‚   â”œâ”€â”€ *.csv
â”‚   â”œâ”€â”€ Ablation_1_chunks_only_qa_dataset.json      # Chunks-only QA JSON
â”‚   â”œâ”€â”€ Ablation_1_chunks_only_analysis_report.json # Evaluation report
â”‚   â”œâ”€â”€ Ablation_2_kg_only_qa_dataset.json          # KG-only QA JSON
â”‚   â”œâ”€â”€ Ablation_2_kg_only_analysis_report.json     # Evaluation report

â”œâ”€â”€ fine_tuning/                       # Fine-tuning experiments on QA datasets
â”‚   â”œâ”€â”€ t5_small/
â”‚   â”‚   â”œâ”€â”€ t5_small_zero.ipynb
â”‚   â”‚   â”œâ”€â”€ t5_small_one.ipynb
â”‚   â”‚   â””â”€â”€ t5_small_few.ipynb
â”‚   â”œâ”€â”€ t5_base/
â”‚   â”œâ”€â”€ t5_large/
â”‚   â”œâ”€â”€ flan_t5_small/
â”‚   â”œâ”€â”€ flan_t5_base/
â”‚   â”œâ”€â”€ flan_t5_large/
â”‚   â””â”€â”€ results/                       # Final results and logs
```
