{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2349c0bc-6079-4f2e-a7b9-d3672491efea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PDF Text Extractor and Cleaner using PyMuPDF and Unstructured\n",
    "\n",
    "This script:\n",
    "1. Extracts structured text from a PDF using the `unstructured` library.\n",
    "2. Skips unwanted pages.\n",
    "3. Cleans common encoding issues using `ftfy` and regex.\n",
    "4. Computes statistics on chunk and paragraph sizes.\n",
    "5. Saves cleaned output to CSV.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Any\n",
    "from pydantic import BaseModel\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "import ftfy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean OCR and encoding artifacts from PDF text.\"\"\"\n",
    "    cleaned_text = ftfy.fix_text(text)\n",
    "\n",
    "    replacements = {\n",
    "        r'‚Äôs': \"'\", r'‚Äô': \"'\", r'‚Äú': '\"', r'‚Äù': '\"',\n",
    "        r'‚Äì': '–', r'‚Ä¶': '…', r'‚Ä¢': '•', r'‚Äò': \"'\",\n",
    "        r'Äô': \"'\", r'Ä¢': '•', r'â€\"': '–', r'â€œ': '\"', r'â€�': '\"'\n",
    "    }\n",
    "\n",
    "    for pattern, replacement in replacements.items():\n",
    "        cleaned_text = re.sub(pattern, replacement, cleaned_text)\n",
    "\n",
    "    cleaned_text = re.sub(r'\\b(Page|Pg|P)\\s*\\d+\\b', '', cleaned_text, flags=re.IGNORECASE)\n",
    "    cleaned_text = re.sub(r'^\\d+$', '', cleaned_text, flags=re.MULTILINE)\n",
    "    cleaned_text = re.sub(r'\\bPage\\s+\\d+\\s+of\\s+\\d+\\b', '', cleaned_text, flags=re.IGNORECASE)\n",
    "    cleaned_text = re.sub(r'‚Ä[^\\w\\s]', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\s{2,}', ' ', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\n\\s*\\n', '\\n', cleaned_text)\n",
    "\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "\n",
    "def extract_pdf_text(pdf_path: str, skip_first_pages: int = 6, skip_last_page: bool = True):\n",
    "    \"\"\"Extract and clean structured text from PDF.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    total_pages = doc.page_count\n",
    "    doc.close()\n",
    "\n",
    "    raw_elements = partition_pdf(\n",
    "        filename=pdf_path,\n",
    "        extract_images_in_pdf=False,\n",
    "        infer_table_structure=True,\n",
    "        chunking_strategy=\"by_title\",\n",
    "        max_characters=1100,\n",
    "        new_after_n_chars=900,\n",
    "        combine_text_under_n_chars=400,\n",
    "    )\n",
    "\n",
    "    last_page = total_pages - 1 if skip_last_page else total_pages\n",
    "    filtered_elements = [\n",
    "        el for el in raw_elements\n",
    "        if hasattr(el, 'metadata') and hasattr(el, 'text') and\n",
    "        el.metadata.page_number and skip_first_pages < el.metadata.page_number < last_page\n",
    "    ]\n",
    "\n",
    "    return filtered_elements, total_pages\n",
    "\n",
    "\n",
    "def analyze_chunks(elements: list[Any], total_pages: int):\n",
    "    \"\"\"Analyze chunk and paragraph sizes and print statistics.\"\"\"\n",
    "    chunk_lengths, paragraph_lengths = [], []\n",
    "\n",
    "    for idx, el in enumerate(elements, 1):\n",
    "        cleaned = clean_text(el.text)\n",
    "        chunk_lengths.append(len(cleaned))\n",
    "        paragraph_lengths.extend([len(p) for p in cleaned.split('\\n\\n') if p])\n",
    "\n",
    "        print(f\"Chunk {idx}:\\n{cleaned}\\n{'-' * 40}\\n\")\n",
    "\n",
    "    print(f\"Total number of chunks: {len(chunk_lengths)}\")\n",
    "    print(f\"Average chunk size: {np.mean(chunk_lengths):.2f} characters\")\n",
    "    print(f\"Average paragraph size: {np.mean(paragraph_lengths):.2f} characters\")\n",
    "    print(f\"Average page size: {sum(chunk_lengths) / total_pages:.2f} characters\")\n",
    "\n",
    "\n",
    "def save_chunks_to_csv(elements: list[Any], output_file: str = \"cleaned_chunks.csv\"):\n",
    "    \"\"\"Save cleaned text chunks to CSV.\"\"\"\n",
    "    df = pd.DataFrame([{'text': clean_text(el.text)} for el in elements if hasattr(el, 'text')])\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\" Cleaned chunks saved to: {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    PDF_PATH = \"FYYP\"                       #pdf educational funding guidance docs \n",
    "    OUTPUT_FILE = \"chunks.csv\"\n",
    "\n",
    "    if not os.path.exists(PDF_PATH):\n",
    "        raise FileNotFoundError(f\"PDF not found at: {PDF_PATH}\")\n",
    "\n",
    "    elements, total_pages = extract_pdf_text(PDF_PATH)\n",
    "    analyze_chunks(elements, total_pages)\n",
    "    save_chunks_to_csv(elements, OUTPUT_FILE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
