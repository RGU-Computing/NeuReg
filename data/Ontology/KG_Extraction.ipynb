{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbd5963-4506-426b-a883-27535135d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Ontology Guided KG Extraction\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Load your OpenAI API key from environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"  #add your openAI key\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "def load_ontology_schema(schema_file_path=\"ontology_schema.json\"):\n",
    "    \"\"\"\n",
    "    Load ontology schema from JSON file\n",
    "    \n",
    "    Args:\n",
    "        schema_file_path: Path to the ontology schema JSON file\n",
    "    \n",
    "    Returns:\n",
    "        str: Formatted ontology schema string for use in prompts\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\" Loading ontology schema from: {schema_file_path}\")\n",
    "        with open(schema_file_path, 'r', encoding='utf-8') as f:\n",
    "            schema_data = json.load(f)\n",
    "        \n",
    "        # Check if the file has the schema_string field (from our ontology extractor)\n",
    "        if 'schema_string' in schema_data:\n",
    "            ontology_schema = schema_data['schema_string']\n",
    "            print(\" Using pre-formatted schema string from file\")\n",
    "        else:\n",
    "            # Build schema string from components\n",
    "            print(\" Building schema string from components\")\n",
    "            ontology_schema = build_schema_from_components(schema_data)\n",
    "        \n",
    "        print(f\" Schema loaded with {len(schema_data.get('classes', []))} classes and {len(schema_data.get('all_properties', []))} properties\")\n",
    "        return ontology_schema\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\" Error: {schema_file_path} not found\")\n",
    "        print(\"Please ensure you have run the ontology schema extractor first\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\" Error: Invalid JSON in {schema_file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\" Error loading schema: {e}\")\n",
    "        return None\n",
    "\n",
    "def build_schema_from_components(schema_data):\n",
    "    \"\"\"\n",
    "    Build ontology schema string from JSON components\n",
    "    \n",
    "    Args:\n",
    "        schema_data: Dictionary containing ontology components\n",
    "    \n",
    "    Returns:\n",
    "        str: Formatted ontology schema string\n",
    "    \"\"\"\n",
    "    schema_parts = []\n",
    "    \n",
    "    # Add classes\n",
    "    if 'classes' in schema_data and schema_data['classes']:\n",
    "        classes_str = ', '.join(sorted(schema_data['classes']))\n",
    "        schema_parts.append(f\"Ontology Classes:\\n- {classes_str}\")\n",
    "    \n",
    "    # Add relationships/properties\n",
    "    if 'all_properties' in schema_data and schema_data['all_properties']:\n",
    "        schema_parts.append(\"\\nOntology Relationships:\")\n",
    "        \n",
    "        property_domains = schema_data.get('property_domains', {})\n",
    "        property_ranges = schema_data.get('property_ranges', {})\n",
    "        \n",
    "        for prop in sorted(schema_data['all_properties']):\n",
    "            domains = property_domains.get(prop, [])\n",
    "            ranges = property_ranges.get(prop, [])\n",
    "            \n",
    "            if domains and ranges:\n",
    "                for domain in domains:\n",
    "                    for range_item in ranges:\n",
    "                        schema_parts.append(f\"- {prop} ({domain} -> {range_item})\")\n",
    "            else:\n",
    "                schema_parts.append(f\"- {prop}\")\n",
    "    \n",
    "    # Add class hierarchy if available\n",
    "    if 'class_hierarchy' in schema_data and schema_data['class_hierarchy']:\n",
    "        schema_parts.append(\"\\nClass Hierarchy:\")\n",
    "        for superclass, subclasses in schema_data['class_hierarchy'].items():\n",
    "            for subclass in subclasses:\n",
    "                schema_parts.append(f\"- {subclass} subClassOf {superclass}\")\n",
    "    \n",
    "    return \"\\n\".join(schema_parts)\n",
    "\n",
    "# Load the input chunks file\n",
    "print(\" Loading chunked data...\")\n",
    "chunks_df = pd.read_csv(\"chunks.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# Clean column names (remove BOM and whitespace)\n",
    "chunks_df.columns = chunks_df.columns.str.strip().str.replace('\\ufeff', '')\n",
    "print(f\" Loaded {len(chunks_df)} chunks\")\n",
    "\n",
    "# Check available columns\n",
    "print(f\" Available columns: {list(chunks_df.columns)}\")\n",
    "\n",
    "# Verify we have the required columns\n",
    "required_columns = ['chunk_id', 'text']\n",
    "missing_columns = [col for col in required_columns if col not in chunks_df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\" Missing required columns: {missing_columns}\")\n",
    "    print(\"Available columns:\", list(chunks_df.columns))\n",
    "    exit(1)\n",
    "\n",
    "print(f\" Data preview:\")\n",
    "print(f\"   • Shape: {chunks_df.shape}\")\n",
    "print(f\"   • Columns: {list(chunks_df.columns)}\")\n",
    "if len(chunks_df) > 0:\n",
    "    print(f\"   • Sample chunk_id: {chunks_df['chunk_id'].iloc[0]}\")\n",
    "    print(f\"   • Sample text: {str(chunks_df['text'].iloc[0])[:100]}...\")\n",
    "\n",
    "# Load ontology schema dynamically\n",
    "ontology_schema = load_ontology_schema(\"ontology_schema.json\")\n",
    "\n",
    "if not ontology_schema:\n",
    "    print(\" Cannot proceed without ontology schema. Exiting...\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"\\n Using the following ontology schema:\")\n",
    "print(\"=\"*60)\n",
    "print(ontology_schema)\n",
    "print(\"=\"*60)\n",
    "\n",
    "#  Guide GPT 4 turbo\n",
    "example = \"\"\"\n",
    "Text:\n",
    "Institutions must assume that EEA students have the right to remain in the UK. Once a student is enrolled, the institution is expected to take all reasonable steps to ensure that the student can complete their programme.\n",
    "\n",
    "Triples:\n",
    "(institution, has_assumption, eea_students_right_to_remain)\n",
    "(student, is_enrolled_in, study_programme)\n",
    "(institution, has_legal_duty, ensure_student_completion)\n",
    "(institution, provides_support, student)\n",
    "\"\"\"\n",
    "\n",
    "# Prompt builder function\n",
    "def build_prompt(text_chunk):\n",
    "    return f\"\"\"\n",
    "You are a knowledge extraction model that converts educational policy text into triples.\n",
    "\n",
    "Below is a text chunk from an educational policy document:\n",
    "\n",
    "\\\"\\\"\\\"{text_chunk}\\\"\\\"\\\"\n",
    "\n",
    "Use the ontology schema below to guide your extraction:\n",
    "\n",
    "{ontology_schema}\n",
    "\n",
    "---\n",
    "\n",
    "Here is an example:\n",
    "\n",
    "{example}\n",
    "\n",
    "---\n",
    "\n",
    "Now, extract all triples from the provided chunk in the format:\n",
    "(subject, predicate, object)\n",
    "\n",
    "-Do not use full sentences or long descriptions as object values. Instead, normalize them into short, meaningful phrases or identifiers (e.g., `GDPR_compliance`, `funding_16_19`, `right_to_remain`).\n",
    "\n",
    "\"\"\"  \n",
    "\n",
    "# Function to query GPT-4\n",
    "def extract_triples(text_chunk):\n",
    "    prompt = build_prompt(text_chunk[:2000])  # Truncate if too long\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in ontology-based information extraction.\"},\n",
    "                {\"role\": \"user\",   \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            max_tokens=512\n",
    "        )\n",
    "        return response['choices'][0]['message']['content']\n",
    "    except Exception as e:\n",
    "        print(f\"Error during API call: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Regex pattern to match triples (handles commas inside the object)\n",
    "triple_pattern = re.compile(r\"^\\(\\s*([^,]+?)\\s*,\\s*([^,]+?)\\s*,\\s*(.+?)\\s*\\)$\")\n",
    "\n",
    "def analyze_triples_statistics(triples_df, output_filename):\n",
    "    \"\"\"\n",
    "    Generate comprehensive statistics and summary for the extracted triples\n",
    "    \n",
    "    Args:\n",
    "        triples_df: DataFrame containing the extracted triples\n",
    "        output_filename: Name of the output CSV file\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing all statistics\n",
    "    \"\"\"\n",
    "    print(f\"\\n GENERATING TRIPLES STATISTICS AND SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_triples = len(triples_df)\n",
    "    unique_chunks = triples_df['chunk_id'].nunique()\n",
    "    unique_subjects = triples_df['subject'].nunique()\n",
    "    unique_predicates = triples_df['predicate'].nunique()\n",
    "    unique_objects = triples_df['object'].nunique()\n",
    "    \n",
    "    # Frequency analysis\n",
    "    subject_counts = Counter(triples_df['subject'])\n",
    "    predicate_counts = Counter(triples_df['predicate'])\n",
    "    object_counts = Counter(triples_df['object'])\n",
    "    \n",
    "    # Chunk-level analysis\n",
    "    triples_per_chunk = triples_df.groupby('chunk_id').size()\n",
    "    \n",
    "    # Relationship patterns\n",
    "    subject_predicate_pairs = Counter(zip(triples_df['subject'], triples_df['predicate']))\n",
    "    predicate_object_pairs = Counter(zip(triples_df['predicate'], triples_df['object']))\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(f\" BASIC STATISTICS:\")\n",
    "    print(f\"   • Total Triples: {total_triples:,}\")\n",
    "    print(f\"   • Unique Chunks Processed: {unique_chunks:,}\")\n",
    "    print(f\"   • Unique Subjects: {unique_subjects:,}\")\n",
    "    print(f\"   • Unique Predicates: {unique_predicates:,}\")\n",
    "    print(f\"   • Unique Objects: {unique_objects:,}\")\n",
    "    print(f\"   • Average Triples per Chunk: {triples_per_chunk.mean():.1f}\")\n",
    "    print(f\"   • Min Triples per Chunk: {triples_per_chunk.min()}\")\n",
    "    print(f\"   • Max Triples per Chunk: {triples_per_chunk.max()}\")\n",
    "    \n",
    "    print(f\"\\n TOP 10 MOST FREQUENT SUBJECTS:\")\n",
    "    for i, (subject, count) in enumerate(subject_counts.most_common(10), 1):\n",
    "        print(f\"   {i:2d}. {subject} ({count} occurrences)\")\n",
    "    \n",
    "    print(f\"\\n TOP 10 MOST FREQUENT PREDICATES:\")\n",
    "    for i, (predicate, count) in enumerate(predicate_counts.most_common(10), 1):\n",
    "        print(f\"   {i:2d}. {predicate} ({count} occurrences)\")\n",
    "    \n",
    "    print(f\"\\n TOP 10 MOST FREQUENT OBJECTS:\")\n",
    "    for i, (obj, count) in enumerate(object_counts.most_common(10), 1):\n",
    "        print(f\"   {i:2d}. {obj} ({count} occurrences)\")\n",
    "    \n",
    "    print(f\"\\n CHUNK DISTRIBUTION:\")\n",
    "    chunk_stats = triples_per_chunk.describe()\n",
    "    print(f\"   • Mean: {chunk_stats['mean']:.1f}\")\n",
    "    print(f\"   • Std Dev: {chunk_stats['std']:.1f}\")\n",
    "    print(f\"   • 25th Percentile: {chunk_stats['25%']:.0f}\")\n",
    "    print(f\"   • Median: {chunk_stats['50%']:.0f}\")\n",
    "    print(f\"   • 75th Percentile: {chunk_stats['75%']:.0f}\")\n",
    "    \n",
    "    print(f\"\\n TOP 10 SUBJECT-PREDICATE PATTERNS:\")\n",
    "    for i, ((subject, predicate), count) in enumerate(subject_predicate_pairs.most_common(10), 1):\n",
    "        print(f\"   {i:2d}. ({subject}, {predicate}) - {count} times\")\n",
    "    \n",
    "    print(f\"\\n TOP 10 PREDICATE-OBJECT PATTERNS:\")\n",
    "    for i, ((predicate, obj), count) in enumerate(predicate_object_pairs.most_common(10), 1):\n",
    "        print(f\"   {i:2d}. ({predicate}, {obj}) - {count} times\")\n",
    "    \n",
    "    # Quality metrics\n",
    "    print(f\"\\n QUALITY METRICS:\")\n",
    "    avg_subject_length = triples_df['subject'].str.len().mean()\n",
    "    avg_predicate_length = triples_df['predicate'].str.len().mean()\n",
    "    avg_object_length = triples_df['object'].str.len().mean()\n",
    "    \n",
    "    print(f\"   • Average Subject Length: {avg_subject_length:.1f} characters\")\n",
    "    print(f\"   • Average Predicate Length: {avg_predicate_length:.1f} characters\")\n",
    "    print(f\"   • Average Object Length: {avg_object_length:.1f} characters\")\n",
    "    \n",
    "    # Coverage analysis\n",
    "    coverage_rate = unique_chunks / len(chunks_df) * 100 if 'chunks_df' in globals() else 0\n",
    "    print(f\"   • Chunk Coverage: {coverage_rate:.1f}% ({unique_chunks} out of {len(chunks_df) if 'chunks_df' in globals() else 'N/A'} chunks)\")\n",
    "    \n",
    "    # Compile statistics dictionary\n",
    "    stats = {\n",
    "        'basic_stats': {\n",
    "            'total_triples': total_triples,\n",
    "            'unique_chunks': unique_chunks,\n",
    "            'unique_subjects': unique_subjects,\n",
    "            'unique_predicates': unique_predicates,\n",
    "            'unique_objects': unique_objects,\n",
    "            'avg_triples_per_chunk': float(triples_per_chunk.mean()),\n",
    "            'min_triples_per_chunk': int(triples_per_chunk.min()),\n",
    "            'max_triples_per_chunk': int(triples_per_chunk.max())\n",
    "        },\n",
    "        'top_subjects': dict(subject_counts.most_common(20)),\n",
    "        'top_predicates': dict(predicate_counts.most_common(20)),\n",
    "        'top_objects': dict(object_counts.most_common(20)),\n",
    "        'top_subject_predicate_patterns': {f\"{s}||{p}\": count for (s, p), count in subject_predicate_pairs.most_common(20)},\n",
    "        'top_predicate_object_patterns': {f\"{p}||{o}\": count for (p, o), count in predicate_object_pairs.most_common(20)},\n",
    "        'chunk_statistics': {k: float(v) if pd.notna(v) else None for k, v in chunk_stats.to_dict().items()},\n",
    "        'triple_statistics': {\n",
    "            'avg_subject_length': float(avg_subject_length),\n",
    "            'avg_predicate_length': float(avg_predicate_length),\n",
    "            'avg_object_length': float(avg_object_length)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def save_detailed_summary(triples_df, stats, output_filename):\n",
    "    \"\"\"\n",
    "    Save detailed summary to JSON format only\n",
    "    \n",
    "    Args:\n",
    "        triples_df: DataFrame containing the extracted triples\n",
    "        stats: Statistics dictionary\n",
    "        output_filename: Base filename for outputs\n",
    "    \"\"\"\n",
    "    base_name = output_filename.replace('.csv', '')\n",
    "    \n",
    "    # Save statistics to JSON\n",
    "    stats_filename = f\"{base_name}_statistics.json\"\n",
    "    with open(stats_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(stats, f, indent=2, ensure_ascii=False)\n",
    "    print(f\" Detailed statistics saved to: {stats_filename}\")\n",
    "\n",
    "# Extraction loop\n",
    "def main():\n",
    "    print(f\"\\n Starting knowledge graph extraction...\")\n",
    "    print(f\" Processing chunks using dynamic ontology schema\")\n",
    "    \n",
    "    triples_list = []\n",
    "  \n",
    "    test_chunks = chunks_df\n",
    "    print(f\" Processing {len(test_chunks)} chunks in FULL mode\")\n",
    "    \n",
    "    for _, row in tqdm(test_chunks.iterrows(), total=len(test_chunks), desc=\"Extracting triples\"):  # FIXED\n",
    "        chunk_id   = row['chunk_id']\n",
    "        chunk_text = row['text']\n",
    "        result     = extract_triples(chunk_text)\n",
    "        time.sleep(1.2)  # throttle requests\n",
    "        \n",
    "        # Append raw output for inspection\n",
    "        with open(\"raw_outputs.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"\\n=== CHUNK ID: {chunk_id} ===\\n{result}\\n\")\n",
    "        \n",
    "        if not result.strip():\n",
    "            continue\n",
    "        \n",
    "        for line in result.splitlines():\n",
    "            # 1) Trim whitespace\n",
    "            # 2) Remove leading bullets or numbering like \"1. \", \"- \", \"* \"\n",
    "            # 3) Strip backticks from code fences\n",
    "            cleaned = line.strip().lstrip(\"-*0123456789. \").strip('`')\n",
    "            m = triple_pattern.match(cleaned)\n",
    "            if m:\n",
    "                s, p, o = m.groups()\n",
    "                triples_list.append({\n",
    "                    \"chunk_id\":  chunk_id,\n",
    "                    \"subject\":   s.strip(),\n",
    "                    \"predicate\": p.strip(),\n",
    "                    \"object\":    o.strip().strip('\"')\n",
    "                })\n",
    "            else:\n",
    "                # Optional: log unmatched lines for further analysis\n",
    "                # print(f\"Unmatched line: {cleaned}\")\n",
    "                pass\n",
    "    \n",
    "    # Save triples to CSV\n",
    "    output_df = pd.DataFrame(triples_list)\n",
    "    output_filename = \"Ontology_Guided__KG_Triples.csv\"\n",
    "    output_df.to_csv(output_filename, index=False)\n",
    "    \n",
    "    # Generate comprehensive statistics and summary\n",
    "    if len(triples_list) > 0:\n",
    "        stats = analyze_triples_statistics(output_df, output_filename)\n",
    "        save_detailed_summary(output_df, stats, output_filename)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n Triple extraction complete!\")\n",
    "    print(f\" Extraction Summary:\")\n",
    "    print(f\"   • Processed chunks: {len(test_chunks)}\")\n",
    "    print(f\"   • Total triples extracted: {len(triples_list)}\")\n",
    "    if len(test_chunks) > 0:\n",
    "        print(f\"   • Average triples per chunk: {len(triples_list)/len(test_chunks):.1f}\")\n",
    "    print(f\"   • Results saved to: {output_filename}\")\n",
    "    print(f\"   • Raw outputs saved to: raw_outputs.txt\")\n",
    "    \n",
    "    # Show sample triples\n",
    "    if len(triples_list) > 0:\n",
    "        print(f\"\\n Sample extracted triples:\")\n",
    "        for i, triple in enumerate(triples_list[:5]):\n",
    "            print(f\"   {i+1}. ({triple['subject']}, {triple['predicate']}, {triple['object']})\")\n",
    "        if len(triples_list) > 5:\n",
    "            print(f\"   ... and {len(triples_list) - 5} more\")\n",
    "    \n",
    "    # List all output files generated\n",
    "    print(f\"\\n OUTPUT FILES GENERATED:\")\n",
    "    print(f\"    Main results: {output_filename}\")\n",
    "    print(f\"    Statistics: {output_filename.replace('.csv', '_statistics.json')}\")\n",
    "    print(f\"    Raw outputs: raw_outputs.txt\")\n",
    "\n",
    "if __name__ == \"__main__\":  \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60efa5f-e2c8-44e7-af7a-5694a09e880d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
