{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750ea15d-cb55-4e02-a9ec-7130f9adb44a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ablation Study 2: KG-Only QA Generator (No Text Context)\n",
    "\n",
    "This is the second controlled ablation study derived from the One-Shot QA Generator.\n",
    "It removes dependency on text context and uses only knowledge graph triples to evaluate \n",
    "their standalone contribution to QA quality.\n",
    "\n",
    "Key Changes from Original:\n",
    "- Removes all text context dependencies \n",
    "- Uses KG-only exemplars and generation\n",
    "- Maintains same filtering pipeline and architecture\n",
    "- Preserves all quality controls and statistics tracking\n",
    "\n",
    "Purpose: Evaluate the impact of text context vs. pure knowledge graph information on QA generation quality.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import time\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from collections import OrderedDict\n",
    "\n",
    "# OpenAI import fix with proper error handling\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    OPENAI_V1 = True\n",
    "    logger_msg = \"Using OpenAI v1.x API\"\n",
    "except ImportError:\n",
    "    try:\n",
    "        import openai\n",
    "        OPENAI_V1 = False\n",
    "        logger_msg = \"Using OpenAI legacy API\"\n",
    "    except ImportError:\n",
    "        print(\"ERROR: OpenAI library not installed. Run: pip install openai\")\n",
    "        sys.exit(1)\n",
    "\n",
    "# Configure environment and logging\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('ablation_kg_only_qa_generation.log'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Device configuration with proper torch import\n",
    "try:\n",
    "    import torch\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \n",
    "                         \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "except ImportError:\n",
    "    print(\"ERROR: PyTorch not installed. Run: pip install torch\")\n",
    "    sys.exit(1)\n",
    "\n",
    "logger.info(logger_msg)\n",
    "\n",
    "\n",
    "class QuestionType(Enum):\n",
    "    \"\"\"Enumeration of supported question types for ablation study.\"\"\"\n",
    "    FACTUAL = \"factual\"\n",
    "    RELATIONSHIP = \"relationship\" \n",
    "    COMPARATIVE = \"comparative\"\n",
    "    INFERENTIAL = \"inferential\"\n",
    "\n",
    "\n",
    "class FilteringDecision(Enum):\n",
    "    \"\"\"Basic filtering decision outcomes.\"\"\"\n",
    "    ACCEPTED = \"accepted\"\n",
    "    REJECTED_LENGTH = \"rejected_length\"\n",
    "    REJECTED_DUPLICATE = \"rejected_duplicate\"\n",
    "    REJECTED_PARSING = \"rejected_parsing\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class QualityThresholds:\n",
    "    \"\"\"Quality thresholds for essential filtering (adapted for KG-only).\"\"\"\n",
    "    min_question_words: int = 8\n",
    "    min_answer_words: int = 20\n",
    "    duplicate_similarity_threshold: float = 0.85\n",
    "    batch_similarity_threshold: float = 0.85\n",
    "    triple_weight: float = 1.0  # Only triple relevance in this ablation\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FilteringResult:\n",
    "    \"\"\"Data class representing filtering result (simplified for KG-only).\"\"\"\n",
    "    accepted: bool\n",
    "    decision: FilteringDecision\n",
    "    relevance_score: float  # Now based only on triple similarity\n",
    "    triple_similarity: float\n",
    "    reasoning: str\n",
    "    metadata: Dict[str, float]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class QAPair:\n",
    "    \"\"\"Data class for QA pairs (simplified for KG-only ablation).\"\"\"\n",
    "    id: str\n",
    "    question: str\n",
    "    answer: str\n",
    "    question_type: QuestionType\n",
    "    qa_metadata: Dict[str, List[str]]\n",
    "    filtering_result: FilteringResult\n",
    "    generation_method: str = \"ablation_kg_only\"\n",
    "    # Ground truth information (only triples)\n",
    "    source_triples: List[Tuple] = None\n",
    "    chunk_id: str = \"\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.source_triples is None:\n",
    "            self.source_triples = []\n",
    "\n",
    "\n",
    "# KG-ONLY EXEMPLAR (Designed specifically for knowledge graph analysis)\n",
    "KG_ONLY_EXEMPLAR = {\n",
    "    \"knowledge_graph\": [\n",
    "        (\"esfa\", \"funds\", \"ehe_children_further_education\"),\n",
    "        (\"ehe_children_further_education\", \"enrolled_in\", \"further_education_colleges\"),\n",
    "        (\"ehe_children_further_education\", \"enrolled_in\", \"sixth_form_colleges\"),\n",
    "        (\"ehe_children_schools_academies\", \"not_eligible_for\", \"esfa_young_people_funding\"),\n",
    "        (\"funding_rates_and_formula_guide\", \"provides_information_on\", \"esfa_funding_details\"),\n",
    "        (\"colleges\", \"can_claim\", \"esfa_young_people_funding\"),\n",
    "        (\"esfa_young_people_funding\", \"for_programme\", \"level_3_course\"),\n",
    "        (\"children_compulsory_school_age\", \"has_achievement_status\", \"full_level_2_qualification\"),\n",
    "        (\"esfa\", \"does_not_require_approval_from\", \"colleges_for_lagged_funding\"),\n",
    "        (\"schools_and_academies\", \"applies_same_advice_as\", \"colleges_for_early_sixth_form_placement\"),\n",
    "        (\"esfa\", \"considers_funding_eligibility_for\", \"individual_students_compulsory_school_age\"),\n",
    "        (\"individual_students_compulsory_school_age\", \"has_reason\", \"arriving_in_uk_during_school_year_11\"),\n",
    "        (\"groups_of_students\", \"not_eligible_for\", \"esfa_young_people_funding_due_to_non_exceptional_circumstances\")\n",
    "    ],\n",
    "    \n",
    "    \"exemplar_questions\": {\n",
    "        QuestionType.FACTUAL: {\n",
    "            \"question\": \"Which types of colleges do EHE children attend when funded by the ESFA?\"\n",
    "        },\n",
    "        \n",
    "        QuestionType.RELATIONSHIP: {\n",
    "            \"question\": \"What is the relationship between colleges and ESFA young people's funding for Level 3 courses?\"\n",
    "        },\n",
    "        \n",
    "        QuestionType.COMPARATIVE: {\n",
    "            \"question\": \"How does ESFA funding eligibility differ for individuals versus groups arriving during school year 11?\"\n",
    "        },\n",
    "        \n",
    "        QuestionType.INFERENTIAL: {\n",
    "            \"question\": \"Based on the ESFA's policy regarding approval for lagged funding, what implication can be drawn about the need for direct oversight by ESFA for such claims?\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# KG-only question generation templates (removed context references)\n",
    "KG_ONLY_QUESTION_TEMPLATES = {\n",
    "    QuestionType.FACTUAL: {\n",
    "        \"task_description\": (\n",
    "            \"Factual questions seek specific information that can be directly extracted from \"\n",
    "            \"the knowledge graph relationships. They focus on entities, their properties, \"\n",
    "            \"and direct connections represented in the triples.\"\n",
    "        ),\n",
    "        \"generation_guidance\": (\n",
    "            \"Generate factual questions that ask about specific entities, relationships, or properties \"\n",
    "            \"directly represented in the knowledge graph triples. Focus on 'what', 'who', 'which' \"\n",
    "            \"questions that can be answered by examining the graph structure and entity connections.\"\n",
    "        )\n",
    "    },\n",
    "    \n",
    "    QuestionType.RELATIONSHIP: {\n",
    "        \"task_description\": (\n",
    "            \"Relationship questions explore direct and indirect connections between entities \"\n",
    "            \"in the knowledge graph. They examine how entities are linked through various \"\n",
    "            \"relationship types and connection patterns.\"\n",
    "        ),\n",
    "        \"generation_guidance\": (\n",
    "            \"Generate questions about relationships and connections between entities in the knowledge graph. \"\n",
    "            \"Focus on how entities are connected, what relationships exist between them, and how \"\n",
    "            \"these connections form meaningful patterns in the graph structure.\"\n",
    "        )\n",
    "    },\n",
    "    \n",
    "    QuestionType.COMPARATIVE: {\n",
    "        \"task_description\": (\n",
    "            \"Comparative questions examine differences and similarities between entities or \"\n",
    "            \"relationship patterns in the knowledge graph. They analyze contrasting paths, \"\n",
    "            \"different relationship types, or varying entity properties.\"\n",
    "        ),\n",
    "        \"generation_guidance\": (\n",
    "            \"Generate questions comparing different entities, relationships, or patterns in the knowledge graph. \"\n",
    "            \"These questions should highlight differences in how entities are connected, what relationships \"\n",
    "            \"they participate in, or how their graph positions differ from one another.\"\n",
    "        )\n",
    "    },\n",
    "    \n",
    "    QuestionType.INFERENTIAL: {\n",
    "        \"task_description\": (\n",
    "            \"Inferential questions require reasoning about the knowledge graph structure to \"\n",
    "            \"derive insights not explicitly stated in individual triples. They synthesize \"\n",
    "            \"multiple relationships to draw conclusions about the domain.\"\n",
    "        ),\n",
    "        \"generation_guidance\": (\n",
    "            \"Generate questions that require analysis and reasoning about the knowledge graph structure. \"\n",
    "            \"These questions should combine information from multiple triples to draw conclusions, \"\n",
    "            \"identify implications, or understand broader patterns in the entity relationships.\"\n",
    "        )\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "class KGOnlyQAGenerator:\n",
    "    \"\"\"\n",
    "    Ablation Study 2: KG-Only QA Generator (No Text Context Required).\n",
    "    \n",
    "    This version removes all dependency on text context and uses only knowledge graph \n",
    "    triples to evaluate their standalone contribution to QA generation quality while \n",
    "    maintaining the same architecture and filtering pipeline as the original.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 openai_api_key: str,\n",
    "                 reset_duplicates_per_chunk: bool = True,\n",
    "                 embedding_model: str = 'BAAI/bge-large-en-v1.5',\n",
    "                 max_embedding_cache: int = 1000):\n",
    "        \"\"\"\n",
    "        Initialize the KG-Only QA Generator.\n",
    "        \n",
    "        Args:\n",
    "            openai_api_key: OpenAI API key for GPT-4 access\n",
    "            reset_duplicates_per_chunk: Whether to reset duplicate tracking per chunk\n",
    "            max_embedding_cache: Maximum number of embeddings to cache (memory management)\n",
    "        \"\"\"\n",
    "        if not openai_api_key or openai_api_key == \"your-openai-api-key-here\":\n",
    "            raise ValueError(\"Please provide a valid OpenAI API key\")\n",
    "            \n",
    "        self.openai_api_key = openai_api_key\n",
    "        self.max_embedding_cache = max_embedding_cache\n",
    "        \n",
    "        # Initialize OpenAI client based on version\n",
    "        if OPENAI_V1:\n",
    "            self.openai_client = OpenAI(api_key=openai_api_key)\n",
    "        else:\n",
    "            openai.api_key = openai_api_key\n",
    "            self.openai_client = None\n",
    "        \n",
    "        # Initialize quality thresholds (adapted for KG-only)\n",
    "        self.thresholds = QualityThresholds()\n",
    "        self.reset_duplicates_per_chunk = reset_duplicates_per_chunk\n",
    "        \n",
    "        # Initialize embedding model\n",
    "        logger.info(f\"Loading embedding model: {embedding_model}\")\n",
    "        try:\n",
    "            self.embedding_model = SentenceTransformer(embedding_model, device=device)\n",
    "            logger.info(f\"Successfully loaded {embedding_model}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading {embedding_model}: {e}\")\n",
    "            raise RuntimeError(f\"Failed to load the specified embedding model '{embedding_model}'.\")\n",
    "        \n",
    "        # Duplicate tracking (same as original)\n",
    "        self.all_questions: set = set()\n",
    "        self.question_embeddings: OrderedDict[str, torch.Tensor] = OrderedDict()\n",
    "        self.batch_questions: set = set()\n",
    "        self.batch_embeddings: Dict[str, torch.Tensor] = {}\n",
    "        \n",
    "        # statistics tracking\n",
    "        self.generation_stats = {\n",
    "            \"total_api_calls\": 0,\n",
    "            \"total_qa_attempts\": 0,\n",
    "            \"successful_generations\": 0,\n",
    "            \"filtering_decisions\": {\n",
    "                \"accepted\": 0,\n",
    "                \"rejected_length\": 0,\n",
    "                \"rejected_duplicate\": 0,\n",
    "                \"rejected_parsing\": 0\n",
    "            },\n",
    "            \"all_attempts\": []\n",
    "        }\n",
    "        \n",
    "        logger.info(\"KG-Only QA Generator initialized successfully\")\n",
    "        logger.info(\"ABLATION STUDY 2: Using knowledge graph triples only (no text context)\")\n",
    "\n",
    "    def _filtering_result_to_dict(self, result: FilteringResult) -> Dict:\n",
    "        \"\"\"Convert FilteringResult to dictionary for JSON serialization.\"\"\"\n",
    "        return {\n",
    "            \"accepted\": result.accepted,\n",
    "            \"decision\": result.decision.value,\n",
    "            \"relevance_score\": result.relevance_score,\n",
    "            \"triple_similarity\": result.triple_similarity,\n",
    "            \"reasoning\": result.reasoning,\n",
    "            \"metadata\": result.metadata\n",
    "        }\n",
    "\n",
    "    def _manage_embedding_cache(self):\n",
    "        \"\"\"Manage embedding cache size to prevent memory issues.\"\"\"\n",
    "        if len(self.question_embeddings) > self.max_embedding_cache:\n",
    "            removed_count = len(self.question_embeddings) - self.max_embedding_cache\n",
    "            for _ in range(removed_count):\n",
    "                oldest_key = next(iter(self.question_embeddings))\n",
    "                del self.question_embeddings[oldest_key]\n",
    "                self.all_questions.discard(oldest_key)\n",
    "            logger.debug(f\"Removed {removed_count} old embeddings from cache\")\n",
    "\n",
    "    def calculate_relevance_score(self, question: str, answer: str, triples: List[Tuple]) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Calculate KG-only relevance score for logging/analysis.\n",
    "        Simplified version that only uses triple similarity (no context).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            qa_combined = f\"{question} {answer}\"\n",
    "            qa_embedding = self.embedding_model.encode(qa_combined, convert_to_tensor=True)\n",
    "            \n",
    "            # Triple similarities only\n",
    "            triple_similarities = []\n",
    "            for triple in triples:\n",
    "                try:\n",
    "                    if len(triple) >= 3:\n",
    "                        n1, edge, n2 = str(triple[0]), str(triple[1]), str(triple[2])\n",
    "                        triple_text = f\"{n1} {edge} {n2}\"\n",
    "                        triple_embedding = self.embedding_model.encode(triple_text, convert_to_tensor=True)\n",
    "                        triple_similarity = util.cos_sim(triple_embedding, qa_embedding).item()\n",
    "                        triple_similarities.append(triple_similarity)\n",
    "                except (IndexError, TypeError) as e:\n",
    "                    logger.debug(f\"Invalid triple format: {triple}, error: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            max_triple_similarity = max(triple_similarities) if triple_similarities else 0.0\n",
    "            \n",
    "            # Overall relevance is just max triple similarity in this ablation\n",
    "            overall_relevance = max_triple_similarity\n",
    "            \n",
    "            return overall_relevance, max_triple_similarity\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error calculating relevance score: {e}\")\n",
    "            return 0.0, 0.0\n",
    "\n",
    "    def passes_length_filter(self, question: str, answer: str) -> bool:\n",
    "        \"\"\"Check if QA pair meets minimum length requirements (unchanged).\"\"\"\n",
    "        try:\n",
    "            q_words = len(question.split())\n",
    "            a_words = len(answer.split())\n",
    "            \n",
    "            if q_words < self.thresholds.min_question_words or a_words < self.thresholds.min_answer_words:\n",
    "                logger.debug(f\"Length filter failed: Q={q_words} words (min {self.thresholds.min_question_words}), \"\n",
    "                            f\"A={a_words} words (min {self.thresholds.min_answer_words})\")\n",
    "                return False\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error in length filter: {e}\")\n",
    "            return False\n",
    "\n",
    "    def is_semantic_duplicate(self, new_question: str) -> Tuple[bool, torch.Tensor]:\n",
    "        \"\"\"Check if question is a semantic duplicate (unchanged from original).\"\"\"\n",
    "        try:\n",
    "            new_embedding = self.embedding_model.encode(new_question, convert_to_tensor=True)\n",
    "            \n",
    "            # Check against historical questions\n",
    "            for existing_question, existing_embedding in self.question_embeddings.items():\n",
    "                similarity = util.cos_sim(new_embedding, existing_embedding).item()\n",
    "                if similarity > self.thresholds.duplicate_similarity_threshold:\n",
    "                    logger.debug(f\"Historical duplicate detected: '{new_question}' ~ '{existing_question}' \"\n",
    "                               f\"(similarity={similarity:.3f})\")\n",
    "                    return True, new_embedding\n",
    "            \n",
    "            # Check against current batch\n",
    "            for batch_question, batch_embedding in self.batch_embeddings.items():\n",
    "                similarity = util.cos_sim(new_embedding, batch_embedding).item()\n",
    "                if similarity > self.thresholds.batch_similarity_threshold:\n",
    "                    logger.debug(f\"Batch duplicate detected: '{new_question}' ~ '{batch_question}' \"\n",
    "                               f\"(similarity={similarity:.3f})\")\n",
    "                    return True, new_embedding\n",
    "            \n",
    "            return False, new_embedding\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error in duplicate detection: {e}\")\n",
    "            dummy_embedding = torch.zeros(384)\n",
    "            return False, dummy_embedding\n",
    "\n",
    "    def filter_qa_pair(self, \n",
    "                      question: str, \n",
    "                      answer: str, \n",
    "                      triples: List[Tuple],\n",
    "                      qa_metadata: Dict) -> Tuple[bool, FilteringResult]:\n",
    "        \"\"\"\n",
    "        Apply filtering pipeline to QA pair (simplified for KG-only).\n",
    "        Removed all text context dependencies while maintaining same filtering logic.\n",
    "        \"\"\"\n",
    "        # Centralized statistics tracking\n",
    "        self.generation_stats[\"total_qa_attempts\"] += 1\n",
    "        \n",
    "        # Calculate relevance score (KG-only)\n",
    "        relevance_score, triple_sim = self.calculate_relevance_score(question, answer, triples)\n",
    "        \n",
    "        # Create base metadata (simplified for KG-only)\n",
    "        base_metadata = {\n",
    "            \"answer_length\": len(answer.split()),\n",
    "            \"question_length\": len(question.split()),\n",
    "            \"num_source_triples\": len(triples)\n",
    "        }\n",
    "        \n",
    "        # Filter 1: Length requirements\n",
    "        if not self.passes_length_filter(question, answer):\n",
    "            result = FilteringResult(\n",
    "                accepted=False,\n",
    "                decision=FilteringDecision.REJECTED_LENGTH,\n",
    "                relevance_score=relevance_score,\n",
    "                triple_similarity=triple_sim,\n",
    "                reasoning=f\"Failed length requirements: Q={len(question.split())} words, A={len(answer.split())} words\",\n",
    "                metadata=base_metadata\n",
    "            )\n",
    "            \n",
    "            self.generation_stats[\"filtering_decisions\"][\"rejected_length\"] += 1\n",
    "            self.generation_stats[\"all_attempts\"].append({\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"filtering_result\": self._filtering_result_to_dict(result),\n",
    "                \"relevance_score\": relevance_score\n",
    "            })\n",
    "            \n",
    "            return False, result\n",
    "        \n",
    "        # Filter 2: Duplicate detection \n",
    "        is_duplicate, question_embedding = self.is_semantic_duplicate(question)\n",
    "        if is_duplicate:\n",
    "            result = FilteringResult(\n",
    "                accepted=False,\n",
    "                decision=FilteringDecision.REJECTED_DUPLICATE,\n",
    "                relevance_score=relevance_score,\n",
    "                triple_similarity=triple_sim,\n",
    "                reasoning=\"Semantic duplicate detected\",\n",
    "                metadata=base_metadata\n",
    "            )\n",
    "            \n",
    "            self.generation_stats[\"filtering_decisions\"][\"rejected_duplicate\"] += 1\n",
    "            self.generation_stats[\"all_attempts\"].append({\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"filtering_result\": self._filtering_result_to_dict(result),\n",
    "                \"relevance_score\": relevance_score\n",
    "            })\n",
    "            \n",
    "            return False, result\n",
    "        \n",
    "        # Accept QA pair\n",
    "        result = FilteringResult(\n",
    "            accepted=True,\n",
    "            decision=FilteringDecision.ACCEPTED,\n",
    "            relevance_score=relevance_score,\n",
    "            triple_similarity=triple_sim,\n",
    "            reasoning=f\"Passed basic quality filters (relevance: {relevance_score:.3f})\",\n",
    "            metadata=base_metadata\n",
    "        )\n",
    "        \n",
    "        # Register accepted question\n",
    "        self.all_questions.add(question)\n",
    "        self.question_embeddings[question] = question_embedding\n",
    "        self.batch_questions.add(question)\n",
    "        self.batch_embeddings[question] = question_embedding\n",
    "        \n",
    "        self._manage_embedding_cache()\n",
    "        \n",
    "        # Update statistics\n",
    "        self.generation_stats[\"successful_generations\"] += 1\n",
    "        self.generation_stats[\"filtering_decisions\"][\"accepted\"] += 1\n",
    "        \n",
    "        self.generation_stats[\"all_attempts\"].append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"filtering_result\": self._filtering_result_to_dict(result),\n",
    "            \"relevance_score\": relevance_score\n",
    "        })\n",
    "        \n",
    "        return True, result\n",
    "\n",
    "    def load_triples_data(self, triples_file: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Load triples data only (no chunks file required for this ablation).\n",
    "        \n",
    "        Args:\n",
    "            triples_file: Path to CSV file containing triples\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping chunk_id to triples\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load triples data (only file needed)\n",
    "            triples_df = pd.read_csv(triples_file)\n",
    "            logger.info(f\"Loaded {len(triples_df)} triples from {triples_file}\")\n",
    "            logger.info(f\"Triples columns: {list(triples_df.columns)}\")\n",
    "            \n",
    "            # Check for required columns\n",
    "            if 'chunk_id' not in triples_df.columns:\n",
    "                raise ValueError(\"triples_file must contain 'chunk_id' column\")\n",
    "            \n",
    "            # Try to identify triple columns\n",
    "            triple_columns = None\n",
    "            possible_combinations = [\n",
    "                ('subject', 'predicate', 'object'),\n",
    "                ('node1', 'edge', 'node2'),\n",
    "                ('head', 'relation', 'tail'),\n",
    "                ('entity1', 'relationship', 'entity2')\n",
    "            ]\n",
    "            \n",
    "            for combo in possible_combinations:\n",
    "                if all(col in triples_df.columns for col in combo):\n",
    "                    triple_columns = combo\n",
    "                    break\n",
    "            \n",
    "            if triple_columns is None:\n",
    "                available_cols = list(triples_df.columns)\n",
    "                raise ValueError(f\"Could not identify triple columns. Available columns: {available_cols}. \"\n",
    "                               f\"Expected one of: {possible_combinations}\")\n",
    "            \n",
    "            logger.info(f\"Using triple columns: {triple_columns}\")\n",
    "            \n",
    "            # Group by chunk_id (KG-only approach)\n",
    "            grouped_data = {}\n",
    "            for chunk_id, group in triples_df.groupby('chunk_id'):\n",
    "                grouped_data[chunk_id] = {\n",
    "                    'text': \"\",  # Empty string - not used in this ablation\n",
    "                    'triples': [(row[triple_columns[0]], row[triple_columns[1]], row[triple_columns[2]]) \n",
    "                              for _, row in group.iterrows()]\n",
    "                }\n",
    "            \n",
    "            logger.info(f\"Processed {len(grouped_data)} unique chunks from triples data\")\n",
    "            return grouped_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading triples data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_kg_only_prompt(self, triples: List[Tuple], question_type: QuestionType, num_questions: int) -> str:\n",
    "        \"\"\"Generate prompt using only KG triples (no text context).\"\"\"\n",
    "        template = KG_ONLY_QUESTION_TEMPLATES.get(question_type, {})\n",
    "        task_description = template.get(\"task_description\", \"\")\n",
    "        generation_guidance = template.get(\"generation_guidance\", \"\")\n",
    "        \n",
    "        # Get the exemplar question for this type\n",
    "        exemplar_question = KG_ONLY_EXEMPLAR[\"exemplar_questions\"][question_type][\"question\"]\n",
    "        \n",
    "        # Format current triples\n",
    "        formatted_triples = []\n",
    "        for triple in triples[:25]:  # Limit to prevent prompt overflow but allow more than chunks\n",
    "            try:\n",
    "                if len(triple) >= 3:\n",
    "                    n1, e, n2 = str(triple[0]), str(triple[1]), str(triple[2])\n",
    "                    formatted_triples.append(f\"- {n1} → '{e}' → {n2}\")\n",
    "            except (IndexError, TypeError):\n",
    "                continue\n",
    "        \n",
    "        triple_text = \"\\n\".join(formatted_triples) if formatted_triples else \"No valid triples available\"\n",
    "    \n",
    "        # Format exemplar triples\n",
    "        exemplar_formatted_triples = []\n",
    "        for triple in KG_ONLY_EXEMPLAR[\"knowledge_graph\"]:\n",
    "            n1, e, n2 = str(triple[0]), str(triple[1]), str(triple[2])\n",
    "            exemplar_formatted_triples.append(f\"- {n1} → '{e}' → {n2}\")\n",
    "        exemplar_triple_text = \"\\n\".join(exemplar_formatted_triples)\n",
    "    \n",
    "        # Create KG-only prompt (no text context references)\n",
    "        prompt = f\"\"\"\n",
    "ABLATION STUDY: KG-ONLY QA GENERATION TASK\n",
    "\n",
    "TASK TYPE: {question_type.value.upper()}\n",
    "{task_description}\n",
    "\n",
    "EXEMPLAR DEMONSTRATION:\n",
    "\n",
    "EXAMPLE KNOWLEDGE GRAPH TRIPLES:\n",
    "{exemplar_triple_text}\n",
    "\n",
    "EXAMPLE {question_type.value.upper()} QUESTION:\n",
    "{exemplar_question}\n",
    "\n",
    "NOW GENERATE FOR NEW KNOWLEDGE GRAPH:\n",
    "\n",
    "TARGET KNOWLEDGE GRAPH TRIPLES:\n",
    "{triple_text}\n",
    "\n",
    "GENERATION INSTRUCTIONS: {generation_guidance}\n",
    "\n",
    "IMPORTANT: Generate questions and answers based ONLY on the knowledge graph triples provided. \n",
    "Do not reference or require additional text context information.\n",
    "\n",
    "DIVERSITY REQUIREMENTS:\n",
    "- Each question must be UNIQUE and ask about DIFFERENT entities or relationships\n",
    "- Use VARIED question starters and phrasing patterns  \n",
    "- Focus on DIFFERENT triples, entity connections, or relationship patterns\n",
    "- Avoid repetitive structures or similar wordings\n",
    "- Make each question distinctly different from others and from the exemplar\n",
    "\n",
    "REQUIRED OUTPUT FORMAT:\n",
    "[\n",
    "  {{\n",
    "    \"id\": \"1\",\n",
    "    \"question\": \"Your detailed question here?\",\n",
    "    \"answer\": \"Your comprehensive answer here.\",\n",
    "    \"type\": \"{question_type.value}\"\n",
    "  }}\n",
    "]\n",
    "\n",
    "Generate {num_questions} {question_type.value} questions with answers based solely on the knowledge graph triples.\n",
    "\"\"\"\n",
    "        return prompt.strip()\n",
    "    \n",
    "    def parse_json_response(self, text_response: str) -> List[Dict]:\n",
    "        \"\"\"Parse JSON response from GPT-4 (unchanged from original).\"\"\"\n",
    "        text_response = text_response.strip()\n",
    "        if text_response.startswith('```json'):\n",
    "            text_response = text_response[7:]\n",
    "        if text_response.endswith('```'):\n",
    "            text_response = text_response[:-3]\n",
    "        text_response = text_response.strip()\n",
    "        \n",
    "        try:\n",
    "            parsed = json.loads(text_response)\n",
    "            if isinstance(parsed, list):\n",
    "                return parsed\n",
    "            elif isinstance(parsed, dict) and \"question\" in parsed:\n",
    "                return [parsed]\n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.debug(f\"Initial JSON parse failed: {e}\")\n",
    "        \n",
    "        # Fallback parsing strategies\n",
    "        try:\n",
    "            json_start = text_response.find('[')\n",
    "            json_end = text_response.rfind(']') + 1\n",
    "            if json_start >= 0 and json_end > json_start:\n",
    "                json_str = text_response[json_start:json_end]\n",
    "                parsed = json.loads(json_str)\n",
    "                if isinstance(parsed, list):\n",
    "                    return parsed\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Array extraction failed: {e}\")\n",
    "        \n",
    "        try:\n",
    "            json_start = text_response.find('{')\n",
    "            json_end = text_response.rfind('}') + 1\n",
    "            if json_start >= 0 and json_end > json_start:\n",
    "                json_str = text_response[json_start:json_end]\n",
    "                parsed = json.loads(json_str)\n",
    "                if isinstance(parsed, dict) and \"question\" in parsed:\n",
    "                    return [parsed]\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Object extraction failed: {e}\")\n",
    "        \n",
    "        logger.warning(f\"Failed to parse JSON response: {text_response[:200]}...\")\n",
    "        return []\n",
    "\n",
    "    def call_openai_api(self, system_message: str, user_message: str, max_tokens: int = 4000, temperature: float = 0.7) -> str:\n",
    "        \"\"\"Call OpenAI API (unchanged from original).\"\"\"\n",
    "        try:\n",
    "            temperature = min(max(temperature, 0.0), 1.0)\n",
    "            \n",
    "            if OPENAI_V1:\n",
    "                response = self.openai_client.chat.completions.create(\n",
    "                    model=\"gpt-4-turbo-preview\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_message},\n",
    "                        {\"role\": \"user\", \"content\": user_message}\n",
    "                    ],\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=temperature,\n",
    "                    presence_penalty=0.3,\n",
    "                    frequency_penalty=0.3\n",
    "                )\n",
    "                return response.choices[0].message.content.strip()\n",
    "            else:\n",
    "                response = openai.ChatCompletion.create(\n",
    "                    model=\"gpt-4-turbo-preview\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_message},\n",
    "                        {\"role\": \"user\", \"content\": user_message}\n",
    "                    ],\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=temperature,\n",
    "                    presence_penalty=0.3,\n",
    "                    frequency_penalty=0.3\n",
    "                )\n",
    "                return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"OpenAI API call failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_qa_pairs(self, \n",
    "                         triples: List[Tuple], \n",
    "                         question_type: QuestionType, \n",
    "                         num_questions: int,\n",
    "                         chunk_id: str) -> List[QAPair]:\n",
    "        \"\"\"Generate QA pairs using KG-only approach (no text context).\"\"\"\n",
    "        qa_pairs = []\n",
    "        max_retries = 3\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                self.generation_stats[\"total_api_calls\"] += 1\n",
    "                \n",
    "                prompt = self.generate_kg_only_prompt(triples, question_type, num_questions)\n",
    "                \n",
    "                system_message = \"\"\"You are an expert at generating domain-specific QA pairs using knowledge graph triples only (no text context required). \n",
    "\n",
    "You have been provided with a high-quality exemplar that demonstrates the expected format and quality for each question type. Use this exemplar as a guide to generate similar high-quality questions for the new knowledge graph provided.\n",
    "\n",
    "Generate questions that can be answered using ONLY the information present in the knowledge graph triples. Focus on entities, relationships, and graph structure patterns.\n",
    "\n",
    "Your output MUST be valid JSON in the exact format specified with only id, question, answer, and type fields.\"\"\"\n",
    "\n",
    "                logger.debug(f\"Generating {num_questions} {question_type.value} questions (attempt {attempt + 1}) - KG ONLY\")\n",
    "                \n",
    "                temperature = min(0.8 + (attempt * 0.1), 1.0)\n",
    "                \n",
    "                text_response = self.call_openai_api(\n",
    "                    system_message, \n",
    "                    prompt, \n",
    "                    temperature=temperature\n",
    "                )\n",
    "                \n",
    "                parsed_items = self.parse_json_response(text_response)\n",
    "                \n",
    "                if parsed_items:\n",
    "                    logger.info(f\"Successfully parsed {len(parsed_items)} QA pairs from KG-only response\")\n",
    "                    \n",
    "                    for item in parsed_items:\n",
    "                        q_text = item.get(\"question\", \"\").strip()\n",
    "                        a_text = item.get(\"answer\", \"\").strip()\n",
    "                        qa_metadata = {}  # Empty dict since simplified format has no metadata\n",
    "\n",
    "                        if q_text and a_text:\n",
    "                            # Apply filtering (only triples passed)\n",
    "                            accepted, filtering_result = self.filter_qa_pair(\n",
    "                                q_text, a_text, triples, qa_metadata\n",
    "                            )\n",
    "                            \n",
    "                            if accepted:\n",
    "                                unique_id = f\"{question_type.value}_kg_only_{chunk_id}_{int(time.time())}_{str(uuid.uuid4())[:8]}\"\n",
    "                                qa_pair = QAPair(\n",
    "                                    id=unique_id,\n",
    "                                    question=q_text,\n",
    "                                    answer=a_text,\n",
    "                                    question_type=question_type,\n",
    "                                    qa_metadata=qa_metadata,\n",
    "                                    filtering_result=filtering_result,\n",
    "                                    generation_method=\"ablation_kg_only\",\n",
    "                                    source_triples=triples.copy(),\n",
    "                                    chunk_id=chunk_id\n",
    "                                )\n",
    "                                \n",
    "                                qa_pairs.append(qa_pair)\n",
    "                                \n",
    "                                logger.debug(f\"Accepted QA pair: {filtering_result.decision.value} \"\n",
    "                                           f\"(relevance={filtering_result.relevance_score:.3f})\")\n",
    "                                \n",
    "                                if len(qa_pairs) >= num_questions:\n",
    "                                    return qa_pairs[:num_questions]\n",
    "                            else:\n",
    "                                logger.debug(f\"Rejected QA pair: {filtering_result.reasoning}\")\n",
    "                        else:\n",
    "                            # Handle parsing failures\n",
    "                            self.generation_stats[\"total_qa_attempts\"] += 1\n",
    "                            self.generation_stats[\"filtering_decisions\"][\"rejected_parsing\"] += 1\n",
    "                            \n",
    "                            dummy_result = FilteringResult(\n",
    "                                accepted=False,\n",
    "                                decision=FilteringDecision.REJECTED_PARSING,\n",
    "                                relevance_score=0.0,\n",
    "                                triple_similarity=0.0,\n",
    "                                reasoning=\"Empty question or answer from parsing\",\n",
    "                                metadata={\"answer_length\": 0, \"question_length\": 0, \"num_source_triples\": 0}\n",
    "                            )\n",
    "                            \n",
    "                            self.generation_stats[\"all_attempts\"].append({\n",
    "                                \"question\": q_text,\n",
    "                                \"answer\": a_text,\n",
    "                                \"filtering_result\": self._filtering_result_to_dict(dummy_result),\n",
    "                                \"relevance_score\": 0.0\n",
    "                            })\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in generation attempt {attempt + 1}: {str(e)}\")\n",
    "                time.sleep(min(2 ** attempt, 10))\n",
    "                continue\n",
    "\n",
    "        return qa_pairs\n",
    "\n",
    "    def create_kg_only_dataset(self, \n",
    "                              merged_grouped: Dict, \n",
    "                              output_file: str, \n",
    "                              limit: Optional[int] = None) -> None:\n",
    "        \"\"\"Create ablation dataset using knowledge graph only (no text context).\"\"\"\n",
    "        logger.info(\"Starting ABLATION STUDY 2: KG-Only QA dataset creation\")\n",
    "        \n",
    "        all_qa_pairs = []\n",
    "        question_types = list(QuestionType)\n",
    "        \n",
    "        # Process chunks\n",
    "        chunks_to_process = list(merged_grouped.items())\n",
    "        if limit:\n",
    "            chunks_to_process = chunks_to_process[:limit]\n",
    "            \n",
    "        total_chunks = len(chunks_to_process)\n",
    "        logger.info(f\"Processing {total_chunks} chunks with KG-only approach\")\n",
    "        \n",
    "        for i, (chunk_id, chunk_data) in enumerate(chunks_to_process, 1):\n",
    "            logger.info(f\"Processing chunk {i}/{total_chunks}: {chunk_id} - ABLATION: KG ONLY\")\n",
    "            \n",
    "            # Reset batch tracking for each chunk if specified\n",
    "            if self.reset_duplicates_per_chunk:\n",
    "                self.batch_questions.clear()\n",
    "                self.batch_embeddings.clear()\n",
    "            \n",
    "            triples = chunk_data['triples']\n",
    "            \n",
    "            if not triples:\n",
    "                logger.warning(f\"No triples found for chunk {chunk_id}, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Generate QA pairs for each question type using KG-only approach\n",
    "            for question_type in question_types:\n",
    "                try:\n",
    "                    logger.debug(f\"Generating {question_type.value} questions for chunk {chunk_id} using KG-only approach\")\n",
    "                    \n",
    "                    qa_pairs = self.generate_qa_pairs(\n",
    "                        triples=triples,\n",
    "                        question_type=question_type,\n",
    "                        num_questions=2,\n",
    "                        chunk_id=chunk_id\n",
    "                    )\n",
    "                    \n",
    "                    all_qa_pairs.extend(qa_pairs)\n",
    "                    logger.info(f\"Generated {len(qa_pairs)} {question_type.value} QA pairs for chunk {chunk_id}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error generating {question_type.value} questions for chunk {chunk_id}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Add delay between chunks to avoid rate limiting\n",
    "            if i < total_chunks:\n",
    "                time.sleep(2)\n",
    "        \n",
    "        # Get embedding model name safely\n",
    "        try:\n",
    "            if hasattr(self.embedding_model, 'model_name'):\n",
    "                embedding_model_name = self.embedding_model.model_name\n",
    "            elif hasattr(self.embedding_model, '_model_name'):\n",
    "                embedding_model_name = self.embedding_model._model_name\n",
    "            elif hasattr(self.embedding_model, 'config') and hasattr(self.embedding_model.config, 'name_or_path'):\n",
    "                embedding_model_name = self.embedding_model.config.name_or_path\n",
    "            else:\n",
    "                try:\n",
    "                    embedding_model_name = self.embedding_model._modules['0'].auto_model.config.name_or_path\n",
    "                except:\n",
    "                    embedding_model_name = 'BAAI/bge-large-en-v1.5'\n",
    "        except:\n",
    "            embedding_model_name = 'BAAI/bge-large-en-v1.5'\n",
    "        \n",
    "        # Calculate question type statistics\n",
    "        question_type_stats = {}\n",
    "        question_type_quality = {}\n",
    "        \n",
    "        for qa_pair in all_qa_pairs:\n",
    "            q_type = qa_pair.question_type.value\n",
    "            \n",
    "            # Count by type\n",
    "            if q_type not in question_type_stats:\n",
    "                question_type_stats[q_type] = {\n",
    "                    \"count\": 0,\n",
    "                    \"avg_relevance_score\": 0.0,\n",
    "                    \"avg_question_length\": 0.0,\n",
    "                    \"avg_answer_length\": 0.0,\n",
    "                    \"avg_triple_similarity\": 0.0\n",
    "                }\n",
    "                question_type_quality[q_type] = {\n",
    "                    \"relevance_scores\": [],\n",
    "                    \"question_lengths\": [],\n",
    "                    \"answer_lengths\": [],\n",
    "                    \"triple_similarities\": []\n",
    "                }\n",
    "            \n",
    "            question_type_stats[q_type][\"count\"] += 1\n",
    "            \n",
    "            # Collect quality metrics\n",
    "            question_type_quality[q_type][\"relevance_scores\"].append(qa_pair.filtering_result.relevance_score)\n",
    "            question_type_quality[q_type][\"question_lengths\"].append(len(qa_pair.question.split()))\n",
    "            question_type_quality[q_type][\"answer_lengths\"].append(len(qa_pair.answer.split()))\n",
    "            question_type_quality[q_type][\"triple_similarities\"].append(qa_pair.filtering_result.triple_similarity)\n",
    "        \n",
    "        # Calculate averages for each question type\n",
    "        for q_type in question_type_stats:\n",
    "            metrics = question_type_quality[q_type]\n",
    "            question_type_stats[q_type][\"avg_relevance_score\"] = np.mean(metrics[\"relevance_scores\"]) if metrics[\"relevance_scores\"] else 0.0\n",
    "            question_type_stats[q_type][\"avg_question_length\"] = np.mean(metrics[\"question_lengths\"]) if metrics[\"question_lengths\"] else 0.0\n",
    "            question_type_stats[q_type][\"avg_answer_length\"] = np.mean(metrics[\"answer_lengths\"]) if metrics[\"answer_lengths\"] else 0.0\n",
    "            question_type_stats[q_type][\"avg_triple_similarity\"] = np.mean(metrics[\"triple_similarities\"]) if metrics[\"triple_similarities\"] else 0.0\n",
    "\n",
    "        # Create final dataset structure with metadata\n",
    "        dataset = {\n",
    "            \"metadata\": {\n",
    "                \"creation_date\": pd.Timestamp.now().isoformat(),\n",
    "                \"total_queries\": len(all_qa_pairs),\n",
    "                \"generation_method\": \"ablation_kg_only\",\n",
    "                \"model_version\": \"Ablation 2.0\",\n",
    "                \"embedding_model\": embedding_model_name,\n",
    "                \"filtering_approach\": \"essential_quality_filters_kg_only\",\n",
    "                \"ablation_study\": {\n",
    "                    \"study_number\": 2,\n",
    "                    \"study_name\": \"KG Only (No Text Context)\",\n",
    "                    \"removed_components\": [\"text_context\", \"context_similarity_scoring\"],\n",
    "                    \"retained_components\": [\"triple_similarity\", \"length_filtering\", \"duplicate_detection\"],\n",
    "                    \"purpose\": \"Evaluate contribution of text context vs. knowledge graph information to QA quality\"\n",
    "                },\n",
    "                \"exemplar_info\": {\n",
    "                    \"num_exemplar_triples\": len(KG_ONLY_EXEMPLAR[\"knowledge_graph\"]),\n",
    "                    \"exemplar_question_types\": [q_type.value for q_type in KG_ONLY_EXEMPLAR[\"exemplar_questions\"].keys()],\n",
    "                    \"text_context_used\": False\n",
    "                },\n",
    "                \"quality_thresholds\": {\n",
    "                    \"min_question_words\": self.thresholds.min_question_words,\n",
    "                    \"min_answer_words\": self.thresholds.min_answer_words,\n",
    "                    \"duplicate_similarity_threshold\": self.thresholds.duplicate_similarity_threshold,\n",
    "                    \"batch_similarity_threshold\": self.thresholds.batch_similarity_threshold\n",
    "                },\n",
    "                \"generation_statistics\": self.get_filtering_statistics(),\n",
    "                \"question_type_breakdown\": question_type_stats,\n",
    "                \"processing_summary\": {\n",
    "                    \"chunks_processed\": total_chunks,\n",
    "                    \"questions_per_chunk\": 2 * len(question_types),\n",
    "                    \"question_types_generated\": list(question_type_stats.keys()),\n",
    "                    \"overall_quality\": {\n",
    "                        \"avg_relevance_score\": np.mean([qa.filtering_result.relevance_score for qa in all_qa_pairs]) if all_qa_pairs else 0.0,\n",
    "                        \"avg_question_length\": np.mean([len(qa.question.split()) for qa in all_qa_pairs]) if all_qa_pairs else 0.0,\n",
    "                        \"avg_answer_length\": np.mean([len(qa.answer.split()) for qa in all_qa_pairs]) if all_qa_pairs else 0.0,\n",
    "                        \"avg_triple_similarity\": np.mean([qa.filtering_result.triple_similarity for qa in all_qa_pairs]) if all_qa_pairs else 0.0\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"queries\": []\n",
    "        }\n",
    "        \n",
    "        # Convert QA pairs to dictionary format\n",
    "        for qa_pair in all_qa_pairs:\n",
    "            query_dict = {\n",
    "                \"id\": qa_pair.id,\n",
    "                \"question\": qa_pair.question,\n",
    "                \"answer\": qa_pair.answer,\n",
    "                \"question_type\": qa_pair.question_type.value,\n",
    "                \"qa_metadata\": qa_pair.qa_metadata,\n",
    "                \"filtering_result\": {\n",
    "                    \"accepted\": qa_pair.filtering_result.accepted,\n",
    "                    \"decision\": qa_pair.filtering_result.decision.value,\n",
    "                    \"relevance_score\": qa_pair.filtering_result.relevance_score,\n",
    "                    \"triple_similarity\": qa_pair.filtering_result.triple_similarity,\n",
    "                    \"reasoning\": qa_pair.filtering_result.reasoning,\n",
    "                    \"metadata\": qa_pair.filtering_result.metadata\n",
    "                },\n",
    "                \"generation_method\": qa_pair.generation_method,\n",
    "                # Add ground truth information for evaluation\n",
    "                \"ground_truth\": {\n",
    "                    \"source_triples\": [\n",
    "                        {\n",
    "                            \"subject\": triple[0],\n",
    "                            \"predicate\": triple[1], \n",
    "                            \"object\": triple[2]\n",
    "                        } for triple in qa_pair.source_triples if len(triple) >= 3\n",
    "                    ],\n",
    "                    \"chunk_id\": qa_pair.chunk_id,\n",
    "                    \"num_source_triples\": len(qa_pair.source_triples),\n",
    "                    \"text_context_used\": False,  # Key difference from original\n",
    "                    \"ablation_study\": \"kg_only\"\n",
    "                }\n",
    "            }\n",
    "            dataset[\"queries\"].append(query_dict)\n",
    "        \n",
    "        # Save dataset\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(dataset, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        logger.info(f\"ABLATION STUDY 2: KG-only dataset creation completed!\")\n",
    "        logger.info(f\"Total QA pairs generated: {len(all_qa_pairs)}\")\n",
    "        logger.info(f\"Dataset saved to: {output_file}\")\n",
    "        \n",
    "        # Log detailed statistics\n",
    "        stats = self.get_filtering_statistics()\n",
    "        logger.info(\"\\nAblation Study 2 - KG-Only Filtering Statistics:\")\n",
    "        logger.info(f\"Total API calls: {stats.get('total_api_calls', 0)}\")\n",
    "        logger.info(f\"Total QA attempts: {stats.get('total_qa_attempts', 0)}\")\n",
    "        logger.info(f\"Successful generations: {stats.get('successful_generations', 0)}\")\n",
    "        \n",
    "        logger.info(\"\\nFiltering Decision Breakdown:\")\n",
    "        for decision_type, count in stats[\"filtering_decisions\"].items():\n",
    "            if count > 0:\n",
    "                percentage = (count / stats['total_qa_attempts']) * 100 if stats['total_qa_attempts'] > 0 else 0\n",
    "                logger.info(f\"  {decision_type}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        if \"acceptance_rate\" in stats:\n",
    "            logger.info(f\"\\nOverall Acceptance Rate: {stats['acceptance_rate']:.1f}%\")\n",
    "        \n",
    "        # Log question type breakdown\n",
    "        logger.info(\"\\nQuestion Type Breakdown:\")\n",
    "        for q_type, type_stats in question_type_stats.items():\n",
    "            count = type_stats[\"count\"]\n",
    "            avg_relevance = type_stats[\"avg_relevance_score\"]\n",
    "            percentage = (count / len(all_qa_pairs)) * 100 if all_qa_pairs else 0\n",
    "            logger.info(f\"  {q_type}: {count} questions ({percentage:.1f}%) - avg relevance: {avg_relevance:.3f}\")\n",
    "        \n",
    "        # Validation check\n",
    "        total_decisions = sum(stats[\"filtering_decisions\"].values())\n",
    "        if total_decisions == stats[\"total_qa_attempts\"]:\n",
    "            logger.info(\"\\nStatistics validation: PASSED\")\n",
    "        else:\n",
    "            logger.warning(f\"\\nStatistics validation: FAILED ({total_decisions} decisions vs {stats['total_qa_attempts']} attempts)\")\n",
    "        \n",
    "        # Ablation-specific logging\n",
    "        logger.info(\"\\nABLATION STUDY 2 SUMMARY:\")\n",
    "        logger.info(\" Removed: Text context and context similarity scoring\")\n",
    "        logger.info(\" Retained: Knowledge graph triples and triple similarity\")\n",
    "        logger.info(\" Purpose: Evaluate text context contribution to QA generation quality\")\n",
    "\n",
    "    def get_filtering_statistics(self) -> Dict:\n",
    "        \"\"\"Get comprehensive statistics about filtering decisions.\"\"\"\n",
    "        stats = self.generation_stats.copy()\n",
    "        \n",
    "        # Calculate acceptance rate based on QA pairs processed\n",
    "        total_qa_attempts = stats[\"total_qa_attempts\"]\n",
    "        if total_qa_attempts > 0:\n",
    "            acceptance_rate = (stats[\"successful_generations\"] / total_qa_attempts) * 100\n",
    "            stats[\"acceptance_rate\"] = acceptance_rate\n",
    "        else:\n",
    "            stats[\"acceptance_rate\"] = 0.0\n",
    "        \n",
    "        # Add validation check\n",
    "        total_decisions = sum(stats[\"filtering_decisions\"].values())\n",
    "        if total_decisions != total_qa_attempts:\n",
    "            logger.warning(f\"Statistics mismatch: {total_decisions} decisions vs {total_qa_attempts} attempts\")\n",
    "            stats[\"validation_status\"] = \"FAILED\"\n",
    "        else:\n",
    "            stats[\"validation_status\"] = \"PASSED\"\n",
    "        \n",
    "        return stats\n",
    "\n",
    "\n",
    "# Analysis and utility functions for ablation study\n",
    "\n",
    "def analyze_kg_ablation_results(dataset_path: str, generator_stats: Optional[Dict] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze the results of ablation study 2 (KG-only).\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to the generated ablation dataset\n",
    "        generator_stats: Optional generator statistics for complete analysis\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "            dataset = json.load(f)\n",
    "        \n",
    "        queries = dataset.get(\"queries\", [])\n",
    "        metadata = dataset.get(\"metadata\", {})\n",
    "        \n",
    "        if not queries:\n",
    "            return {\"error\": \"No queries found in dataset\"}\n",
    "        \n",
    "        # Get ablation-specific information\n",
    "        ablation_info = metadata.get(\"ablation_study\", {})\n",
    "        generation_stats = metadata.get(\"generation_statistics\", {})\n",
    "        if generator_stats:\n",
    "            generation_stats = generator_stats\n",
    "        \n",
    "        # Analyze accepted queries\n",
    "        accepted_count = len(queries)\n",
    "        \n",
    "        # Calculate quality metrics for accepted queries\n",
    "        quality_metrics = {\n",
    "            \"relevance_scores\": [],\n",
    "            \"question_lengths\": [],\n",
    "            \"answer_lengths\": [],\n",
    "            \"triple_similarities\": []\n",
    "        }\n",
    "        \n",
    "        question_type_distribution = {}\n",
    "        \n",
    "        for query in queries:\n",
    "            filtering_info = query.get(\"filtering_result\", {})\n",
    "            quality_metrics[\"relevance_scores\"].append(filtering_info.get(\"relevance_score\", 0.0))\n",
    "            quality_metrics[\"question_lengths\"].append(len(query[\"question\"].split()))\n",
    "            quality_metrics[\"answer_lengths\"].append(len(query[\"answer\"].split()))\n",
    "            quality_metrics[\"triple_similarities\"].append(filtering_info.get(\"triple_similarity\", 0.0))\n",
    "            \n",
    "            # Count question types\n",
    "            q_type = query.get(\"question_type\", \"unknown\")\n",
    "            question_type_distribution[q_type] = question_type_distribution.get(q_type, 0) + 1\n",
    "        \n",
    "        # Use generation statistics for complete picture\n",
    "        total_attempts = generation_stats.get(\"total_qa_attempts\", accepted_count)\n",
    "        rejected_count = total_attempts - accepted_count\n",
    "        \n",
    "        # Calculate comprehensive statistics\n",
    "        analysis_results = {\n",
    "            \"ablation_study_info\": ablation_info,\n",
    "            \"total_qa_attempts\": total_attempts,\n",
    "            \"final_dataset_queries\": accepted_count,\n",
    "            \"decision_distribution\": {\n",
    "                \"accepted_count\": accepted_count,\n",
    "                \"rejected_count\": rejected_count,\n",
    "                \"acceptance_rate\": (accepted_count / total_attempts) * 100 if total_attempts > 0 else 0\n",
    "            },\n",
    "            \"detailed_filtering_breakdown\": generation_stats.get(\"filtering_decisions\", {}),\n",
    "            \"question_type_distribution\": question_type_distribution,\n",
    "            \"quality_statistics\": {\n",
    "                \"avg_relevance_score\": np.mean(quality_metrics[\"relevance_scores\"]) if quality_metrics[\"relevance_scores\"] else 0,\n",
    "                \"min_relevance_score\": min(quality_metrics[\"relevance_scores\"]) if quality_metrics[\"relevance_scores\"] else 0,\n",
    "                \"max_relevance_score\": max(quality_metrics[\"relevance_scores\"]) if quality_metrics[\"relevance_scores\"] else 0,\n",
    "                \"std_relevance_score\": np.std(quality_metrics[\"relevance_scores\"]) if quality_metrics[\"relevance_scores\"] else 0,\n",
    "                \"avg_question_length\": np.mean(quality_metrics[\"question_lengths\"]) if quality_metrics[\"question_lengths\"] else 0,\n",
    "                \"avg_answer_length\": np.mean(quality_metrics[\"answer_lengths\"]) if quality_metrics[\"answer_lengths\"] else 0,\n",
    "                \"avg_triple_similarity\": np.mean(quality_metrics[\"triple_similarities\"]) if quality_metrics[\"triple_similarities\"] else 0\n",
    "            },\n",
    "            \"generation_efficiency\": {\n",
    "                \"api_calls\": generation_stats.get(\"total_api_calls\", 0),\n",
    "                \"qa_per_api_call\": accepted_count / generation_stats.get(\"total_api_calls\", 1),\n",
    "                \"acceptance_rate\": generation_stats.get(\"acceptance_rate\", 0)\n",
    "            },\n",
    "            \"ablation_specific_metrics\": {\n",
    "                \"context_components_removed\": ablation_info.get(\"removed_components\", []),\n",
    "                \"kg_components_retained\": ablation_info.get(\"retained_components\", []),\n",
    "                \"kg_only_approach\": True,\n",
    "                \"context_similarity_available\": False\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return analysis_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error analyzing KG ablation results: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "def generate_kg_ablation_report(dataset_path: str, output_report_path: str, generator_stats: Optional[Dict] = None) -> None:\n",
    "    \"\"\"\n",
    "    Generate a comprehensive report on ablation study 2 performance.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to the ablation study dataset\n",
    "        output_report_path: Path to save the ablation report\n",
    "        generator_stats: Optional generator statistics for complete analysis\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Analyze with complete statistics\n",
    "        analysis = analyze_kg_ablation_results(dataset_path, generator_stats)\n",
    "        \n",
    "        if \"error\" in analysis:\n",
    "            logger.error(f\"Analysis failed: {analysis['error']}\")\n",
    "            return\n",
    "        \n",
    "        # Generate comprehensive report\n",
    "        report = {\n",
    "            \"report_metadata\": {\n",
    "                \"generation_date\": pd.Timestamp.now().isoformat(),\n",
    "                \"dataset_analyzed\": dataset_path,\n",
    "                \"report_type\": \"Ablation_Study_2_Analysis\",\n",
    "                \"approach\": \"KG-only generation (no text context)\"\n",
    "            },\n",
    "            \"ablation_study_details\": {\n",
    "                \"study_number\": 2,\n",
    "                \"study_name\": \"KG Only (No Text Context)\",\n",
    "                \"hypothesis\": \"Text context significantly improves QA generation quality beyond KG structure\",\n",
    "                \"removed_components\": analysis.get(\"ablation_study_info\", {}).get(\"removed_components\", []),\n",
    "                \"retained_components\": analysis.get(\"ablation_study_info\", {}).get(\"retained_components\", []),\n",
    "                \"expected_impact\": \"Questions may lack nuanced understanding without contextual text information\"\n",
    "            },\n",
    "            \"executive_summary\": {\n",
    "                \"total_qa_attempts\": analysis.get(\"total_qa_attempts\", 0),\n",
    "                \"final_questions_generated\": analysis.get(\"final_dataset_queries\", 0),\n",
    "                \"overall_acceptance_rate\": f\"{analysis.get('decision_distribution', {}).get('acceptance_rate', 0):.1f}%\",\n",
    "                \"avg_relevance_score\": f\"{analysis.get('quality_statistics', {}).get('avg_relevance_score', 0):.3f}\",\n",
    "                \"triple_similarity_only\": True,\n",
    "                \"api_efficiency\": f\"{analysis.get('generation_efficiency', {}).get('qa_per_api_call', 0):.1f} QA pairs per API call\"\n",
    "            },\n",
    "            \"detailed_analysis\": analysis,\n",
    "            \"comparison_baseline\": {\n",
    "                \"note\": \"This is ablation study 2. Compare results with full one-shot model and study 1 to assess text context contribution.\",\n",
    "                \"key_differences\": [\n",
    "                    \"No text context used in generation\",\n",
    "                    \"No context similarity scoring in relevance calculation\",\n",
    "                    \"KG-only exemplar guidance\",\n",
    "                    \"Simplified relevance scoring (triple similarity only)\"\n",
    "                ]\n",
    "            },\n",
    "            \"expected_findings\": [\n",
    "                \"Questions may be more abstract without contextual grounding\",\n",
    "                \"Factual questions may be more entity-focused rather than content-specific\",\n",
    "                \"Answer quality may depend heavily on KG completeness\",\n",
    "                \"Generation may produce more formal, structured questions\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Save report\n",
    "        with open(output_report_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        logger.info(f\"Ablation study 2 report generated: {output_report_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating KG ablation report: {e}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function for ablation study 2: KG-only QA generation.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting ABLATION STUDY 2: KG-Only QA Generation\")\n",
    "    logger.info(\"Purpose: Evaluate contribution of text context vs. knowledge graph information to QA quality\")\n",
    "    \n",
    "    try:\n",
    "        # Get API key\n",
    "        openai_api_key = input(\"Please enter your OpenAI API key: \").strip()\n",
    "        \n",
    "        if not openai_api_key or openai_api_key == \"your-openai-api-key-here\":\n",
    "            logger.error(\"Please provide a valid OpenAI API key\")\n",
    "            return\n",
    "        \n",
    "        # Initialize generator for ablation study\n",
    "        generator = KGOnlyQAGenerator(\n",
    "            openai_api_key=openai_api_key,\n",
    "            reset_duplicates_per_chunk=True,\n",
    "            embedding_model='BAAI/bge-large-en-v1.5',\n",
    "            max_embedding_cache=1000\n",
    "        )\n",
    "\n",
    "        # File paths - only triples file needed for KG-only study\n",
    "        triples_file = input(\"Enter path to triples CSV file (or press Enter for 'Ontology_Guided_Triples.csv'): \").strip() or \"Ontology_Guided_Triples.csv\"\n",
    "        output_file = input(\"Enter output file name (or press Enter for 'Ablation_2_kg_only_qa_dataset.json'): \").strip() or \"Ablation_2_kg_only_qa_dataset.json\"\n",
    "\n",
    "        if not os.path.exists(triples_file):\n",
    "            logger.error(f\"Triples file not found: {triples_file}\")\n",
    "            logger.info(\"Please ensure your triples file exists and has the correct path\")\n",
    "            return\n",
    "\n",
    "        # Load triples data only (no chunks file needed)\n",
    "        logger.info(\"Loading triples data for KG-only ablation study\")\n",
    "        merged_data = generator.load_triples_data(triples_file)\n",
    "\n",
    "        if not merged_data:\n",
    "            logger.error(\"No data loaded. Please check your input files.\")\n",
    "            return\n",
    "        \n",
    "        # Get user input for number of chunks to process\n",
    "        try:\n",
    "            limit = input(\"Enter number of chunks to process (or press Enter for all): \").strip()\n",
    "            limit = int(limit) if limit else None\n",
    "        except ValueError:\n",
    "            limit = None\n",
    "            logger.info(\"Using default: processing all chunks\")\n",
    "        \n",
    "        if limit is None:\n",
    "            logger.info(\"Processing all chunks for ablation study\")\n",
    "        else:\n",
    "            logger.info(f\"Processing {limit} chunks for ablation study\")\n",
    "        \n",
    "        # Create KG-only dataset\n",
    "        logger.info(f\"Creating Ablation Study 2 dataset (KG-only approach)\")\n",
    "        generator.create_kg_only_dataset(\n",
    "            merged_grouped=merged_data,\n",
    "            output_file=output_file,\n",
    "            limit=limit\n",
    "        )\n",
    "\n",
    "        # Get final statistics from generator\n",
    "        final_stats = generator.get_filtering_statistics()\n",
    "\n",
    "        # Generate analysis report\n",
    "        logger.info(\"Analyzing KG ablation study results\")\n",
    "        analysis_results = analyze_kg_ablation_results(output_file, final_stats)\n",
    "        \n",
    "        if \"error\" in analysis_results:\n",
    "            logger.error(f\"Analysis failed: {analysis_results['error']}\")\n",
    "        else:\n",
    "            logger.info(\"Ablation Study 2 Results:\")\n",
    "            logger.info(f\"  Total QA attempts: {analysis_results.get('total_qa_attempts', 0)}\")\n",
    "            logger.info(f\"  Final questions generated: {analysis_results.get('final_dataset_queries', 0)}\")\n",
    "            logger.info(f\"  Overall acceptance rate: {analysis_results.get('decision_distribution', {}).get('acceptance_rate', 0):.1f}%\")\n",
    "            logger.info(f\"  Average relevance score (KG-only): {analysis_results.get('quality_statistics', {}).get('avg_relevance_score', 0):.3f}\")\n",
    "\n",
    "        # Generate comprehensive report\n",
    "        report_file = \"Ablation_2_kg_only_analysis_report.json\"\n",
    "        generate_kg_ablation_report(output_file, report_file, final_stats)\n",
    "        \n",
    "        # Display final statistics\n",
    "        logger.info(\"\\nFINAL Ablation Study 2 Statistics:\")\n",
    "        logger.info(f\"Total API calls: {final_stats.get('total_api_calls', 0)}\")\n",
    "        logger.info(f\"Total QA attempts: {final_stats.get('total_qa_attempts', 0)}\")\n",
    "        logger.info(f\"Successful generations: {final_stats.get('successful_generations', 0)}\")\n",
    "\n",
    "        logger.info(\"\\nFiltering Decision Breakdown:\")\n",
    "        for decision_type, count in final_stats[\"filtering_decisions\"].items():\n",
    "            if count > 0:\n",
    "                percentage = (count / final_stats['total_qa_attempts']) * 100 if final_stats['total_qa_attempts'] > 0 else 0\n",
    "                logger.info(f\"  {decision_type}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        if \"acceptance_rate\" in final_stats:\n",
    "            logger.info(f\"\\nOverall Acceptance Rate: {final_stats['acceptance_rate']:.1f}%\")\n",
    "        \n",
    "        # Validation check\n",
    "        validation_status = final_stats.get(\"validation_status\", \"UNKNOWN\")\n",
    "        logger.info(f\"Statistics validation: {validation_status}\")\n",
    "        \n",
    "        logger.info(f\"\\nOutputs generated:\")\n",
    "        logger.info(f\"  Ablation Dataset: {output_file}\")\n",
    "        logger.info(f\"  Analysis Report: {report_file}\")\n",
    "        logger.info(\"\\nABLATION STUDY 2 completed successfully!\")\n",
    "        logger.info(\"\\nAblation Study 2 Features:\")\n",
    "        logger.info(\"    Removed: Text context and context similarity\")\n",
    "        logger.info(\"    Retained: Knowledge graph triples and triple similarity\")\n",
    "        logger.info(\"    Purpose: Evaluate text context contribution to QA generation quality\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"\\nProcess interrupted by user\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main execution: {str(e)}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
