{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64468640-235e-4384-83d8-539cd410fb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation Study 1 - Chunks Only Evaluation Analysis\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define file mappings for Ablation Study 1 - Chunks Only (One-Shot only)\n",
    "FILES = {\n",
    "    'Mixtral-8x22B': {\n",
    "        'One-Shot': 'Ablation_Study_1_Chunks_Only_evaluation(mixtral)_results.csv'\n",
    "    },\n",
    "    'Llama-3.3-70B': {\n",
    "        'One-Shot': 'Study_1_Chunks_Only_evaluation_results_Llma3.3).csv'\n",
    "    },\n",
    "    'DeepSeek-R1': {\n",
    "        'One-Shot': 'Ablation_Study_1_Chunks_Only(DeepSeekR1)_evaluation_results.csv'\n",
    "    },\n",
    "    'Qwen3-32B': {\n",
    "        'One-Shot': 'Ablation_Study_1_Chunks_Only(Qwen-Qwen3-32B)_evaluation_results.csv'\n",
    "    },\n",
    "    'Gemma-2-27B-IT': {\n",
    "        'One-Shot': 'Ablation_Study_1_Chunks_Only_Gemma2_evaluation_results.csv'\n",
    "    }\n",
    "}\n",
    "\n",
    "METRICS = ['Relevance', 'Accuracy', 'Completeness', 'Fluency']\n",
    "NUM_JUDGES = 5 # Define the number of judges (5 different LLM models)\n",
    "\n",
    "class AblationStudy1EvaluationAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.df = None\n",
    "        self.majority_vote_df = None\n",
    "        self.model_majority_agreement = {} # New attribute to store per-model agreement\n",
    "\n",
    "    def load_all_data(self):\n",
    "        \"\"\"Load all CSV files and combine them into a single DataFrame.\"\"\"\n",
    "        all_data = []\n",
    "        \n",
    "        for model, shot_types in FILES.items():\n",
    "            for shot_type, filename in shot_types.items():\n",
    "                try:\n",
    "                    df = pd.read_csv(filename)\n",
    "                    df['model'] = model\n",
    "                    df['shot_type'] = shot_type\n",
    "                    all_data.append(df)\n",
    "                    print(f\"Loaded: {filename} ({len(df)} rows)\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {filename}: {e}\")\n",
    "        \n",
    "        if not all_data:\n",
    "            raise ValueError(\"No data files could be loaded!\")\n",
    "        \n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        print(f\"\\nTotal records loaded: {len(combined_df)}\")\n",
    "        \n",
    "        # Ensure Overall_Score is calculated if not present\n",
    "        if 'Overall_Score' not in combined_df.columns or combined_df['Overall_Score'].isna().any():\n",
    "            combined_df['Overall_Score'] = combined_df[METRICS].mean(axis=1)\n",
    "        \n",
    "        self.df = combined_df\n",
    "        \n",
    "        # Compute majority vote dataframe (and now also per-model agreement stats)\n",
    "        self.compute_majority_vote()\n",
    "        \n",
    "        return combined_df\n",
    "    \n",
    "    def compute_majority_vote(self):\n",
    "        \"\"\"\n",
    "        Compute majority vote per QA pair per metric.\n",
    "        In this structure, each model IS a judge, so we need to compare scores across models for the same qa_id.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"COMPUTING MAJORITY VOTE AND AGREEMENT STATISTICS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # In this data structure:\n",
    "        # - Each CSV file = one model's evaluations\n",
    "        # - Each model = one judge\n",
    "        # - For majority vote, we need to compare scores from all models for the same qa_id\n",
    "        \n",
    "        # Group by qa_id to see how many models evaluated each question\n",
    "        qa_id_counts = self.df.groupby('qa_id')['model'].nunique().reset_index(name='num_models')\n",
    "        \n",
    "        # Debug: Show distribution of model counts per qa_id\n",
    "        print(\"\\nDistribution of models (judges) per qa_id:\")\n",
    "        count_distribution = qa_id_counts['num_models'].value_counts().sort_index()\n",
    "        for model_count, instances in count_distribution.items():\n",
    "            print(f\"  {model_count} models: {instances} qa_ids\")\n",
    "        \n",
    "        # Filter to keep only qa_ids that were evaluated by all 5 models\n",
    "        valid_qa_ids = qa_id_counts[qa_id_counts['num_models'] == NUM_JUDGES]['qa_id'].tolist()\n",
    "        \n",
    "        print(f\"\\nTotal unique qa_ids: {len(qa_id_counts)}\")\n",
    "        print(f\"qa_ids evaluated by all {NUM_JUDGES} models: {len(valid_qa_ids)}\")\n",
    "        \n",
    "        if len(valid_qa_ids) == 0:\n",
    "            print(\"\\nWARNING: No qa_ids were evaluated by all 5 models!\")\n",
    "            self.majority_vote_df = pd.DataFrame()\n",
    "            self.model_majority_agreement = {}\n",
    "            return self.majority_vote_df\n",
    "        \n",
    "        majority_vote_data = []\n",
    "        agreement_stats_by_metric = {metric: {'full_agreement': 0, 'majority_exists': 0, 'no_majority': 0} for metric in METRICS}\n",
    "        agreement_stats_by_qa = {'all_metrics_majority': 0, 'some_metrics_majority': 0, 'no_metrics_majority': 0}\n",
    "        \n",
    "        for qa_id in valid_qa_ids:\n",
    "            # Get all evaluations for this qa_id (one from each model/judge)\n",
    "            qa_data = self.df[self.df['qa_id'] == qa_id]\n",
    "            \n",
    "            if len(qa_data) != NUM_JUDGES:\n",
    "                continue  # Skip if we don't have exactly NUM_JUDGES evaluations\n",
    "            \n",
    "            # Get metadata from the first row\n",
    "            first_row = qa_data.iloc[0]\n",
    "            row_data = {\n",
    "                'qa_id': qa_id,\n",
    "                'question_type': first_row['question_type'],\n",
    "                'question': first_row['question'],\n",
    "                'answer': first_row['answer']\n",
    "            }\n",
    "            \n",
    "            metrics_with_majority = 0\n",
    "            \n",
    "            # Compute majority vote for each metric\n",
    "            for metric in METRICS:\n",
    "                # Get scores from all models/judges for this metric\n",
    "                scores = qa_data[metric].values\n",
    "                score_counts = Counter(scores)\n",
    "                \n",
    "                if not score_counts:\n",
    "                    row_data[f'{metric}_majority'] = np.nan\n",
    "                    continue\n",
    "                \n",
    "                # Get the most common score and its count\n",
    "                majority_score, majority_count = score_counts.most_common(1)[0]\n",
    "                row_data[f'{metric}_majority'] = majority_score\n",
    "                row_data[f'{metric}_majority_count'] = majority_count\n",
    "                \n",
    "                # Check for full agreement (all judges gave the same score)\n",
    "                if len(score_counts) == 1:\n",
    "                    agreement_stats_by_metric[metric]['full_agreement'] += 1\n",
    "                    metrics_with_majority += 1\n",
    "                # Check for simple majority (> NUM_JUDGES / 2)\n",
    "                elif majority_count > NUM_JUDGES / 2:\n",
    "                    agreement_stats_by_metric[metric]['majority_exists'] += 1\n",
    "                    metrics_with_majority += 1\n",
    "                else:\n",
    "                    # No clear majority (tie or no score exceeds half)\n",
    "                    agreement_stats_by_metric[metric]['no_majority'] += 1\n",
    "            \n",
    "            # Track QA-level agreement\n",
    "            if metrics_with_majority == len(METRICS):\n",
    "                agreement_stats_by_qa['all_metrics_majority'] += 1\n",
    "            elif metrics_with_majority > 0:\n",
    "                agreement_stats_by_qa['some_metrics_majority'] += 1\n",
    "            else:\n",
    "                agreement_stats_by_qa['no_metrics_majority'] += 1\n",
    "            \n",
    "            # Compute overall score from majority votes\n",
    "            majority_scores = [row_data[f'{metric}_majority'] for metric in METRICS if f'{metric}_majority' in row_data]\n",
    "            row_data['Overall_Score_majority'] = np.mean(majority_scores) if majority_scores else np.nan\n",
    "            \n",
    "            majority_vote_data.append(row_data)\n",
    "        \n",
    "        self.majority_vote_df = pd.DataFrame(majority_vote_data)\n",
    "        \n",
    "        # Only proceed if we have data\n",
    "        if len(self.majority_vote_df) == 0:\n",
    "            print(\"\\nNo majority vote data to analyze.\")\n",
    "            return self.majority_vote_df\n",
    "        \n",
    "        # Print agreement statistics by metric\n",
    "        print(f\"\\nTotal qa_ids with majority vote computed: {len(self.majority_vote_df)}\")\n",
    "        print(\"\\nAgreement Statistics by Metric:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{'Metric':15s} {'Full Agreement':>20s} {'Majority Exists':>20s} {'No Majority':>20s}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for metric in METRICS:\n",
    "            full = agreement_stats_by_metric[metric]['full_agreement']\n",
    "            majority = agreement_stats_by_metric[metric]['majority_exists']\n",
    "            no_maj = agreement_stats_by_metric[metric]['no_majority']\n",
    "            total = full + majority + no_maj\n",
    "            \n",
    "            if total > 0:\n",
    "                full_pct = (full / total) * 100\n",
    "                majority_pct = (majority / total) * 100\n",
    "                no_maj_pct = (no_maj / total) * 100\n",
    "                print(f\"{metric:15s} {full:>7d} ({full_pct:>5.1f}%) {majority:>7d} ({majority_pct:>5.1f}%) {no_maj:>7d} ({no_maj_pct:>5.1f}%)\")\n",
    "        \n",
    "        # Print QA-level agreement statistics\n",
    "        print(\"\\nAgreement Statistics by QA Pair:\")\n",
    "        print(\"-\" * 50)\n",
    "        total_qa = sum(agreement_stats_by_qa.values())\n",
    "        if total_qa > 0:\n",
    "            all_maj = agreement_stats_by_qa['all_metrics_majority']\n",
    "            some_maj = agreement_stats_by_qa['some_metrics_majority']\n",
    "            no_maj = agreement_stats_by_qa['no_metrics_majority']\n",
    "            \n",
    "            print(f\"All metrics have majority:  {all_maj:>5d} ({(all_maj/total_qa)*100:>5.1f}%)\")\n",
    "            print(f\"Some metrics have majority: {some_maj:>5d} ({(some_maj/total_qa)*100:>5.1f}%)\")\n",
    "            print(f\"No metrics have majority:   {no_maj:>5d} ({(no_maj/total_qa)*100:>5.1f}%)\")\n",
    "        \n",
    "        # Calculate per-model agreement statistics (how often each model agrees with majority)\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PER-MODEL AGREEMENT WITH MAJORITY VOTE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        model_agreement_stats = {}\n",
    "        \n",
    "        for model in self.df['model'].unique():\n",
    "            total_comparisons = 0\n",
    "            agreements = 0\n",
    "            \n",
    "            # For each qa_id in majority vote results\n",
    "            for _, maj_row in self.majority_vote_df.iterrows():\n",
    "                qa_id = maj_row['qa_id']\n",
    "                \n",
    "                # Get this model's evaluation for this qa_id\n",
    "                model_eval = self.df[(self.df['qa_id'] == qa_id) & (self.df['model'] == model)]\n",
    "                \n",
    "                if len(model_eval) == 0:\n",
    "                    continue\n",
    "                \n",
    "                model_row = model_eval.iloc[0]\n",
    "                \n",
    "                # Compare model's scores with majority votes\n",
    "                for metric in METRICS:\n",
    "                    if f'{metric}_majority' in maj_row and not pd.isna(maj_row[f'{metric}_majority']):\n",
    "                        majority_vote = maj_row[f'{metric}_majority']\n",
    "                        model_vote = model_row[metric]\n",
    "                        \n",
    "                        total_comparisons += 1\n",
    "                        if model_vote == majority_vote:\n",
    "                            agreements += 1\n",
    "            \n",
    "            if total_comparisons > 0:\n",
    "                agreement_pct = (agreements / total_comparisons) * 100\n",
    "                model_agreement_stats[model] = {\n",
    "                    'agreements': agreements,\n",
    "                    'total': total_comparisons,\n",
    "                    'percentage': agreement_pct\n",
    "                }\n",
    "                print(f\"  {model:25s}: {agreement_pct:.1f}% ({agreements}/{total_comparisons} metric evaluations)\")\n",
    "            else:\n",
    "                print(f\"  {model:25s}: N/A (No comparisons)\")\n",
    "        \n",
    "        self.model_majority_agreement = model_agreement_stats\n",
    "        \n",
    "        return self.majority_vote_df\n",
    "    \n",
    "    def normalize_to_percentage(self, score, max_score=5):\n",
    "        \"\"\"Convert score to percentage (0-100 scale).\"\"\"\n",
    "        return (score / max_score) * 100\n",
    "    \n",
    "    def compute_overall_metrics(self):\n",
    "        \"\"\"1. Compute mean and std dev per metric across all QA pairs.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"1. MEAN AND STD DEV PER METRIC (OVERALL) - ABLATION STUDY 1\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        results = {}\n",
    "        for metric in METRICS:\n",
    "            mean_score = self.df[metric].mean()\n",
    "            std_score = self.df[metric].std()\n",
    "            mean_pct = self.normalize_to_percentage(mean_score)\n",
    "            \n",
    "            results[metric] = {\n",
    "                'mean': mean_score,\n",
    "                'std': std_score,\n",
    "                'mean_pct': mean_pct\n",
    "            }\n",
    "            \n",
    "            print(f\"{metric:15s}: {mean_score:.3f} ± {std_score:.3f} ({mean_pct:.1f}%)\")\n",
    "        \n",
    "        # Overall score (mean of all metrics)\n",
    "        overall_mean = self.df['Overall_Score'].mean()\n",
    "        overall_std = self.df['Overall_Score'].std()\n",
    "        overall_pct = self.normalize_to_percentage(overall_mean)\n",
    "        \n",
    "        print(f\"{'Overall Score':15s}: {overall_mean:.3f} ± {overall_std:.3f} ({overall_pct:.1f}%)\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def compute_overall_score_distribution(self):\n",
    "        \"\"\"2. Compute overall score distribution.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"2. OVERALL SCORE DISTRIBUTION - ABLATION STUDY 1\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        overall_scores = self.df['Overall_Score']\n",
    "        \n",
    "        print(f\"Mean:          {overall_scores.mean():.3f}\")\n",
    "        print(f\"Std Dev:       {overall_scores.std():.3f}\")\n",
    "        print(f\"Min:           {overall_scores.min():.3f}\")\n",
    "        print(f\"Max:           {overall_scores.max():.3f}\")\n",
    "        \n",
    "        # Distribution by ranges\n",
    "        print(\"\\nDistribution by score ranges:\")\n",
    "        ranges = [(1.0, 2.0), (2.0, 3.0), (3.0, 4.0), (4.0, 5.0)]\n",
    "        \n",
    "        for start, end in ranges:\n",
    "            if start == 4.0:  # Include 5.0 in the last range\n",
    "                count = ((overall_scores >= start) & (overall_scores <= end)).sum()\n",
    "            else:\n",
    "                count = ((overall_scores >= start) & (overall_scores < end)).sum()\n",
    "            pct = (count / len(overall_scores)) * 100\n",
    "            print(f\"  [{start:.1f}-{end:.1f}]: {count:5d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    def compute_breakdown_by_question_type(self):\n",
    "        \"\"\"3. Breakdown by question type (since we only have One-Shot).\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"3. BREAKDOWN BY QUESTION TYPE - ABLATION STUDY 1\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        question_type_results = {}\n",
    "        \n",
    "        for q_type in sorted(self.df['question_type'].unique()):\n",
    "            q_df = self.df[self.df['question_type'] == q_type]\n",
    "            count = len(q_df)\n",
    "            \n",
    "            print(f\"\\n{q_type} (n={count}):\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            results = {}\n",
    "            for metric in METRICS:\n",
    "                mean_score = q_df[metric].mean()\n",
    "                std_score = q_df[metric].std()\n",
    "                mean_pct = self.normalize_to_percentage(mean_score)\n",
    "                \n",
    "                results[metric] = {'mean': mean_score, 'std': std_score, 'mean_pct': mean_pct}\n",
    "                print(f\"  {metric:15s}: {mean_score:.3f} ± {std_score:.3f} ({mean_pct:.1f}%)\")\n",
    "            \n",
    "            # Overall score\n",
    "            overall_mean = q_df['Overall_Score'].mean()\n",
    "            overall_std = q_df['Overall_Score'].std()\n",
    "            overall_pct = self.normalize_to_percentage(overall_mean)\n",
    "            \n",
    "            print(f\"  {'Overall Score':15s}: {overall_mean:.3f} ± {overall_std:.3f} ({overall_pct:.1f}%)\")\n",
    "            \n",
    "            question_type_results[q_type] = {\n",
    "                'count': count,\n",
    "                'metrics': results,\n",
    "                'overall': {'mean': overall_mean, 'std': overall_std, 'pct': overall_pct}\n",
    "            }\n",
    "        \n",
    "        return question_type_results\n",
    "    \n",
    "    def compute_individual_model_analysis(self):\n",
    "        \"\"\"4. Individual model analysis by question type (One-Shot only).\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"4. INDIVIDUAL MODEL ANALYSIS - ABLATION STUDY 1\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        models = self.df['model'].unique()\n",
    "        \n",
    "        for model in sorted(models):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"MODEL: {model}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            model_df = self.df[self.df['model'] == model]\n",
    "            \n",
    "            # 4.1 Overall performance for this model\n",
    "            print(f\"\\n4.1 Overall Performance for {model}:\")\n",
    "            print(\"-\" * 40)\n",
    "            for metric in METRICS:\n",
    "                mean_score = model_df[metric].mean()\n",
    "                std_score = model_df[metric].std()\n",
    "                mean_pct = self.normalize_to_percentage(mean_score)\n",
    "                print(f\"  {metric:15s}: {mean_score:.3f} ± {std_score:.3f} ({mean_pct:.1f}%)\")\n",
    "            \n",
    "            overall_mean = model_df['Overall_Score'].mean()\n",
    "            overall_std = model_df['Overall_Score'].std()\n",
    "            overall_pct = self.normalize_to_percentage(overall_mean)\n",
    "            print(f\"  {'Overall Score':15s}: {overall_mean:.3f} ± {overall_std:.3f} ({overall_pct:.1f}%)\")\n",
    "            \n",
    "            # 4.2 By Question Type\n",
    "            print(f\"\\n4.2 {model} - Performance by Question Type:\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            for q_type in sorted(model_df['question_type'].unique()):\n",
    "                q_model_df = model_df[model_df['question_type'] == q_type]\n",
    "                count = len(q_model_df)\n",
    "                \n",
    "                overall_mean = q_model_df['Overall_Score'].mean()\n",
    "                overall_pct = self.normalize_to_percentage(overall_mean)\n",
    "                \n",
    "                print(f\"  {q_type:20s}: {overall_mean:.3f} ({overall_pct:.1f}%) [n={count}]\")\n",
    "    \n",
    "    def compute_model_comparison_summary(self):\n",
    "        \"\"\"5. Model comparison summary table.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"5. MODEL COMPARISON SUMMARY - ABLATION STUDY 1\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        models = sorted(self.df['model'].unique())\n",
    "        \n",
    "        # Create summary table\n",
    "        print(f\"\\n{'Model':20s}\", end=\"\")\n",
    "        for metric in METRICS:\n",
    "            print(f\"{metric:>12s}\", end=\"\")\n",
    "        print(f\"{'Overall':>12s}\")\n",
    "        print(\"-\" * (20 + 12 * (len(METRICS) + 1)))\n",
    "        \n",
    "        for model in models:\n",
    "            model_df = self.df[self.df['model'] == model]\n",
    "            print(f\"{model:20s}\", end=\"\")\n",
    "            \n",
    "            for metric in METRICS:\n",
    "                mean_score = model_df[metric].mean()\n",
    "                print(f\"{mean_score:>12.3f}\", end=\"\")\n",
    "            \n",
    "            overall_mean = model_df['Overall_Score'].mean()\n",
    "            print(f\"{overall_mean:>12.3f}\")\n",
    "        \n",
    "        # Add average across all models\n",
    "        print(\"-\" * (20 + 12 * (len(METRICS) + 1)))\n",
    "        print(f\"{'AVERAGE':20s}\", end=\"\")\n",
    "        \n",
    "        for metric in METRICS:\n",
    "            mean_score = self.df[metric].mean()\n",
    "            print(f\"{mean_score:>12.3f}\", end=\"\")\n",
    "        \n",
    "        overall_mean = self.df['Overall_Score'].mean()\n",
    "        print(f\"{overall_mean:>12.3f}\")\n",
    "    \n",
    "    def run_all_analyses(self):\n",
    "        \"\"\"Main function to run all analyses.\"\"\"\n",
    "        print(\"ABLATION STUDY 1 - CHUNKS ONLY EVALUATION ANALYSIS\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"Prompting Strategy: One-Shot Only\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Load all data (this also computes majority vote)\n",
    "        self.load_all_data()\n",
    "        \n",
    "        # Run all analyses\n",
    "        self.compute_overall_metrics()\n",
    "        self.compute_overall_score_distribution()\n",
    "        self.compute_breakdown_by_question_type()\n",
    "        self.compute_individual_model_analysis()\n",
    "        self.compute_model_comparison_summary()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ABLATION STUDY 1 ANALYSIS COMPLETE\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "def main():\n",
    "    analyzer = AblationStudy1EvaluationAnalyzer()\n",
    "    analyzer.run_all_analyses()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
