{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f14968c-9dec-49c0-9f4c-2b0e758abbcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from typing import Dict, List, Optional\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "from together import Together\n",
    "import random\n",
    "\n",
    "class ChunksOnlyLLMJudgeEvaluator:\n",
    "    \"\"\"\n",
    "    Ablation Study 1: LLM Judge Evaluation System for Chunks-Only Model\n",
    "    Evaluates QA pairs generated using only chunk-based retrieval without KG information.\n",
    "    Together AI Version with Gemma-2-27b-it\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, model_name: str = \"google/gemma-2-27b-it\"):\n",
    "        \"\"\"\n",
    "        Initialize the Chunks-Only LLM Judge Evaluator.\n",
    "        \n",
    "        Args:\n",
    "            api_key: Together AI API key\n",
    "            model_name: Model identifier for the LLM judge\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # Initialize Together AI client\n",
    "        self.client = Together(api_key=api_key)\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        self.evaluation_prompt = \"\"\"You are a STRICT QA evaluator following precise scoring guidelines for a CHUNKS-ONLY model.\n",
    "\n",
    "You will evaluate a model-generated question-answer pair on FOUR metrics using a 1-5 scale where:\n",
    "5 = Excellent  \n",
    "4 = Good  \n",
    "3 = Fair  \n",
    "2 = Poor  \n",
    "1 = Very Poor\n",
    "\n",
    "==============================\n",
    "DETAILED SCORING CRITERIA\n",
    "==============================\n",
    "\n",
    "1. RELEVANCE (1-5): Does the question appropriately relate to the source chunks?\n",
    "   5: Perfectly relevant to the chunks, clearly grounded in the text\n",
    "   4: Mostly relevant, with minor off-topic elements  \n",
    "   3: Addresses the main question but misses some important points from chunks\n",
    "   2: Loosely related, with significant tangents or irrelevance to chunks\n",
    "   1: Entirely irrelevant or unrelated to the source chunks\n",
    "\n",
    "2. ACCURACY (1-5): Is the answer factually correct based on the source chunks?\n",
    "   5: All facts are accurate and fully verifiable in the chunks\n",
    "   4: Mostly accurate; contains only minor factual issues\n",
    "   3: Some factual inconsistencies or assumptions beyond chunks\n",
    "   2: Several factual errors that affect reliability\n",
    "   1: Mostly inaccurate or misleading information\n",
    "\n",
    "3. COMPLETENESS (1-5): Does the answer fully address the question using available chunks?\n",
    "   5: Thorough and complete response utilizing chunk information effectively\n",
    "   4: Covers most parts but misses minor aspects available in chunks\n",
    "   3: Addresses main part, omits some key details from chunks\n",
    "   2: Partial answer with significant gaps despite relevant chunk information\n",
    "   1: Severely incomplete or ignores available chunk information\n",
    "\n",
    "4. FLUENCY (1-5): Is the answer well-written and grammatically correct?\n",
    "   5: Excellent grammar and clarity; highly readable\n",
    "   4: Minor grammatical or structural issues\n",
    "   3: Understandable, but contains noticeable language errors\n",
    "   2: Somewhat unclear due to poor grammar or phrasing\n",
    "   1: Difficult to read or understand\n",
    "\n",
    "==============================\n",
    "EVALUATION CONTEXT\n",
    "==============================\n",
    "This model uses ONLY text chunks for retrieval — no structured knowledge graph information.\n",
    "Evaluate how well the model utilizes the available chunk information to answer questions.\n",
    "\n",
    "==============================\n",
    "INPUT\n",
    "==============================\n",
    "**Question:** {question}  \n",
    "**Answer:** {answer}  \n",
    "**Source Chunks:** {source_context}\n",
    "\n",
    "JUST return your answer in this exact format:\n",
    "\n",
    "Relevance: X  \n",
    "Accuracy: X  \n",
    "Completeness: X  \n",
    "Fluency: X\n",
    "\n",
    "Where X is a number from 1 to 5.\"\"\"\n",
    "\n",
    "    def get_source_context(self, item: Dict) -> str:\n",
    "        \"\"\"Extract source context from QA item.\"\"\"\n",
    "        # Check if source_context is nested under ground_truth\n",
    "        if \"ground_truth\" in item and isinstance(item[\"ground_truth\"], dict):\n",
    "            return item[\"ground_truth\"].get(\"source_context\", \"\")\n",
    "        \n",
    "        # Fallback to root level\n",
    "        return item.get(\"source_context\", \"No source context available\")\n",
    "\n",
    "    def call_llm_judge(self, question: str, answer: str, source_context: str, \n",
    "                      max_retries: int = 3) -> Optional[Dict[str, int]]:\n",
    "        \"\"\"\n",
    "        Call the LLM judge to evaluate a QA pair with retry logic using Together AI.\n",
    "        \n",
    "        Args:\n",
    "            question: The question to evaluate\n",
    "            answer: The answer to evaluate\n",
    "            source_context: Source context for the QA pair\n",
    "            max_retries: Maximum number of retry attempts\n",
    "            \n",
    "        Returns:\n",
    "            Dict with evaluation scores or None if failed\n",
    "        \"\"\"\n",
    "        # Format the prompt (no KG triples needed for chunks-only model)\n",
    "        prompt = self.evaluation_prompt.format(\n",
    "            question=question,\n",
    "            answer=answer,\n",
    "            source_context=source_context\n",
    "        )\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Together AI API call\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model_name,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=0.0,  # Deterministic for consistent scoring\n",
    "                    max_tokens=150,   # Short response expected\n",
    "                    top_p=0.8\n",
    "                )\n",
    "                \n",
    "                if response and response.choices:\n",
    "                    content = response.choices[0].message.content\n",
    "                    \n",
    "                    if content:\n",
    "                        parsed_result = self.parse_evaluation_response(content)\n",
    "                        if parsed_result:\n",
    "                            return parsed_result\n",
    "                    else:\n",
    "                        self.logger.warning(f\"Empty response content from {self.model_name}\")\n",
    "                else:\n",
    "                    self.logger.warning(f\"No response or choices from {self.model_name}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                # Handle rate limiting and other errors\n",
    "                if \"rate limit\" in str(e).lower() or \"429\" in str(e):\n",
    "                    wait_time = 2 ** attempt  # Exponential backoff\n",
    "                    self.logger.warning(f\"Rate limit hit, waiting {wait_time}s before retry {attempt + 1}/{max_retries}\")\n",
    "                    time.sleep(wait_time)\n",
    "                elif \"timeout\" in str(e).lower():\n",
    "                    self.logger.warning(f\"Timeout on attempt {attempt + 1}/{max_retries}\")\n",
    "                    if attempt < max_retries - 1:\n",
    "                        time.sleep(2 ** attempt)\n",
    "                else:\n",
    "                    self.logger.error(f\"API Error on attempt {attempt + 1}: {str(e)}\")\n",
    "                    if attempt < max_retries - 1:\n",
    "                        time.sleep(2 ** attempt)\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def parse_evaluation_response(self, response: str) -> Optional[Dict[str, int]]:\n",
    "        \"\"\"\n",
    "        Parse the LLM response to extract numerical scores for 4 metrics (no KG alignment).\n",
    "        \n",
    "        Expected formats:\n",
    "        Relevance: 4\n",
    "        Accuracy: 5/5  \n",
    "        Completeness:3\n",
    "        Fluency:  4\n",
    "        \"\"\"\n",
    "        try:\n",
    "            scores = {}\n",
    "            lines = response.strip().split('\\n')\n",
    "            \n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if ':' in line:\n",
    "                    parts = line.split(':', 1)\n",
    "                    if len(parts) == 2:\n",
    "                        metric = parts[0].strip()\n",
    "                        score_text = parts[1].strip()\n",
    "                        \n",
    "                        # Use improved regex pattern\n",
    "                        score_match = re.search(r':\\s*([1-5])(?:/5)?', line)\n",
    "                        if score_match:\n",
    "                            score = int(score_match.group(1))\n",
    "                            \n",
    "                            # Normalize metric names (case-insensitive)\n",
    "                            metric_lower = metric.lower()\n",
    "                            if 'relevance' in metric_lower:\n",
    "                                scores['Relevance'] = score\n",
    "                            elif 'accuracy' in metric_lower:\n",
    "                                scores['Accuracy'] = score\n",
    "                            elif 'completeness' in metric_lower:\n",
    "                                scores['Completeness'] = score\n",
    "                            elif 'fluency' in metric_lower:\n",
    "                                scores['Fluency'] = score\n",
    "            \n",
    "            # FIXED: If standard parsing fails, extract from reasoning text\n",
    "            expected_metrics = ['Relevance', 'Accuracy', 'Completeness', 'Fluency']\n",
    "            if len(scores) < 4:\n",
    "                # Extract from reasoning patterns like \"Relevance should be a 5\"\n",
    "                text_lower = response.lower()\n",
    "                \n",
    "                if 'Relevance' not in scores:\n",
    "                    match = re.search(r'relevance.*?(?:should be|is).*?([1-5])', text_lower)\n",
    "                    if match:\n",
    "                        scores['Relevance'] = int(match.group(1))\n",
    "                \n",
    "                if 'Accuracy' not in scores:\n",
    "                    match = re.search(r'accuracy.*?(?:should be|is).*?([1-5])', text_lower)\n",
    "                    if match:\n",
    "                        scores['Accuracy'] = int(match.group(1))\n",
    "                \n",
    "                if 'Completeness' not in scores:\n",
    "                    match = re.search(r'completeness.*?(?:should be|is).*?([1-5])', text_lower)\n",
    "                    if match:\n",
    "                        scores['Completeness'] = int(match.group(1))\n",
    "                \n",
    "                if 'Fluency' not in scores:\n",
    "                    match = re.search(r'fluency.*?(?:should be|is).*?([1-5])', text_lower)\n",
    "                    if match:\n",
    "                        scores['Fluency'] = int(match.group(1))\n",
    "            \n",
    "            # If we have partial scores, try to get the rest\n",
    "            if len(scores) > 0 and len(scores) < 4:\n",
    "                self.logger.warning(f\"Got partial response with {len(scores)} scores: {scores}\")\n",
    "                # Fill missing scores with average of existing ones\n",
    "                if len(scores) >= 2:\n",
    "                    avg_score = round(sum(scores.values()) / len(scores))\n",
    "                    expected_metrics = ['Relevance', 'Accuracy', 'Completeness', 'Fluency']\n",
    "                    for metric in expected_metrics:\n",
    "                        if metric not in scores:\n",
    "                            scores[metric] = avg_score\n",
    "                            self.logger.info(f\"Filled missing {metric} with average {avg_score}\")\n",
    "                    return scores\n",
    "            \n",
    "            # Validate we have all 4 scores (no KG_Alignment for chunks-only)\n",
    "            if all(metric in scores for metric in expected_metrics):\n",
    "                return scores\n",
    "            else:\n",
    "                self.logger.warning(f\"Missing metrics in response: {response}\")\n",
    "                return self._parse_with_fallback(response)\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to parse response: {response}. Error: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _parse_with_fallback(self, response: str) -> Optional[Dict[str, int]]:\n",
    "        \"\"\"Fallback parsing with multiple regex patterns for 4 metrics.\"\"\"\n",
    "        patterns = [\n",
    "            r':\\s*([1-5])(?:/5)?',          # Improved pattern\n",
    "            r'\\b([1-5])\\b',                 # Original pattern\n",
    "            r'([1-5])\\s*(?:out of 5|/5)?'   # Alternative pattern\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            try:\n",
    "                scores = {}\n",
    "                lines = response.strip().split('\\n')\n",
    "                \n",
    "                for line in lines:\n",
    "                    if ':' in line:\n",
    "                        parts = line.split(':', 1)\n",
    "                        if len(parts) == 2:\n",
    "                            metric = parts[0].strip()\n",
    "                            score_text = parts[1].strip()\n",
    "                            \n",
    "                            score_match = re.search(pattern, score_text)\n",
    "                            if score_match:\n",
    "                                score = int(score_match.group(1))\n",
    "                                \n",
    "                                # Normalize metric names (only 4 metrics for chunks-only)\n",
    "                                metric_lower = metric.lower()\n",
    "                                if 'relevance' in metric_lower:\n",
    "                                    scores['Relevance'] = score\n",
    "                                elif 'accuracy' in metric_lower:\n",
    "                                    scores['Accuracy'] = score\n",
    "                                elif 'completeness' in metric_lower:\n",
    "                                    scores['Completeness'] = score\n",
    "                                elif 'fluency' in metric_lower:\n",
    "                                    scores['Fluency'] = score\n",
    "                \n",
    "                # Check if this pattern worked (only 4 metrics)\n",
    "                expected_metrics = ['Relevance', 'Accuracy', 'Completeness', 'Fluency']\n",
    "                if all(metric in scores for metric in expected_metrics):\n",
    "                    return scores\n",
    "                    \n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def validate_evaluation_scores(self, evaluation: Dict[str, int], qa_id: str) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Validate and clamp evaluation scores to valid range [1, 5].\n",
    "        \"\"\"\n",
    "        validated = {}\n",
    "        issues = []\n",
    "        \n",
    "        for metric, score in evaluation.items():\n",
    "            original_score = score\n",
    "            \n",
    "            # Handle non-numeric values\n",
    "            if not isinstance(score, (int, float)):\n",
    "                try:\n",
    "                    score = int(score)\n",
    "                except (ValueError, TypeError):\n",
    "                    score = 3  # Default to middle score\n",
    "                    issues.append(f\"{metric}: non-numeric '{original_score}' → {score}\")\n",
    "            \n",
    "            # Clamp to valid range\n",
    "            clamped_score = min(max(int(score), 1), 5)\n",
    "            validated[metric] = clamped_score\n",
    "            \n",
    "            # Log if clamped\n",
    "            if clamped_score != original_score:\n",
    "                issues.append(f\"{metric}: {original_score} → {clamped_score}\")\n",
    "        \n",
    "        # Log any issues\n",
    "        if issues:\n",
    "            self.logger.warning(f\"Score validation issues for {qa_id}: {'; '.join(issues)}\")\n",
    "        \n",
    "        return validated\n",
    "\n",
    "    def load_qa_dataset(self, file_path: str) -> List[Dict]:\n",
    "        \"\"\"Load QA dataset from JSON file.\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            queries = data.get('queries', [])\n",
    "            self.logger.info(f\"Loaded {len(queries)} QA pairs from {file_path}\")\n",
    "            return queries\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load dataset: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def save_checkpoint(self, results: List[Dict], checkpoint_path: str):\n",
    "        \"\"\"Save evaluation results as checkpoint.\"\"\"\n",
    "        try:\n",
    "            df = pd.DataFrame(results)\n",
    "            df.to_csv(checkpoint_path, index=False)\n",
    "            self.logger.info(f\"Checkpoint saved: {len(results)} results to {checkpoint_path}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to save checkpoint: {str(e)}\")\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_path: str) -> List[Dict]:\n",
    "        \"\"\"Load evaluation results from checkpoint.\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(checkpoint_path):\n",
    "                df = pd.read_csv(checkpoint_path)\n",
    "                results = df.to_dict('records')\n",
    "                self.logger.info(f\"Loaded checkpoint: {len(results)} results from {checkpoint_path}\")\n",
    "                return results\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load checkpoint: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def evaluate_dataset(self, dataset_path: str, output_path: str, \n",
    "                        sample_size: Optional[int] = None, \n",
    "                        delay_seconds: float = 1.0,\n",
    "                        checkpoint_interval: int = 50) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Evaluate QA dataset using chunks-only LLM judge with checkpointing.\n",
    "        \n",
    "        Args:\n",
    "            dataset_path: Path to the QA dataset JSON file\n",
    "            output_path: Path to save evaluation results\n",
    "            sample_size: Number of samples to evaluate (None for all)\n",
    "            delay_seconds: Delay between API calls to avoid rate limits\n",
    "            checkpoint_interval: Save checkpoint every N evaluations\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with evaluation results\n",
    "        \"\"\"\n",
    "        # Load dataset\n",
    "        qa_items = self.load_qa_dataset(dataset_path)\n",
    "        if not qa_items:\n",
    "            self.logger.error(\"No QA items loaded. Exiting.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Sample subset if requested\n",
    "        if sample_size and sample_size < len(qa_items):\n",
    "            random.seed(42)  # For reproducibility\n",
    "            qa_items = random.sample(qa_items, sample_size)\n",
    "            self.logger.info(f\"Sampling {sample_size} items for evaluation\")\n",
    "        \n",
    "        # Setup checkpoint\n",
    "        checkpoint_path = f\"{output_path}.checkpoint\"\n",
    "        results = self.load_checkpoint(checkpoint_path)\n",
    "        \n",
    "        # Track processed items\n",
    "        processed_ids = {result['qa_id'] for result in results} if results else set()\n",
    "        \n",
    "        # Filter unprocessed items - FIXED indexing\n",
    "        remaining_items = []\n",
    "        for i, item in enumerate(qa_items):\n",
    "            item_id = item.get('id', f'item_{i}')\n",
    "            if item_id not in processed_ids:\n",
    "                remaining_items.append(item)\n",
    "        \n",
    "        if processed_ids:\n",
    "            self.logger.info(f\"Resuming from checkpoint: {len(results)} completed, {len(remaining_items)} remaining\")\n",
    "        \n",
    "        failed_evaluations = 0\n",
    "        \n",
    "        # Progress bar with correct total and initial values\n",
    "        total_items = len(qa_items)\n",
    "        completed_items = len(results)\n",
    "        pbar = tqdm(\n",
    "            remaining_items, \n",
    "            desc=\"Evaluating Chunks-Only QA pairs\",\n",
    "            total=total_items,\n",
    "            initial=completed_items,\n",
    "            unit=\"items\"\n",
    "        )\n",
    "        \n",
    "        for i, item in enumerate(remaining_items):\n",
    "            try:\n",
    "                # Extract data from item - FIXED ID generation\n",
    "                original_index = next((i for i, orig_item in enumerate(qa_items) if orig_item is item), len(qa_items))\n",
    "                qa_id = item.get('id', f'item_{original_index}')\n",
    "                question = item.get('question', '')\n",
    "                answer = item.get('answer', '')\n",
    "                question_type = item.get('question_type', 'unknown')\n",
    "                \n",
    "                # Extract source context (chunks only)\n",
    "                source_context = self.get_source_context(item)\n",
    "                \n",
    "                # Skip if essential data is missing\n",
    "                if not question or not answer:\n",
    "                    self.logger.warning(f\"Skipping item {qa_id}: missing question or answer\")\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                \n",
    "                # Call LLM judge (no KG triples needed)\n",
    "                evaluation = self.call_llm_judge(question, answer, source_context)\n",
    "                \n",
    "                if evaluation:\n",
    "                    evaluation = self.validate_evaluation_scores(evaluation, qa_id)\n",
    "                    \n",
    "                    # Store results (only 4 metrics for chunks-only)\n",
    "                    result = {\n",
    "                        'qa_id': qa_id,\n",
    "                        'question_type': question_type,\n",
    "                        'question': question,\n",
    "                        'answer': answer,\n",
    "                        'Relevance': evaluation['Relevance'],\n",
    "                        'Accuracy': evaluation['Accuracy'],\n",
    "                        'Completeness': evaluation['Completeness'],\n",
    "                        'Fluency': evaluation['Fluency'],\n",
    "                        'Overall_Score': sum(evaluation.values()) / len(evaluation)\n",
    "                    }\n",
    "                    results.append(result)\n",
    "                    \n",
    "                    # Update progress bar with detailed status\n",
    "                    pbar.set_postfix({\n",
    "                        'Completed': len(results),\n",
    "                        'Failed': failed_evaluations,\n",
    "                        'Success_Rate': f\"{len(results)/(len(results)+failed_evaluations)*100:.1f}%\",\n",
    "                        'Last_Score': f\"{result['Overall_Score']:.1f}\"\n",
    "                    })\n",
    "                    \n",
    "                    # Save checkpoint\n",
    "                    if len(results) % checkpoint_interval == 0:\n",
    "                        self.save_checkpoint(results, checkpoint_path)\n",
    "                    \n",
    "                else:\n",
    "                    failed_evaluations += 1\n",
    "                    self.logger.warning(f\"Failed to evaluate item {qa_id}\")\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Rate limiting\n",
    "                if delay_seconds > 0:\n",
    "                    time.sleep(delay_seconds)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                failed_evaluations += 1\n",
    "                self.logger.error(f\"Error processing item {qa_id}: {str(e)}\")\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        # Final save\n",
    "        if results:\n",
    "            df = pd.DataFrame(results)\n",
    "            df.to_csv(output_path, index=False)\n",
    "            \n",
    "            # Clean up checkpoint\n",
    "            if os.path.exists(checkpoint_path):\n",
    "                os.remove(checkpoint_path)\n",
    "            \n",
    "            # Print summary statistics\n",
    "            self.print_evaluation_summary(df, failed_evaluations)\n",
    "            \n",
    "            return df\n",
    "        else:\n",
    "            self.logger.error(\"No successful evaluations completed\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def print_evaluation_summary(self, df: pd.DataFrame, failed_count: int):\n",
    "        \"\"\"Print comprehensive summary statistics of the chunks-only evaluation.\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\" ABLATION STUDY 1: CHUNKS-ONLY MODEL EVALUATION SUMMARY\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        print(f\" Evaluation Statistics:\")\n",
    "        print(f\"   Total Evaluated: {len(df)}\")\n",
    "        print(f\"   Failed Evaluations: {failed_count}\")\n",
    "        print(f\"   Success Rate: {len(df)/(len(df)+failed_count)*100:.1f}%\")\n",
    "        print(f\"   Evaluation Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"   Model Type: CHUNKS-ONLY (No Knowledge Graph)\")\n",
    "        print(f\"   Judge Model: {self.model_name}\")\n",
    "        \n",
    "        print(f\"\\n Average Scores by Metric:\")\n",
    "        metrics = ['Relevance', 'Accuracy', 'Completeness', 'Fluency']\n",
    "        for metric in metrics:\n",
    "            mean_score = df[metric].mean()\n",
    "            std_score = df[metric].std()\n",
    "            min_score = df[metric].min()\n",
    "            max_score = df[metric].max()\n",
    "            print(f\"   {metric}: {mean_score:.2f} ± {std_score:.2f} (range: {min_score}-{max_score})\")\n",
    "        \n",
    "        print(f\"\\n Overall Performance:\")\n",
    "        print(f\"   Mean Overall Score: {df['Overall_Score'].mean():.2f}\")\n",
    "        print(f\"   Median Overall Score: {df['Overall_Score'].median():.2f}\")\n",
    "        print(f\"   Best Score: {df['Overall_Score'].max():.2f}\")\n",
    "        print(f\"   Worst Score: {df['Overall_Score'].min():.2f}\")\n",
    "        \n",
    "        # Score distribution\n",
    "        print(f\"\\n Score Distribution:\")\n",
    "        score_ranges = [(1.0, 2.0), (2.0, 3.0), (3.0, 4.0), (4.0, 5.0)]\n",
    "        for min_score, max_score in score_ranges:\n",
    "            count = len(df[(df['Overall_Score'] >= min_score) & (df['Overall_Score'] < max_score)])\n",
    "            percentage = (count / len(df)) * 100\n",
    "            print(f\"   {min_score:.1f}-{max_score:.1f}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Perfect scores\n",
    "        perfect_scores = len(df[df['Overall_Score'] == 5.0])\n",
    "        print(f\"   Perfect (5.0): {perfect_scores} ({(perfect_scores/len(df))*100:.1f}%)\")\n",
    "        \n",
    "        if 'question_type' in df.columns:\n",
    "            print(f\"\\n Performance by Question Type:\")\n",
    "            type_summary = df.groupby('question_type').agg({\n",
    "                'Overall_Score': ['mean', 'std', 'count'],\n",
    "                'Relevance': 'mean',\n",
    "                'Accuracy': 'mean',\n",
    "                'Completeness': 'mean',\n",
    "                'Fluency': 'mean'\n",
    "            }).round(3)\n",
    "            \n",
    "            for qtype in type_summary.index:\n",
    "                stats = type_summary.loc[qtype]\n",
    "                mean_score = stats[('Overall_Score', 'mean')]\n",
    "                std_score = stats[('Overall_Score', 'std')]\n",
    "                count = stats[('Overall_Score', 'count')]\n",
    "                print(f\"   {qtype}: {mean_score:.2f} ± {std_score:.2f} (n={count})\")\n",
    "        \n",
    "        print(f\"\\n Ablation Study Notes:\")\n",
    "        print(f\"   - This evaluation focuses on chunk-based retrieval only\")\n",
    "        print(f\"   - No Knowledge Graph information was used\")\n",
    "        print(f\"   - Evaluation uses 4 metrics (no KG Alignment)\")\n",
    "        print(f\"   - Results can be compared with full model performance\")\n",
    "\n",
    "def run_chunks_only_evaluation():\n",
    "    \"\"\"Run evaluation for Ablation Study 1: Chunks-Only Model.\"\"\"\n",
    "    \n",
    "    # Configuration - Together AI\n",
    "    API_KEY = \"\"  # Your Together AI API key\n",
    "    DATASET_PATH = \"Ablation_1_chunks_only_qa_dataset.json\"  # Update with your chunks-only dataset path\n",
    "    OUTPUT_PATH = \"Ablation_Study_1_Chunks_Only_Gemma2_evaluation_results.csv\"\n",
    "    \n",
    "    # Initialize chunks-only evaluator with Gemma-2-27b-it\n",
    "    evaluator = ChunksOnlyLLMJudgeEvaluator(\n",
    "        api_key=API_KEY,\n",
    "        model_name=\"google/gemma-2-27b-it\"\n",
    "    )\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\" ABLATION STUDY 1: CHUNKS-ONLY MODEL EVALUATION\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\" This evaluation will assess QA pairs generated using ONLY chunks\")\n",
    "    print(\" (no Knowledge Graph information)\")\n",
    "    print(\" \")\n",
    "    print(\" Evaluation Details:\")\n",
    "    print(\"   - Model: Chunks-Only Retrieval\")\n",
    "    print(f\"   - Judge Model: {evaluator.model_name}\")\n",
    "    print(\"   - API Provider: Together AI\")\n",
    "    print(\"   - Metrics: 4 (Relevance, Accuracy, Completeness, Fluency)\")\n",
    "    print(\"   - Scoring: Strict 1-5 scale\")\n",
    "    print(\"   - Checkpoints: Every 50 evaluations\")\n",
    "    print(\"   - Can resume if interrupted\")\n",
    "    print(\" \")\n",
    "    \n",
    "    # Confirm before starting\n",
    "    response = input(\"\\nProceed with chunks-only evaluation? (y/n): \").strip().lower()\n",
    "    if response != 'y':\n",
    "        print(\" Evaluation cancelled.\")\n",
    "        return\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run evaluation on chunks-only dataset\n",
    "    results_df = evaluator.evaluate_dataset(\n",
    "        dataset_path=DATASET_PATH,\n",
    "        output_path=OUTPUT_PATH,\n",
    "        sample_size=None,  # Process all items\n",
    "        delay_seconds=1.0,  # 1 second delay between requests\n",
    "        checkpoint_interval=50  # Save every 50 evaluations\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        print(f\"\\n CHUNKS-ONLY EVALUATION COMPLETED!\")\n",
    "        print(f\"  Total time: {duration/60:.1f} minutes\")\n",
    "        print(f\" Evaluated: {len(results_df)} QA pairs\")\n",
    "        print(f\" Results saved to: {OUTPUT_PATH}\")\n",
    "        print(f\" Judge Model: {evaluator.model_name}\")\n",
    "        print(f\" API Provider: Together AI\")\n",
    "        \n",
    "        # Additional analysis\n",
    "        print(f\"\\n Processing Statistics:\")\n",
    "        print(f\"   Average processing time: {duration/len(results_df):.2f} seconds per QA pair\")\n",
    "        print(f\"   Items per minute: {len(results_df)/(duration/60):.1f}\")\n",
    "        \n",
    "        # Score distribution preview\n",
    "        print(f\"\\n Chunks-Only Performance Preview:\")\n",
    "        print(f\"   Mean Overall Score: {results_df['Overall_Score'].mean():.2f}\")\n",
    "        print(f\"   Score Range: {results_df['Overall_Score'].min():.2f} - {results_df['Overall_Score'].max():.2f}\")\n",
    "        print(f\"   Standard Deviation: {results_df['Overall_Score'].std():.2f}\")\n",
    "        \n",
    "    else:\n",
    "        print(\" Chunks-only evaluation failed. Check logs for details.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_chunks_only_evaluation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
