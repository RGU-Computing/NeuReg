{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5bf04b-fe28-405e-be9c-a5c79894c1e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from typing import Dict, List, Optional\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "class KGOnlyLLMJudgeEvaluator:\n",
    "    \"\"\"\n",
    "    Second Ablation Study: KG-Only LLM Judge Evaluation System\n",
    "    Evaluates QA pairs generated using only Knowledge Graph information without text chunks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, model_name: str = \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\"):\n",
    "        \"\"\"\n",
    "        Initialize the KG-Only LLM Judge Evaluator.\n",
    "        \n",
    "        Args:\n",
    "            api_key: NScale API key\n",
    "            model_name: Model identifier for the LLM judge\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.model_name = model_name\n",
    "        self.base_url = \"https://inference.api.nscale.com/v1/chat/completions\"\n",
    "        self.headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {api_key}\"\n",
    "        }\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # KG-Only evaluation prompt (modified for second ablation study)\n",
    "        self.evaluation_prompt = \"\"\"You are a STRICT QA evaluator following precise scoring guidelines for a KNOWLEDGE GRAPH-ONLY.\n",
    "\n",
    "You will evaluate a model-generated question-answer pair on FIVE metrics using a 1-5 scale where:\n",
    "5 = Excellent  \n",
    "4 = Good  \n",
    "3 = Fair  \n",
    "2 = Poor  \n",
    "1 = Very Poor\n",
    "\n",
    "\n",
    "=============================================\n",
    "DETAILED SCORING CRITERIA FOR KG-ONLY SYSTEM\n",
    "=============================================\n",
    "\n",
    "1. RELEVANCE (1-5): Does the question appropriately relate to the available KG information?\n",
    "   5: Perfectly relevant to the KG triples, clearly grounded in the graph structure\n",
    "   4: Mostly relevant, with minor deviations from KG scope\n",
    "   3: Addresses KG information but may miss some connections\n",
    "   2: Loosely related to KG, with significant irrelevant elements\n",
    "   1: Entirely irrelevant or unrelated to the available KG triples\n",
    "\n",
    "2. ACCURACY (1-5): Is the answer factually correct based on the KG triples?\n",
    "   5: All facts are accurate and fully derivable from KG triples\n",
    "   4: Mostly accurate; contains only minor factual issues\n",
    "   3: Some factual inconsistencies or unsupported assumptions\n",
    "   2: Several factual errors that contradict KG information\n",
    "   1: Mostly inaccurate or contradicts KG triples\n",
    "\n",
    "3. COMPLETENESS (1-5): Does the answer fully address the question using available KG information?\n",
    "   5: Thorough response using all relevant KG connections\n",
    "   4: Covers most aspects but may miss some KG relationships\n",
    "   3: Addresses main question but omits important KG details\n",
    "   2: Partial answer with significant gaps in KG utilization\n",
    "   1: Severely incomplete or fails to use relevant KG information\n",
    "\n",
    "4. FLUENCY (1-5): Is the answer well-written and grammatically correct?\n",
    "   5: Excellent grammar and clarity; highly readable despite KG-only constraints\n",
    "   4: Minor grammatical or structural issues\n",
    "   3: Understandable, but contains noticeable language errors\n",
    "   2: Somewhat unclear due to poor grammar or phrasing\n",
    "   1: Difficult to read or understand\n",
    "\n",
    "5. KG ALIGNMENT (1-5): How well does the answer reflect the KG triples?\n",
    "   5: Effectively uses KG relationships; no contradictions; may include additional relevant info\n",
    "   4: Uses most relevant KG information correctly; may omit minor details, no contradictions\n",
    "   3: Uses some KG information; may miss important relationships but generally consistent\n",
    "   2: Limited use of KG information; may contain minor contradictions or misinterpretations\n",
    "   1: Ignores KG information entirely or includes clear contradictions\n",
    "\n",
    "EVALUATION GUIDELINES FOR KG-ONLY SYSTEM:\n",
    "• Remember: This system has NO access to source text chunks\n",
    "• Focus on how well the system leverages graph relationships\n",
    "• Penalize hallucinations not supported by KG triples\n",
    "• Reward effective connection of multiple KG relationships\n",
    "• Consider that some limitations are expected due to KG-only constraint\n",
    "\n",
    "\n",
    "==============================\n",
    "INPUT\n",
    "==============================\n",
    "**Question:** {question}\n",
    "**Answer:** {answer}\n",
    "**Available Knowledge Graph Triples:** {kg_triples}\n",
    "\n",
    "==============================\n",
    "RESPONSE FORMAT\n",
    "==============================\n",
    "Provide ONLY the numerical scores in this exact format (no explanation):\n",
    "\n",
    "Relevance: X  \n",
    "Accuracy: X  \n",
    "Completeness: X  \n",
    "Fluency: X  \n",
    "KG_Alignment: X\n",
    "\n",
    "\n",
    "Where X is a number from 1 to 5.\"\"\"\n",
    "\n",
    "    def extract_triples_from_item(self, item: Dict) -> str:\n",
    "        \"\"\"Extract and format KG triples from a QA item with KG-only dataset structure.\"\"\"\n",
    "        triples = []\n",
    "        \n",
    "        # Extract from ground_truth.source_triples structure\n",
    "        if \"ground_truth\" in item and \"source_triples\" in item[\"ground_truth\"]:\n",
    "            source_triples = item[\"ground_truth\"][\"source_triples\"]\n",
    "            \n",
    "            # Handle list of triple dictionaries\n",
    "            if isinstance(source_triples, list):\n",
    "                for triple_dict in source_triples:\n",
    "                    if isinstance(triple_dict, dict) and all(key in triple_dict for key in [\"subject\", \"predicate\", \"object\"]):\n",
    "                        triple = f\"{triple_dict['subject']} → {triple_dict['predicate']} → {triple_dict['object']}\"\n",
    "                        triples.append(triple)\n",
    "            \n",
    "            # Handle single triple dictionary\n",
    "            elif isinstance(source_triples, dict) and all(key in source_triples for key in [\"subject\", \"predicate\", \"object\"]):\n",
    "                triple = f\"{source_triples['subject']} → {source_triples['predicate']} → {source_triples['object']}\"\n",
    "                triples.append(triple)\n",
    "        \n",
    "        # Fallback: try to find triples in other locations\n",
    "        if not triples:\n",
    "            def extract_all_triples(obj):\n",
    "                found_triples = []\n",
    "                if isinstance(obj, dict):\n",
    "                    if set(obj.keys()) >= {\"subject\", \"predicate\", \"object\"}:\n",
    "                        triple = f\"{obj['subject']} → {obj['predicate']} → {obj['object']}\"\n",
    "                        found_triples.append(triple)\n",
    "                    for value in obj.values():\n",
    "                        found_triples.extend(extract_all_triples(value))\n",
    "                elif isinstance(obj, list):\n",
    "                    for item in obj:\n",
    "                        found_triples.extend(extract_all_triples(item))\n",
    "                return found_triples\n",
    "            \n",
    "            triples = extract_all_triples(item)\n",
    "        \n",
    "        if triples:\n",
    "            formatted_triples = []\n",
    "            for i, triple in enumerate(triples, 1):\n",
    "                formatted_triples.append(f\"{i}. {triple}\")\n",
    "            return \"\\n\".join(formatted_triples)\n",
    "        return \"No KG triples available\"\n",
    "\n",
    "    def call_llm_judge(self, question: str, answer: str, kg_triples: str, \n",
    "                      max_retries: int = 3) -> Optional[Dict[str, int]]:\n",
    "        \"\"\"\n",
    "        Call the LLM judge to evaluate a KG-only QA pair with retry logic.\n",
    "        \n",
    "        Args:\n",
    "            question: The question to evaluate\n",
    "            answer: The answer to evaluate\n",
    "            kg_triples: Knowledge graph triples (only source of information)\n",
    "            max_retries: Maximum number of retry attempts\n",
    "            \n",
    "        Returns:\n",
    "            Dict with evaluation scores or None if failed\n",
    "        \"\"\"\n",
    "        # Format the prompt for KG-only evaluation\n",
    "        prompt = self.evaluation_prompt.format(\n",
    "            question=question,\n",
    "            answer=answer,\n",
    "            kg_triples=kg_triples\n",
    "        )\n",
    "        \n",
    "        # Prepare API request\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"temperature\": 0.0,  # Deterministic for consistent scoring\n",
    "            \"max_tokens\": 800,   # Short response expected\n",
    "            \"top_p\": 0.8\n",
    "        }\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.post(\n",
    "                    self.base_url,\n",
    "                    headers=self.headers,\n",
    "                    json=payload,\n",
    "                    timeout=45\n",
    "                )\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    result = response.json()\n",
    "                    msg = result['choices'][0].get('message', {})\n",
    "                    content = msg.get(\"content\", \"\") or msg.get(\"reasoning_content\", \"\")\n",
    "\n",
    "                    if content:\n",
    "                        parsed_result = self.parse_evaluation_response(content)\n",
    "                        if parsed_result:\n",
    "                            return parsed_result\n",
    "                    else:\n",
    "                        self.logger.warning(f\"Empty response content from model\")\n",
    "                \n",
    "                elif response.status_code == 429:\n",
    "                    wait_time = 2 ** attempt\n",
    "                    self.logger.warning(f\"Rate limit hit, waiting {wait_time}s before retry {attempt + 1}/{max_retries}\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    self.logger.error(f\"API Error {response.status_code}: {response.text}\")\n",
    "            \n",
    "            except requests.exceptions.Timeout:\n",
    "                self.logger.warning(f\"Timeout on attempt {attempt + 1}/{max_retries}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(2 ** attempt)\n",
    "            \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Request failed on attempt {attempt + 1}: {str(e)}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(2 ** attempt)\n",
    "    \n",
    "        return None\n",
    "\n",
    "    def parse_evaluation_response(self, response: str) -> Optional[Dict[str, int]]:\n",
    "        \"\"\"\n",
    "        Parse the LLM response to extract numerical scores for KG-only evaluation.\n",
    "        \n",
    "        Expected format includes KG_Utilization instead of KG_Alignment:\n",
    "        Relevance: 4\n",
    "        Accuracy: 5\n",
    "        Completeness: 3\n",
    "        Fluency: 4\n",
    "        Kg Alignement 2\n",
    "        \"\"\"\n",
    "        try:\n",
    "            scores = {}\n",
    "            lines = response.strip().split('\\n')\n",
    "            \n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if ':' in line:\n",
    "                    parts = line.split(':', 1)\n",
    "                    if len(parts) == 2:\n",
    "                        metric = parts[0].strip()\n",
    "                        score_text = parts[1].strip()\n",
    "                        \n",
    "                        # Improved regex pattern\n",
    "                        score_match = re.search(r':\\s*([1-5])(?:/5)?', line)\n",
    "                        if score_match:\n",
    "                            score = int(score_match.group(1))\n",
    "                            \n",
    "                            # Normalize metric names (adapted for KG-only)\n",
    "                            metric_lower = metric.lower()\n",
    "                            if 'relevance' in metric_lower:\n",
    "                                scores['Relevance'] = score\n",
    "                            elif 'accuracy' in metric_lower:\n",
    "                                scores['Accuracy'] = score\n",
    "                            elif 'completeness' in metric_lower:\n",
    "                                scores['Completeness'] = score\n",
    "                            elif 'fluency' in metric_lower:\n",
    "                                scores['Fluency'] = score\n",
    "                            elif 'kg' in metric_lower or 'alignment' in metric_lower:\n",
    "                                scores['KG_Alignment'] = score\n",
    "            \n",
    "            # Validate we have all 5 scores for KG-only evaluation\n",
    "            expected_metrics = ['Relevance', 'Accuracy', 'Completeness', 'Fluency', 'KG_Alignment']\n",
    "            if all(metric in scores for metric in expected_metrics):\n",
    "                return scores\n",
    "            else:\n",
    "                self.logger.warning(f\"Missing metrics in response: {response}\")\n",
    "                return self._parse_with_fallback(response)\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to parse response: {response}. Error: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _parse_with_fallback(self, response: str) -> Optional[Dict[str, int]]:\n",
    "        \"\"\"Fallback parsing with multiple regex patterns for KG-only evaluation.\"\"\"\n",
    "        patterns = [\n",
    "            r':\\s*([1-5])(?:/5)?',          # Primary pattern\n",
    "            r'\\b([1-5])\\b',                 # Simple number pattern\n",
    "            r'([1-5])\\s*(?:out of 5|/5)?'   # Alternative pattern\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            try:\n",
    "                scores = {}\n",
    "                lines = response.strip().split('\\n')\n",
    "                \n",
    "                for line in lines:\n",
    "                    if ':' in line:\n",
    "                        parts = line.split(':', 1)\n",
    "                        if len(parts) == 2:\n",
    "                            metric = parts[0].strip()\n",
    "                            score_text = parts[1].strip()\n",
    "                            \n",
    "                            score_match = re.search(pattern, score_text)\n",
    "                            if score_match:\n",
    "                                score = int(score_match.group(1))\n",
    "                                \n",
    "                                # Normalize metric names for KG-only\n",
    "                                metric_lower = metric.lower()\n",
    "                                if 'relevance' in metric_lower:\n",
    "                                    scores['Relevance'] = score\n",
    "                                elif 'accuracy' in metric_lower:\n",
    "                                    scores['Accuracy'] = score\n",
    "                                elif 'completeness' in metric_lower:\n",
    "                                    scores['Completeness'] = score\n",
    "                                elif 'fluency' in metric_lower:\n",
    "                                    scores['Fluency'] = score\n",
    "                                elif 'kg' in metric_lower or 'alignment' in metric_lower:\n",
    "                                    scores['KG_Alignment'] = score\n",
    "                \n",
    "                # Check if this pattern worked\n",
    "                expected_metrics = ['Relevance', 'Accuracy', 'Completeness', 'Fluency', 'KG_Alignment']\n",
    "                if all(metric in scores for metric in expected_metrics):\n",
    "                    return scores\n",
    "                    \n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def load_qa_dataset(self, file_path: str) -> List[Dict]:\n",
    "        \"\"\"Load KG-only QA dataset from JSON file with correct structure.\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Handle nested structure: root -> metadata -> queries\n",
    "            if \"metadata\" in data and \"queries\" in data[\"metadata\"]:\n",
    "                queries = data[\"metadata\"][\"queries\"]\n",
    "            elif \"queries\" in data:\n",
    "                queries = data[\"queries\"] \n",
    "            else:\n",
    "                # Fallback: assume data is the queries list directly\n",
    "                queries = data if isinstance(data, list) else []\n",
    "            \n",
    "            self.logger.info(f\"Loaded {len(queries)} KG-only QA pairs from {file_path}\")\n",
    "            \n",
    "            # Log sample structure for debugging\n",
    "            if queries:\n",
    "                sample = queries[0]\n",
    "                self.logger.info(f\"Sample structure keys: {list(sample.keys())}\")\n",
    "                if \"ground_truth\" in sample:\n",
    "                    self.logger.info(f\"Ground truth keys: {list(sample['ground_truth'].keys())}\")\n",
    "            \n",
    "            return queries\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load dataset: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def save_checkpoint(self, results: List[Dict], checkpoint_path: str):\n",
    "        \"\"\"Save evaluation results as checkpoint.\"\"\"\n",
    "        try:\n",
    "            df = pd.DataFrame(results)\n",
    "            df.to_csv(checkpoint_path, index=False)\n",
    "            self.logger.info(f\"Checkpoint saved: {len(results)} results to {checkpoint_path}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to save checkpoint: {str(e)}\")\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_path: str) -> List[Dict]:\n",
    "        \"\"\"Load evaluation results from checkpoint.\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(checkpoint_path):\n",
    "                df = pd.read_csv(checkpoint_path)\n",
    "                results = df.to_dict('records')\n",
    "                self.logger.info(f\"Loaded checkpoint: {len(results)} results from {checkpoint_path}\")\n",
    "                return results\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load checkpoint: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def evaluate_dataset(self, dataset_path: str, output_path: str, \n",
    "                        sample_size: Optional[int] = None, \n",
    "                        delay_seconds: float = 0.5,\n",
    "                        checkpoint_interval: int = 50) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Evaluate KG-only QA dataset using LLM judge.\n",
    "        \n",
    "        Args:\n",
    "            dataset_path: Path to the KG-only QA dataset JSON file\n",
    "            output_path: Path to save evaluation results\n",
    "            sample_size: Number of samples to evaluate (None for all)\n",
    "            delay_seconds: Delay between API calls to avoid rate limits\n",
    "            checkpoint_interval: Save checkpoint every N evaluations\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with evaluation results\n",
    "        \"\"\"\n",
    "        # Load dataset\n",
    "        qa_items = self.load_qa_dataset(dataset_path)\n",
    "        if not qa_items:\n",
    "            self.logger.error(\"No QA items loaded. Exiting.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Sample subset if requested\n",
    "        if sample_size and sample_size < len(qa_items):\n",
    "            import random\n",
    "            random.seed(42)  # For reproducibility\n",
    "            qa_items = random.sample(qa_items, sample_size)\n",
    "            self.logger.info(f\"Sampling {sample_size} items for evaluation\")\n",
    "        \n",
    "        # Setup checkpoint\n",
    "        checkpoint_path = f\"{output_path}.checkpoint\"\n",
    "        results = self.load_checkpoint(checkpoint_path)\n",
    "        \n",
    "        # Track processed items\n",
    "        processed_ids = {result['qa_id'] for result in results} if results else set()\n",
    "        \n",
    "        # Filter unprocessed items\n",
    "        remaining_items = [item for item in qa_items \n",
    "                          if item.get('id', f'item_{qa_items.index(item)}') not in processed_ids]\n",
    "        \n",
    "        if processed_ids:\n",
    "            self.logger.info(f\"Resuming from checkpoint: {len(results)} completed, {len(remaining_items)} remaining\")\n",
    "        \n",
    "        failed_evaluations = 0\n",
    "        \n",
    "        # Progress bar with correct total and initial values\n",
    "        total_items = len(qa_items)\n",
    "        completed_items = len(results)\n",
    "        pbar = tqdm(\n",
    "            remaining_items, \n",
    "            desc=\"Evaluating KG-only QA pairs\",\n",
    "            total=total_items,\n",
    "            initial=completed_items,\n",
    "            unit=\"items\"\n",
    "        )\n",
    "        \n",
    "        for i, item in enumerate(remaining_items):\n",
    "            try:\n",
    "                # Extract data from item with KG-only dataset structure\n",
    "                qa_id = item.get('id', f'item_{qa_items.index(item)}')\n",
    "                question = item.get('question', '')\n",
    "                answer = item.get('answer', '')\n",
    "                question_type = item.get('question_type', 'unknown')\n",
    "                generation_method = item.get('generation_method', 'unknown')\n",
    "                \n",
    "                # Verify this is from KG-only ablation study\n",
    "                if generation_method != 'ablation_kg_only':\n",
    "                    self.logger.warning(f\"Item {qa_id} not from KG-only ablation (method: {generation_method})\")\n",
    "                \n",
    "                # Extract KG triples (only source of information for ablation study)\n",
    "                kg_triples = self.extract_triples_from_item(item)\n",
    "                \n",
    "                # Skip if essential data is missing\n",
    "                if not question or not answer:\n",
    "                    self.logger.warning(f\"Skipping item {qa_id}: missing question or answer\")\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                \n",
    "                # Skip if no KG triples available\n",
    "                if kg_triples == \"No KG triples available\":\n",
    "                    self.logger.warning(f\"Skipping item {qa_id}: no KG triples found\")\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                \n",
    "                # Call LLM judge (KG-only evaluation)\n",
    "                evaluation = self.call_llm_judge(question, answer, kg_triples)\n",
    "                \n",
    "                if evaluation:\n",
    "                    # Store results with additional KG-only metadata\n",
    "                    result = {\n",
    "                        'qa_id': qa_id,\n",
    "                        'question_type': question_type,\n",
    "                        'generation_method': generation_method,\n",
    "                        'question': question,\n",
    "                        'answer': answer,\n",
    "                        'num_source_triples': item.get('ground_truth', {}).get('num_source_triples', 0),\n",
    "                        'text_context_used': item.get('ground_truth', {}).get('text_context_used', False),\n",
    "                        'ablation_study': item.get('ground_truth', {}).get('ablation_study', 'unknown'),\n",
    "                        'Relevance': evaluation['Relevance'],\n",
    "                        'Accuracy': evaluation['Accuracy'],\n",
    "                        'Completeness': evaluation['Completeness'],\n",
    "                        'Fluency': evaluation['Fluency'],\n",
    "                        'KG_Alignment': evaluation['KG_Alignment'],\n",
    "                        'Overall_Score': sum(evaluation.values()) / len(evaluation)\n",
    "                    }\n",
    "                    results.append(result)\n",
    "                    \n",
    "                    # Update progress bar with detailed status\n",
    "                    pbar.set_postfix({\n",
    "                        'Completed': len(results),\n",
    "                        'Failed': failed_evaluations,\n",
    "                        'Success_Rate': f\"{len(results)/(len(results)+failed_evaluations)*100:.1f}%\",\n",
    "                        'Last_Score': f\"{result['Overall_Score']:.1f}\"\n",
    "                    })\n",
    "                    \n",
    "                    # Save checkpoint\n",
    "                    if len(results) % checkpoint_interval == 0:\n",
    "                        self.save_checkpoint(results, checkpoint_path)\n",
    "                    \n",
    "                else:\n",
    "                    failed_evaluations += 1\n",
    "                    self.logger.warning(f\"Failed to evaluate item {qa_id}\")\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Rate limiting\n",
    "                if delay_seconds > 0:\n",
    "                    time.sleep(delay_seconds)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                failed_evaluations += 1\n",
    "                self.logger.error(f\"Error processing item {qa_id}: {str(e)}\")\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        # Final save\n",
    "        if results:\n",
    "            df = pd.DataFrame(results)\n",
    "            df.to_csv(output_path, index=False)\n",
    "            \n",
    "            # Clean up checkpoint\n",
    "            if os.path.exists(checkpoint_path):\n",
    "                os.remove(checkpoint_path)\n",
    "            \n",
    "            # Print summary statistics\n",
    "            self.print_evaluation_summary(df, failed_evaluations)\n",
    "            \n",
    "            return df\n",
    "        else:\n",
    "            self.logger.error(\"No successful evaluations completed\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def print_evaluation_summary(self, df: pd.DataFrame, failed_count: int):\n",
    "        \"\"\"Print comprehensive summary statistics of the KG-only evaluation.\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\" SECOND ABLATION STUDY: KG-ONLY EVALUATION SUMMARY\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        print(f\" Evaluation Statistics:\")\n",
    "        print(f\"   Total Evaluated: {len(df)}\")\n",
    "        print(f\"   Failed Evaluations: {failed_count}\")\n",
    "        print(f\"   Success Rate: {len(df)/(len(df)+failed_count)*100:.1f}%\")\n",
    "        print(f\"   Evaluation Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        print(f\"\\n Average Scores by Metric (KG-Only System):\")\n",
    "        metrics = ['Relevance', 'Accuracy', 'Completeness', 'Fluency', 'KG_Alignment']\n",
    "        for metric in metrics:\n",
    "            mean_score = df[metric].mean()\n",
    "            std_score = df[metric].std()\n",
    "            min_score = df[metric].min()\n",
    "            max_score = df[metric].max()\n",
    "            print(f\"   {metric}: {mean_score:.2f} ± {std_score:.2f} (range: {min_score}-{max_score})\")\n",
    "        \n",
    "        print(f\"\\n Overall Performance:\")\n",
    "        print(f\"   Mean Overall Score: {df['Overall_Score'].mean():.2f}\")\n",
    "        print(f\"   Median Overall Score: {df['Overall_Score'].median():.2f}\")\n",
    "        print(f\"   Best Score: {df['Overall_Score'].max():.2f}\")\n",
    "        print(f\"   Worst Score: {df['Overall_Score'].min():.2f}\")\n",
    "        \n",
    "        # Score distribution\n",
    "        print(f\"\\n Score Distribution:\")\n",
    "        score_ranges = [(1.0, 2.0), (2.0, 3.0), (3.0, 4.0), (4.0, 5.0)]\n",
    "        for min_score, max_score in score_ranges:\n",
    "            count = len(df[(df['Overall_Score'] >= min_score) & (df['Overall_Score'] < max_score)])\n",
    "            percentage = (count / len(df)) * 100\n",
    "            print(f\"   {min_score:.1f}-{max_score:.1f}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Perfect scores\n",
    "        perfect_scores = len(df[df['Overall_Score'] == 5.0])\n",
    "        print(f\"   Perfect (5.0): {perfect_scores} ({(perfect_scores/len(df))*100:.1f}%)\")\n",
    "        \n",
    "        # KG-specific analysis\n",
    "        print(f\"\\n KG-Only System Analysis:\")\n",
    "        print(f\"   Average KG Alignment: {df['KG_Alignment'].mean():.2f}\")\n",
    "        print(f\"   KG Alignment Range: {df['KG_Alignment'].min():.1f} - {df['KG_Alignment'].max():.1f}\")\n",
    "        \n",
    "        # Dataset-specific insights\n",
    "        if 'num_source_triples' in df.columns:\n",
    "            print(f\"   Average Source Triples: {df['num_source_triples'].mean():.1f}\")\n",
    "            print(f\"   Triple Count Range: {df['num_source_triples'].min()}-{df['num_source_triples'].max()}\")\n",
    "        \n",
    "        if 'text_context_used' in df.columns:\n",
    "            text_context_count = df['text_context_used'].sum()\n",
    "            print(f\"   Text Context Used: {text_context_count} ({(text_context_count/len(df))*100:.1f}%)\")\n",
    "        \n",
    "        if 'ablation_study' in df.columns:\n",
    "            kg_only_count = len(df[df['ablation_study'] == 'kg_only'])\n",
    "            print(f\"   KG-Only Items: {kg_only_count} ({(kg_only_count/len(df))*100:.1f}%)\")\n",
    "        \n",
    "        # Low KG alignment items\n",
    "        low_kg_align = len(df[df['KG_Alignment'] <= 2])\n",
    "        print(f\"   Poor KG Alignment (≤2): {low_kg_align} ({(low_kg_align/len(df))*100:.1f}%)\")\n",
    "        \n",
    "        # Correlation analysis between triple count and performance\n",
    "        if 'num_source_triples' in df.columns and len(df) > 1:\n",
    "            correlation = df['num_source_triples'].corr(df['Overall_Score'])\n",
    "            print(f\"   Triple Count vs Performance Correlation: {correlation:.3f}\")\n",
    "        \n",
    "        if 'question_type' in df.columns:\n",
    "            print(f\"\\n Performance by Question Type (KG-Only):\")\n",
    "            type_summary = df.groupby('question_type').agg({\n",
    "                'Overall_Score': ['mean', 'std', 'count'],\n",
    "                'Relevance': 'mean',\n",
    "                'Accuracy': 'mean',\n",
    "                'Completeness': 'mean',\n",
    "                'Fluency': 'mean',\n",
    "                'KG_Alignment': 'mean'\n",
    "            }).round(3)\n",
    "            \n",
    "            for qtype in type_summary.index:\n",
    "                stats = type_summary.loc[qtype]\n",
    "                mean_score = stats[('Overall_Score', 'mean')]\n",
    "                std_score = stats[('Overall_Score', 'std')]\n",
    "                count = stats[('Overall_Score', 'count')]\n",
    "                kg_align = stats[('KG_Alignment', 'mean')]\n",
    "                print(f\"   {qtype}: {mean_score:.2f} ± {std_score:.2f} (n={count}, KG_Align={kg_align:.2f})\")\n",
    "\n",
    "def run_kg_only_evaluation():\n",
    "    \"\"\"Run KG-only evaluation for the second ablation study.\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    API_KEY = \"\"  # Replace with your actual API key\n",
    "    DATASET_PATH = \"Ablation_2_kg_only_qa_dataset.json\"  # Update with your KG-only dataset path\n",
    "    OUTPUT_PATH = \"DeepSeek-R1_Distill_Llma70B_kg_only_evaluation_results.csv\"\n",
    "    \n",
    "    # Initialize KG-only evaluator\n",
    "    evaluator = KGOnlyLLMJudgeEvaluator(api_key=API_KEY)\n",
    "    \n",
    "    print(\"┌\" + \"─\" * 68 + \"┐\")\n",
    "    print(\"│ SECOND ABLATION STUDY: KG-ONLY EVALUATION                       │\")\n",
    "    print(\"├\" + \"─\" * 68 + \"┤\")\n",
    "    print(\"│ This evaluates QA pairs generated using ONLY Knowledge Graph    │\")\n",
    "    print(\"│ information without any source text chunks.                     │\")\n",
    "   \n",
    "    # Confirm before starting\n",
    "    response = input(\"\\nProceed with KG-only evaluation? (y/n): \").strip().lower()\n",
    "    if response != 'y':\n",
    "        print(\"✗ Evaluation cancelled.\")\n",
    "        return\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run evaluation on KG-only dataset\n",
    "    results_df = evaluator.evaluate_dataset(\n",
    "        dataset_path=DATASET_PATH,\n",
    "        output_path=OUTPUT_PATH,\n",
    "        sample_size=None,  # Process all items\n",
    "        delay_seconds=0.5,  # 0.5 second delay between requests\n",
    "        checkpoint_interval=50  # Save every 50 evaluations\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        print(f\" KG-ONLY EVALUATION COMPLETED!\")\n",
    "        print(f\"  Total time: {duration/60:.1f} minutes\")\n",
    "        print(f\"  Evaluated: {len(results_df)} QA pairs\")\n",
    "        print(f\"  Results saved to: {OUTPUT_PATH}\")\n",
    "        \n",
    "        # Additional analysis\n",
    "        print(f\"\\n Processing Statistics:\")\n",
    "        print(f\"   Average processing time: {duration/len(results_df):.2f} seconds per QA pair\")\n",
    "        print(f\"   Items per minute: {len(results_df)/(duration/60):.1f}\")\n",
    "        \n",
    "        # KG-specific insights\n",
    "        print(f\"\\n KG-Only System Insights:\")\n",
    "        print(f\"   Mean Overall Score: {results_df['Overall_Score'].mean():.2f}\")\n",
    "        print(f\"   Mean KG Alignment: {results_df['KG_Alignment'].mean():.2f}\")\n",
    "        print(f\"   Score Range: {results_df['Overall_Score'].min():.2f} - {results_df['Overall_Score'].max():.2f}\")\n",
    "        print(f\"   Standard Deviation: {results_df['Overall_Score'].std():.2f}\")\n",
    "\n",
    "        \n",
    "    else:\n",
    "        print(\" Evaluation failed. Check logs for details.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_kg_only_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb0ecd3-bb83-4c97-a3ea-ef885a10a318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
