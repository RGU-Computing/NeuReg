{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f06529-adc0-42d7-849d-4c323ea9a018",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from typing import Dict, List, Optional\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install and import together package\n",
    "try:\n",
    "    from together import Together\n",
    "except ImportError:\n",
    "    print(\"Installing together package...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"together\"])\n",
    "    from together import Together\n",
    "\n",
    "class KGOnlyLLMJudgeEvaluator:\n",
    "    \"\"\"\n",
    "    Second Ablation Study: KG-Only LLM Judge Evaluation System\n",
    "    Evaluates QA pairs generated using only Knowledge Graph information without text chunks.\n",
    "    With Google Gemma model.\n",
    "    \n",
    "    Final Production Version - Ready for Testing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, model_name: str = \"google/gemma-2-27b-it\"):\n",
    "        \"\"\"\n",
    "        Initialize the KG-Only LLM Judge Evaluator with Together.ai.\n",
    "        \n",
    "        Args:\n",
    "            api_key: Together.ai API key\n",
    "            model_name: Model identifier for the LLM judge\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # Set environment variable and initialize Together client\n",
    "        os.environ[\"TOGETHER_API_KEY\"] = api_key\n",
    "        \n",
    "        try:\n",
    "            self.client = Together()\n",
    "            self.logger = self._setup_logging()\n",
    "            self.logger.info(f\"✓ Together.ai client initialized with model: {model_name}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to initialize Together.ai client: {e}\")\n",
    "        \n",
    "        self.evaluation_prompt = \"\"\"You are a STRICT QA evaluator following precise scoring guidelines for a KNOWLEDGE GRAPH-ONLY.\n",
    "\n",
    "You will evaluate a model-generated question-answer pair on FIVE metrics using a 1-5 scale where:\n",
    "5 = Excellent  \n",
    "4 = Good  \n",
    "3 = Fair  \n",
    "2 = Poor  \n",
    "1 = Very Poor\n",
    "\n",
    "\n",
    "=============================================\n",
    "DETAILED SCORING CRITERIA FOR KG-ONLY SYSTEM\n",
    "=============================================\n",
    "\n",
    "1. RELEVANCE (1-5): Does the question appropriately relate to the available KG information?\n",
    "   5: Perfectly relevant to the KG triples, clearly grounded in the graph structure\n",
    "   4: Mostly relevant, with minor deviations from KG scope\n",
    "   3: Addresses KG information but may miss some connections\n",
    "   2: Loosely related to KG, with significant irrelevant elements\n",
    "   1: Entirely irrelevant or unrelated to the available KG triples\n",
    "\n",
    "2. ACCURACY (1-5): Is the answer factually correct based on the KG triples?\n",
    "   5: All facts are accurate and fully derivable from KG triples\n",
    "   4: Mostly accurate; contains only minor factual issues\n",
    "   3: Some factual inconsistencies or unsupported assumptions\n",
    "   2: Several factual errors that contradict KG information\n",
    "   1: Mostly inaccurate or contradicts KG triples\n",
    "\n",
    "3. COMPLETENESS (1-5): Does the answer fully address the question using available KG information?\n",
    "   5: Thorough response using all relevant KG connections\n",
    "   4: Covers most aspects but may miss some KG relationships\n",
    "   3: Addresses main question but omits important KG details\n",
    "   2: Partial answer with significant gaps in KG utilization\n",
    "   1: Severely incomplete or fails to use relevant KG information\n",
    "\n",
    "4. FLUENCY (1-5): Is the answer well-written and grammatically correct?\n",
    "   5: Excellent grammar and clarity; highly readable despite KG-only constraints\n",
    "   4: Minor grammatical or structural issues\n",
    "   3: Understandable, but contains noticeable language errors\n",
    "   2: Somewhat unclear due to poor grammar or phrasing\n",
    "   1: Difficult to read or understand\n",
    "\n",
    "5. KG ALIGNMENT (1-5): How well does the answer reflect the KG triples?\n",
    "   5: Effectively uses KG relationships; no contradictions; may include additional relevant info\n",
    "   4: Uses most relevant KG information correctly; may omit minor details, no contradictions\n",
    "   3: Uses some KG information; may miss important relationships but generally consistent\n",
    "   2: Limited use of KG information; may contain minor contradictions or misinterpretations\n",
    "   1: Ignores KG information entirely or includes clear contradictions\n",
    "\n",
    "EVALUATION GUIDELINES FOR KG-ONLY SYSTEM:\n",
    "• Remember: This system has NO access to source text chunks\n",
    "• Focus on how well the system leverages graph relationships\n",
    "• Penalize hallucinations not supported by KG triples\n",
    "• Reward effective connection of multiple KG relationships\n",
    "• Consider that some limitations are expected due to KG-only constraint\n",
    "\n",
    "\n",
    "==============================\n",
    "INPUT\n",
    "==============================\n",
    "**Question:** {question}\n",
    "**Answer:** {answer}\n",
    "**Available Knowledge Graph Triples:** {kg_triples}\n",
    "\n",
    "==============================\n",
    "RESPONSE FORMAT\n",
    "==============================\n",
    "Provide ONLY the numerical scores in this exact format (no explanation):\n",
    "\n",
    "Relevance: X  \n",
    "Accuracy: X  \n",
    "Completeness: X  \n",
    "Fluency: X  \n",
    "KG_Alignment: X\n",
    "\n",
    "\n",
    "Where X is a number from 1 to 5.\"\"\"\n",
    "\n",
    "    def _setup_logging(self) -> logging.Logger:\n",
    "        \"\"\"Setup comprehensive logging for the evaluation process.\"\"\"\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.setLevel(logging.INFO)\n",
    "        \n",
    "        # Clear existing handlers\n",
    "        for handler in logger.handlers[:]:\n",
    "            logger.removeHandler(handler)\n",
    "        \n",
    "        # Create formatter\n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "            datefmt='%Y-%m-%d %H:%M:%S'\n",
    "        )\n",
    "        \n",
    "        # Console handler\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setLevel(logging.INFO)\n",
    "        console_handler.setFormatter(formatter)\n",
    "        logger.addHandler(console_handler)\n",
    "        \n",
    "        # File handler\n",
    "        try:\n",
    "            log_filename = f\"kg_only_evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "            file_handler = logging.FileHandler(log_filename)\n",
    "            file_handler.setLevel(logging.DEBUG)\n",
    "            file_handler.setFormatter(formatter)\n",
    "            logger.addHandler(file_handler)\n",
    "            logger.info(f\"Log file created: {log_filename}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not create log file: {e}\")\n",
    "        \n",
    "        return logger\n",
    "\n",
    "    def extract_triples_from_item(self, item: Dict) -> str:\n",
    "        \"\"\"Extract and format KG triples from a QA item with robust error handling.\"\"\"\n",
    "        triples = []\n",
    "        \n",
    "        try:\n",
    "            # Extract from ground_truth.source_triples structure\n",
    "            if \"ground_truth\" in item and \"source_triples\" in item[\"ground_truth\"]:\n",
    "                source_triples = item[\"ground_truth\"][\"source_triples\"]\n",
    "                \n",
    "                # Handle list of triple dictionaries\n",
    "                if isinstance(source_triples, list):\n",
    "                    for triple_dict in source_triples:\n",
    "                        if isinstance(triple_dict, dict) and all(key in triple_dict for key in [\"subject\", \"predicate\", \"object\"]):\n",
    "                            triple = f\"{triple_dict['subject']} → {triple_dict['predicate']} → {triple_dict['object']}\"\n",
    "                            triples.append(triple)\n",
    "                \n",
    "                # Handle single triple dictionary\n",
    "                elif isinstance(source_triples, dict) and all(key in source_triples for key in [\"subject\", \"predicate\", \"object\"]):\n",
    "                    triple = f\"{source_triples['subject']} → {source_triples['predicate']} → {source_triples['object']}\"\n",
    "                    triples.append(triple)\n",
    "            \n",
    "            # Fallback: comprehensive search for triples\n",
    "            if not triples:\n",
    "                def extract_all_triples(obj, path=\"\"):\n",
    "                    found_triples = []\n",
    "                    if isinstance(obj, dict):\n",
    "                        # Check if this dict is a triple\n",
    "                        if set(obj.keys()) >= {\"subject\", \"predicate\", \"object\"}:\n",
    "                            triple = f\"{obj['subject']} → {obj['predicate']} → {obj['object']}\"\n",
    "                            found_triples.append(triple)\n",
    "                        # Recursively search nested structures\n",
    "                        for key, value in obj.items():\n",
    "                            found_triples.extend(extract_all_triples(value, f\"{path}.{key}\" if path else key))\n",
    "                    elif isinstance(obj, list):\n",
    "                        for i, item in enumerate(obj):\n",
    "                            found_triples.extend(extract_all_triples(item, f\"{path}[{i}]\" if path else f\"[{i}]\"))\n",
    "                    return found_triples\n",
    "                \n",
    "                triples = extract_all_triples(item)\n",
    "            \n",
    "            # Format triples for output\n",
    "            if triples:\n",
    "                # Remove duplicates while preserving order\n",
    "                unique_triples = list(dict.fromkeys(triples))\n",
    "                formatted_triples = []\n",
    "                for i, triple in enumerate(unique_triples, 1):\n",
    "                    formatted_triples.append(f\"{i}. {triple}\")\n",
    "                return \"\\n\".join(formatted_triples)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error extracting triples from item: {e}\")\n",
    "        \n",
    "        return \"No KG triples available\"\n",
    "\n",
    "    def call_llm_judge(self, question: str, answer: str, kg_triples: str, \n",
    "                      max_retries: int = 3) -> Optional[Dict[str, int]]:\n",
    "        \"\"\"\n",
    "        Call the LLM judge with robust error handling and retry logic.\n",
    "        \n",
    "        Args:\n",
    "            question: The question to evaluate\n",
    "            answer: The answer to evaluate\n",
    "            kg_triples: Knowledge graph triples (only source of information)\n",
    "            max_retries: Maximum number of retry attempts\n",
    "            \n",
    "        Returns:\n",
    "            Dict with evaluation scores or None if failed\n",
    "        \"\"\"\n",
    "        if not question.strip() or not answer.strip() or not kg_triples.strip():\n",
    "            self.logger.warning(\"Empty question, answer, or KG triples provided\")\n",
    "            return None\n",
    "        \n",
    "        # Format the prompt for KG-only evaluation\n",
    "        prompt = self.evaluation_prompt.format(\n",
    "            question=question.strip(),\n",
    "            answer=answer.strip(),\n",
    "            kg_triples=kg_triples.strip()\n",
    "        )\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Add timeout and improved parameters for Gemma model\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model_name,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=0.0,  # Deterministic for consistent scoring\n",
    "                    max_tokens=200,   # Increased for better response parsing\n",
    "                    top_p=0.9,       # Slightly higher for Gemma\n",
    "                    repetition_penalty=1.1,  # Reduce repetition\n",
    "                    stop=[\"###\", \"---\", \"END\"]  # Stop sequences\n",
    "                )\n",
    "                \n",
    "                content = response.choices[0].message.content.strip()\n",
    "                self.logger.debug(f\"LLM Response (attempt {attempt + 1}): {content}\")\n",
    "                \n",
    "                parsed_result = self.parse_evaluation_response(content)\n",
    "                if parsed_result:\n",
    "                    return parsed_result\n",
    "                else:\n",
    "                    self.logger.warning(f\"Failed to parse response on attempt {attempt + 1}: {content}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_msg = str(e).lower()\n",
    "                if any(term in error_msg for term in [\"rate\", \"limit\", \"quota\", \"429\"]):\n",
    "                    wait_time = min(2 ** attempt, 30)  # Cap at 30 seconds\n",
    "                    self.logger.warning(f\"Rate limit hit, waiting {wait_time}s before retry {attempt + 1}/{max_retries}\")\n",
    "                    time.sleep(wait_time)\n",
    "                elif \"timeout\" in error_msg:\n",
    "                    wait_time = 5 * (attempt + 1)\n",
    "                    self.logger.warning(f\"Timeout error, waiting {wait_time}s before retry {attempt + 1}/{max_retries}\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    self.logger.error(f\"API Error on attempt {attempt + 1}: {e}\")\n",
    "                    if attempt < max_retries - 1:\n",
    "                        time.sleep(2 ** attempt)\n",
    "        \n",
    "        self.logger.error(f\"Failed to get valid response after {max_retries} attempts\")\n",
    "        return None\n",
    "\n",
    "    def parse_evaluation_response(self, response: str) -> Optional[Dict[str, int]]:\n",
    "        \"\"\"\n",
    "        Parse LLM response with improved pattern matching and validation.\n",
    "        \"\"\"\n",
    "        if not response or not response.strip():\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            scores = {}\n",
    "            lines = response.strip().split('\\n')\n",
    "            \n",
    "            # Primary parsing with exact format matching\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if ':' in line and not line.startswith('#'):\n",
    "                    parts = line.split(':', 1)\n",
    "                    if len(parts) == 2:\n",
    "                        metric = parts[0].strip()\n",
    "                        score_part = parts[1].strip()\n",
    "                        \n",
    "                        # Extract numeric score with multiple patterns\n",
    "                        score_patterns = [\n",
    "                            r'^([1-5])(?:/5)?(?:\\s|$)',  # Start with score\n",
    "                            r':\\s*([1-5])(?:/5)?',       # After colon\n",
    "                            r'\\b([1-5])(?:/5)?\\b'        # Anywhere in text\n",
    "                        ]\n",
    "                        \n",
    "                        score = None\n",
    "                        for pattern in score_patterns:\n",
    "                            match = re.search(pattern, score_part)\n",
    "                            if match:\n",
    "                                score = int(match.group(1))\n",
    "                                break\n",
    "                        \n",
    "                        if score is not None and 1 <= score <= 5:\n",
    "                            # Normalize metric names with fuzzy matching\n",
    "                            metric_lower = metric.lower().replace('_', ' ')\n",
    "                            if any(term in metric_lower for term in ['relevance', 'relevant']):\n",
    "                                scores['Relevance'] = score\n",
    "                            elif any(term in metric_lower for term in ['accuracy', 'accurate']):\n",
    "                                scores['Accuracy'] = score\n",
    "                            elif any(term in metric_lower for term in ['completeness', 'complete']):\n",
    "                                scores['Completeness'] = score\n",
    "                            elif any(term in metric_lower for term in ['fluency', 'fluent']):\n",
    "                                scores['Fluency'] = score\n",
    "                            elif any(term in metric_lower for term in ['kg', 'alignment', 'align']):\n",
    "                                scores['KG_Alignment'] = score\n",
    "            \n",
    "            # Validate all required metrics are present\n",
    "            expected_metrics = ['Relevance', 'Accuracy', 'Completeness', 'Fluency', 'KG_Alignment']\n",
    "            if all(metric in scores for metric in expected_metrics):\n",
    "                # Additional validation: check score ranges\n",
    "                if all(1 <= score <= 5 for score in scores.values()):\n",
    "                    self.logger.debug(f\"Successfully parsed scores: {scores}\")\n",
    "                    return scores\n",
    "                else:\n",
    "                    self.logger.warning(f\"Scores out of valid range (1-5): {scores}\")\n",
    "            else:\n",
    "                missing = set(expected_metrics) - set(scores.keys())\n",
    "                self.logger.warning(f\"Missing metrics: {missing}. Found: {scores}\")\n",
    "                \n",
    "                # Attempt fallback parsing\n",
    "                return self._parse_with_fallback(response)\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error parsing response: {e}\")\n",
    "            return self._parse_with_fallback(response)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _parse_with_fallback(self, response: str) -> Optional[Dict[str, int]]:\n",
    "        \"\"\"Enhanced fallback parsing with multiple strategies.\"\"\"\n",
    "        fallback_patterns = [\n",
    "            # Pattern 1: Numbers after metric names\n",
    "            (r'relevance[:\\s]*([1-5])', 'Relevance'),\n",
    "            (r'accuracy[:\\s]*([1-5])', 'Accuracy'),\n",
    "            (r'completeness[:\\s]*([1-5])', 'Completeness'),\n",
    "            (r'fluency[:\\s]*([1-5])', 'Fluency'),\n",
    "            (r'(?:kg|alignment)[:\\s]*([1-5])', 'KG_Alignment'),\n",
    "        ]\n",
    "        \n",
    "        scores = {}\n",
    "        response_lower = response.lower()\n",
    "        \n",
    "        for pattern, metric in fallback_patterns:\n",
    "            matches = re.findall(pattern, response_lower)\n",
    "            if matches:\n",
    "                try:\n",
    "                    score = int(matches[0])\n",
    "                    if 1 <= score <= 5:\n",
    "                        scores[metric] = score\n",
    "                except (ValueError, IndexError):\n",
    "                    continue\n",
    "        \n",
    "        # If we found all 5 metrics, return the scores\n",
    "        expected_metrics = ['Relevance', 'Accuracy', 'Completeness', 'Fluency', 'KG_Alignment']\n",
    "        if len(scores) == 5 and all(metric in scores for metric in expected_metrics):\n",
    "            self.logger.info(f\"Fallback parsing successful: {scores}\")\n",
    "            return scores\n",
    "        \n",
    "        # Last resort: extract any 5 numbers between 1-5\n",
    "        all_numbers = re.findall(r'\\b([1-5])\\b', response)\n",
    "        if len(all_numbers) >= 5:\n",
    "            try:\n",
    "                result = {}\n",
    "                for i, metric in enumerate(expected_metrics):\n",
    "                    result[metric] = int(all_numbers[i])\n",
    "                self.logger.warning(f\"Used last resort parsing: {result}\")\n",
    "                return result\n",
    "            except (ValueError, IndexError):\n",
    "                pass\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def load_qa_dataset(self, file_path: str) -> List[Dict]:\n",
    "        \"\"\"Load KG-only QA dataset with comprehensive error handling.\"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            self.logger.error(f\"Dataset file not found: {file_path}\")\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Handle various dataset structures\n",
    "            queries = []\n",
    "            if isinstance(data, list):\n",
    "                queries = data\n",
    "            elif isinstance(data, dict):\n",
    "                # Try different possible keys\n",
    "                for key in [\"metadata.queries\", \"queries\", \"data\", \"items\"]:\n",
    "                    if \".\" in key:\n",
    "                        # Handle nested keys\n",
    "                        current = data\n",
    "                        for part in key.split(\".\"):\n",
    "                            if isinstance(current, dict) and part in current:\n",
    "                                current = current[part]\n",
    "                            else:\n",
    "                                current = None\n",
    "                                break\n",
    "                        if isinstance(current, list):\n",
    "                            queries = current\n",
    "                            break\n",
    "                    elif key in data and isinstance(data[key], list):\n",
    "                        queries = data[key]\n",
    "                        break\n",
    "            \n",
    "            if not queries:\n",
    "                self.logger.error(f\"No valid queries found in dataset structure: {list(data.keys()) if isinstance(data, dict) else 'list'}\")\n",
    "                return []\n",
    "            \n",
    "            self.logger.info(f\"✓ Loaded {len(queries)} KG-only QA pairs from {file_path}\")\n",
    "            \n",
    "            # Log sample structure for debugging\n",
    "            if queries:\n",
    "                sample = queries[0]\n",
    "                self.logger.info(f\"Sample structure keys: {list(sample.keys())}\")\n",
    "                if \"ground_truth\" in sample:\n",
    "                    self.logger.info(f\"Ground truth keys: {list(sample['ground_truth'].keys())}\")\n",
    "                    \n",
    "                # Validate essential fields\n",
    "                required_fields = ['question', 'answer']\n",
    "                missing_fields = [field for field in required_fields if field not in sample]\n",
    "                if missing_fields:\n",
    "                    self.logger.warning(f\"Missing required fields in sample: {missing_fields}\")\n",
    "            \n",
    "            return queries\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            self.logger.error(f\"Invalid JSON format in dataset file: {e}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load dataset: {e}\")\n",
    "        \n",
    "        return []\n",
    "\n",
    "    def save_checkpoint(self, results: List[Dict], checkpoint_path: str):\n",
    "        \"\"\"Save evaluation results with error handling.\"\"\"\n",
    "        try:\n",
    "            df = pd.DataFrame(results)\n",
    "            df.to_csv(checkpoint_path, index=False)\n",
    "            self.logger.info(f\" Checkpoint saved: {len(results)} results to {checkpoint_path}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to save checkpoint: {e}\")\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_path: str) -> List[Dict]:\n",
    "        \"\"\"Load evaluation results from checkpoint with validation.\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(checkpoint_path):\n",
    "                df = pd.read_csv(checkpoint_path)\n",
    "                \n",
    "                # Validate checkpoint format\n",
    "                required_columns = ['qa_id', 'Relevance', 'Accuracy', 'Completeness', 'Fluency', 'KG_Alignment']\n",
    "                missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "                if missing_columns:\n",
    "                    self.logger.warning(f\"Checkpoint missing required columns: {missing_columns}\")\n",
    "                    return []\n",
    "                \n",
    "                results = df.to_dict('records')\n",
    "                self.logger.info(f\"✓ Loaded checkpoint: {len(results)} results from {checkpoint_path}\")\n",
    "                return results\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load checkpoint: {e}\")\n",
    "            return []\n",
    "\n",
    "    def validate_dataset_item(self, item: Dict, qa_id: str) -> bool:\n",
    "        \"\"\"Validate individual dataset item for required fields.\"\"\"\n",
    "        required_fields = ['question', 'answer']\n",
    "        for field in required_fields:\n",
    "            if field not in item or not str(item[field]).strip():\n",
    "                self.logger.warning(f\"Item {qa_id}: missing or empty field '{field}'\")\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def evaluate_dataset(self, dataset_path: str, output_path: str, \n",
    "                        sample_size: Optional[int] = None, \n",
    "                        delay_seconds: float = 1.0,\n",
    "                        checkpoint_interval: int = 25) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Evaluate KG-only QA dataset with comprehensive monitoring and error handling.\n",
    "        \n",
    "        Args:\n",
    "            dataset_path: Path to the KG-only QA dataset JSON file\n",
    "            output_path: Path to save evaluation results\n",
    "            sample_size: Number of samples to evaluate (None for all)\n",
    "            delay_seconds: Delay between API calls to avoid rate limits\n",
    "            checkpoint_interval: Save checkpoint every N evaluations\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with evaluation results\n",
    "        \"\"\"\n",
    "        self.logger.info(f\" Starting KG-only evaluation...\")\n",
    "        self.logger.info(f\"Dataset: {dataset_path}\")\n",
    "        self.logger.info(f\"Output: {output_path}\")\n",
    "        self.logger.info(f\"Model: {self.model_name}\")\n",
    "        \n",
    "        # Load dataset\n",
    "        qa_items = self.load_qa_dataset(dataset_path)\n",
    "        if not qa_items:\n",
    "            self.logger.error(\" No QA items loaded. Exiting.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Sample subset if requested\n",
    "        if sample_size and sample_size < len(qa_items):\n",
    "            import random\n",
    "            random.seed(42)  # For reproducibility\n",
    "            qa_items = random.sample(qa_items, sample_size)\n",
    "            self.logger.info(f\" Sampling {sample_size} items for evaluation\")\n",
    "        \n",
    "        # Setup checkpoint\n",
    "        checkpoint_path = f\"{output_path}.checkpoint\"\n",
    "        results = self.load_checkpoint(checkpoint_path)\n",
    "        \n",
    "        # Track processed items\n",
    "        processed_ids = {result['qa_id'] for result in results} if results else set()\n",
    "        \n",
    "        # Filter unprocessed items\n",
    "        remaining_items = []\n",
    "        for i, item in enumerate(qa_items):\n",
    "            qa_id = item.get('id', f'item_{i}')\n",
    "            if qa_id not in processed_ids:\n",
    "                if self.validate_dataset_item(item, qa_id):\n",
    "                    remaining_items.append((i, item))\n",
    "        \n",
    "        if processed_ids:\n",
    "            self.logger.info(f\" Resuming from checkpoint: {len(results)} completed, {len(remaining_items)} remaining\")\n",
    "        \n",
    "        if not remaining_items:\n",
    "            self.logger.info(\"✅ All items already processed!\")\n",
    "            if results:\n",
    "                return pd.DataFrame(results)\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        failed_evaluations = 0\n",
    "        skipped_items = 0\n",
    "        \n",
    "        # Progress bar with detailed tracking\n",
    "        total_items = len(qa_items)\n",
    "        completed_items = len(results)\n",
    "        pbar = tqdm(\n",
    "            remaining_items, \n",
    "            desc=\" Evaluating KG-only QA pairs\",\n",
    "            total=total_items,\n",
    "            initial=completed_items,\n",
    "            unit=\"items\",\n",
    "            ncols=100\n",
    "        )\n",
    "        \n",
    "        for item_index, item in pbar:\n",
    "            try:\n",
    "                # Extract data from item\n",
    "                qa_id = item.get('id', f'item_{item_index}')\n",
    "                question = str(item.get('question', '')).strip()\n",
    "                answer = str(item.get('answer', '')).strip()\n",
    "                question_type = item.get('question_type', 'unknown')\n",
    "                generation_method = item.get('generation_method', 'unknown')\n",
    "                \n",
    "                # Extract KG triples (only source of information for ablation study)\n",
    "                kg_triples = self.extract_triples_from_item(item)\n",
    "                \n",
    "                # Skip if no KG triples available\n",
    "                if kg_triples == \"No KG triples available\":\n",
    "                    self.logger.warning(f\" Skipping item {qa_id}: no KG triples found\")\n",
    "                    skipped_items += 1\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                \n",
    "                # Call LLM judge (KG-only evaluation)\n",
    "                evaluation = self.call_llm_judge(question, answer, kg_triples)\n",
    "                \n",
    "                if evaluation:\n",
    "                    # Store results with comprehensive metadata\n",
    "                    result = {\n",
    "                        'qa_id': qa_id,\n",
    "                        'question_type': question_type,\n",
    "                        'generation_method': generation_method,\n",
    "                        'question': question,\n",
    "                        'answer': answer,\n",
    "                        'kg_triples': kg_triples,\n",
    "                        'num_source_triples': len(kg_triples.split('\\n')) if kg_triples != \"No KG triples available\" else 0,\n",
    "                        'text_context_used': item.get('ground_truth', {}).get('text_context_used', False),\n",
    "                        'ablation_study': item.get('ground_truth', {}).get('ablation_study', 'kg_only'),\n",
    "                        'Relevance': evaluation['Relevance'],\n",
    "                        'Accuracy': evaluation['Accuracy'],\n",
    "                        'Completeness': evaluation['Completeness'],\n",
    "                        'Fluency': evaluation['Fluency'],\n",
    "                        'KG_Alignment': evaluation['KG_Alignment'],\n",
    "                        'Overall_Score': sum(evaluation.values()) / len(evaluation),\n",
    "                        'evaluation_timestamp': datetime.now().isoformat()\n",
    "                    }\n",
    "                    results.append(result)\n",
    "                    \n",
    "                    # Update progress bar with rich status\n",
    "                    success_rate = len(results) / (len(results) + failed_evaluations + skipped_items) * 100\n",
    "                    pbar.set_postfix({\n",
    "                        'Success': f\"{success_rate:.1f}%\",\n",
    "                        'Failed': failed_evaluations,\n",
    "                        'Skipped': skipped_items,\n",
    "                        'Last': f\"{result['Overall_Score']:.1f}\"\n",
    "                    })\n",
    "                    \n",
    "                    # Save checkpoint at intervals\n",
    "                    if len(results) % checkpoint_interval == 0:\n",
    "                        self.save_checkpoint(results, checkpoint_path)\n",
    "                    \n",
    "                else:\n",
    "                    failed_evaluations += 1\n",
    "                    self.logger.warning(f\" Failed to evaluate item {qa_id}\")\n",
    "                \n",
    "                # Rate limiting\n",
    "                if delay_seconds > 0:\n",
    "                    time.sleep(delay_seconds)\n",
    "                    \n",
    "            except KeyboardInterrupt:\n",
    "                self.logger.info(\" Evaluation interrupted by user\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                failed_evaluations += 1\n",
    "                self.logger.error(f\" Error processing item {qa_id}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        # Final save and cleanup\n",
    "        if results:\n",
    "            try:\n",
    "                df = pd.DataFrame(results)\n",
    "                df.to_csv(output_path, index=False)\n",
    "                \n",
    "                # Clean up checkpoint file\n",
    "                if os.path.exists(checkpoint_path):\n",
    "                    os.remove(checkpoint_path)\n",
    "                    self.logger.info(f\" Cleaned up checkpoint file\")\n",
    "                \n",
    "                # Print comprehensive summary\n",
    "                self.print_evaluation_summary(df, failed_evaluations, skipped_items)\n",
    "                \n",
    "                return df\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Failed to save final results: {e}\")\n",
    "                return pd.DataFrame()\n",
    "        else:\n",
    "            self.logger.error(\" No successful evaluations completed\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def print_evaluation_summary(self, df: pd.DataFrame, failed_count: int, skipped_count: int):\n",
    "        \"\"\"Print comprehensive and visually appealing evaluation summary.\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\" SECOND ABLATION STUDY: KG-ONLY EVALUATION SUMMARY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Basic statistics\n",
    "        total_attempted = len(df) + failed_count + skipped_count\n",
    "        success_rate = (len(df) / total_attempted * 100) if total_attempted > 0 else 0\n",
    "        \n",
    "        print(f\" Evaluation Statistics:\")\n",
    "        print(f\"   Total Items Processed: {total_attempted}\")\n",
    "        print(f\"   Successfully Evaluated: {len(df)}\")\n",
    "        print(f\"   Failed Evaluations: {failed_count}\")\n",
    "        print(f\"   Skipped Items: {skipped_count}\")\n",
    "        print(f\"   Success Rate: {success_rate:.1f}%\")\n",
    "        print(f\"   Model Used: {self.model_name}\")\n",
    "        print(f\"   Evaluation Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            print(\"\\n No data to analyze!\")\n",
    "            return\n",
    "        \n",
    "        # Metric-by-metric analysis\n",
    "        print(f\"\\n Average Scores by Metric (KG-Only System):\")\n",
    "        metrics = ['Relevance', 'Accuracy', 'Completeness', 'Fluency', 'KG_Alignment']\n",
    "        \n",
    "        for metric in metrics:\n",
    "            if metric in df.columns:\n",
    "                mean_score = df[metric].mean()\n",
    "                std_score = df[metric].std()\n",
    "                min_score = df[metric].min()\n",
    "                max_score = df[metric].max()\n",
    "                median_score = df[metric].median()\n",
    "                \n",
    "                # Add performance indicator\n",
    "                if mean_score >= 4.0:\n",
    "                    indicator = \"🟢\"\n",
    "                elif mean_score >= 3.0:\n",
    "                    indicator = \"🟡\"\n",
    "                else:\n",
    "                    indicator = \"🔴\"\n",
    "                \n",
    "                print(f\"   {indicator} {metric}: {mean_score:.2f} ± {std_score:.2f} \"\n",
    "                      f\"(range: {min_score}-{max_score}, median: {median_score:.1f})\")\n",
    "        \n",
    "        # Overall performance metrics\n",
    "        print(f\"\\n Overall Performance:\")\n",
    "        overall_stats = {\n",
    "            'Mean': df['Overall_Score'].mean(),\n",
    "            'Median': df['Overall_Score'].median(),\n",
    "            'Best': df['Overall_Score'].max(),\n",
    "            'Worst': df['Overall_Score'].min(),\n",
    "            'Std Dev': df['Overall_Score'].std()\n",
    "        }\n",
    "        \n",
    "        for stat_name, value in overall_stats.items():\n",
    "            print(f\"   {stat_name}: {value:.2f}\")\n",
    "        \n",
    "        # Score distribution analysis\n",
    "        print(f\"\\n Score Distribution:\")\n",
    "        score_ranges = [(1.0, 2.0), (2.0, 3.0), (3.0, 4.0), (4.0, 5.0)]\n",
    "        for min_score, max_score in score_ranges:\n",
    "            count = len(df[(df['Overall_Score'] >= min_score) & (df['Overall_Score'] < max_score)])\n",
    "            percentage = (count / len(df)) * 100\n",
    "            bar = \"\" * int(percentage / 5)  # Visual bar\n",
    "            print(f\"   {min_score:.1f}-{max_score:.1f}: {count:3d} ({percentage:5.1f}%) {bar}\")\n",
    "        \n",
    "        # Perfect scores\n",
    "        perfect_scores = len(df[df['Overall_Score'] == 5.0])\n",
    "        print(f\"   Perfect (5.0): {perfect_scores:3d} ({(perfect_scores/len(df))*100:5.1f}%)\")\n",
    "        \n",
    "        # KG-specific analysis\n",
    "        print(f\"\\n KG-Only System Analysis:\")\n",
    "        print(f\"   Average KG Alignment: {df['KG_Alignment'].mean():.2f}\")\n",
    "        print(f\"   KG Alignment Range: {df['KG_Alignment'].min():.1f} - {df['KG_Alignment'].max():.1f}\")\n",
    "        \n",
    "        # Dataset insights\n",
    "        if 'num_source_triples' in df.columns:\n",
    "            print(f\"   Average Source Triples: {df['num_source_triples'].mean():.1f}\")\n",
    "            print(f\"   Triple Count Range: {df['num_source_triples'].min()}-{df['num_source_triples'].max()}\")\n",
    "            \n",
    "            # Correlation analysis\n",
    "            if len(df) > 1:\n",
    "                correlation = df['num_source_triples'].corr(df['Overall_Score'])\n",
    "                print(f\"   Triple Count vs Performance Correlation: {correlation:.3f}\")\n",
    "        \n",
    "        # Performance by question type\n",
    "        if 'question_type' in df.columns and df['question_type'].nunique() > 1:\n",
    "            print(f\"\\n🔍 Performance by Question Type:\")\n",
    "            type_summary = df.groupby('question_type').agg({\n",
    "                'Overall_Score': ['mean', 'std', 'count'],\n",
    "                'KG_Alignment': 'mean'\n",
    "            }).round(3)\n",
    "            \n",
    "            for qtype in type_summary.index:\n",
    "                stats = type_summary.loc[qtype]\n",
    "                mean_score = stats[('Overall_Score', 'mean')]\n",
    "                std_score = stats[('Overall_Score', 'std')]\n",
    "                count = stats[('Overall_Score', 'count')]\n",
    "                kg_align = stats[('KG_Alignment', 'mean')]\n",
    "                print(f\"   {qtype}: {mean_score:.2f} ± {std_score:.2f} (n={count}, KG={kg_align:.2f})\")\n",
    "        \n",
    "        # Quality assessment\n",
    "        print(f\"\\n Quality Assessment:\")\n",
    "        high_quality = len(df[df['Overall_Score'] >= 4.0])\n",
    "        medium_quality = len(df[(df['Overall_Score'] >= 3.0) & (df['Overall_Score'] < 4.0)])\n",
    "        low_quality = len(df[df['Overall_Score'] < 3.0])\n",
    "        \n",
    "        print(f\"   High Quality (≥4.0): {high_quality} ({(high_quality/len(df))*100:.1f}%)\")\n",
    "        print(f\"   Medium Quality (3.0-3.9): {medium_quality} ({(medium_quality/len(df))*100:.1f}%)\")\n",
    "        print(f\"   Low Quality (<3.0): {low_quality} ({(low_quality/len(df))*100:.1f}%)\")\n",
    "        \n",
    "        # KG alignment specific insights\n",
    "        print(f\"\\n KG Alignment Insights:\")\n",
    "        excellent_kg = len(df[df['KG_Alignment'] >= 4])\n",
    "        poor_kg = len(df[df['KG_Alignment'] <= 2])\n",
    "        print(f\"   Excellent KG Alignment (≥4): {excellent_kg} ({(excellent_kg/len(df))*100:.1f}%)\")\n",
    "        print(f\"   Poor KG Alignment (≤2): {poor_kg} ({(poor_kg/len(df))*100:.1f}%)\")\n",
    "        \n",
    "        # Recommendations\n",
    "        print(f\"\\n Key Insights & Recommendations:\")\n",
    "        avg_score = df['Overall_Score'].mean()\n",
    "        if avg_score >= 4.0:\n",
    "            print(f\"    Excellent performance! The KG-only system shows strong capabilities.\")\n",
    "        elif avg_score >= 3.5:\n",
    "            print(f\"    Good performance with room for improvement in specific areas.\")\n",
    "        elif avg_score >= 3.0:\n",
    "            print(f\"    Moderate performance. Consider enhancing KG utilization strategies.\")\n",
    "        else:\n",
    "            print(f\"    Performance below expectations. Review KG extraction and reasoning.\")\n",
    "        \n",
    "        # Specific recommendations based on metrics\n",
    "        if df['KG_Alignment'].mean() < 3.5:\n",
    "            print(f\"    Focus on improving KG triple utilization and alignment.\")\n",
    "        if df['Completeess'].mean() < 3.5:\n",
    "            print(f\"    Enhance answer completeness by leveraging more KG relationships.\")\n",
    "        if df['Accuracy'].mean() < 3.5:\n",
    "            print(f\"    Improve factual accuracy by better grounding in available KG triples.\")\n",
    "        \n",
    "        print(f\"\\n Next Steps:\")\n",
    "        print(f\"   1. Compare results with baseline/original system\")\n",
    "        print(f\"   2. Analyze high-performing vs low-performing examples\")\n",
    "        print(f\"   3. Identify question types that benefit most from KG-only approach\")\n",
    "        print(f\"   4. Investigate correlation between triple count and performance\")\n",
    "        print(f\"   5. Consider hybrid approaches for completeness improvement\")\n",
    "\n",
    "def run_kg_only_evaluation():\n",
    "    \"\"\"\n",
    "    Main function to run KG-only evaluation for the second ablation study.\n",
    "    Production-ready with comprehensive error handling and user guidance.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    API_KEY = \"\" # enter you api\n",
    "    MODEL_NAME = \"google/gemma-2-27b-it\"\n",
    "    DATASET_PATH = \"Ablation_2_kg_only_qa_dataset.json\"\n",
    "    OUTPUT_PATH = \"Gemma_27B_kg_only_evaluation_results.csv\"\n",
    "    \n",
    "    # Display startup banner\n",
    "    print(\"┌\" + \"─\" * 78 + \"┐\")\n",
    "    print(\"│\" + \" \" * 78 + \"│\")\n",
    "    print(\"│   SECOND ABLATION STUDY: KG-ONLY EVALUATION SYSTEM\" + \" \" * 20 + \"│\")\n",
    "    print(\"│\" + \" \" * 78 + \"│\")\n",
    "    print(\"│   Evaluates QA pairs generated using ONLY Knowledge Graph triples\" + \" \" * 8 + \"│\")\n",
    "    print(\"│   Using Google Gemma 2-27B model\" + \" \" * 22 + \"│\")\n",
    "    print(\"│   Evaluation with comprehensive monitoring\" + \" \" * 12 + \"│\")\n",
    "    print(\"│\" + \" \" * 78 + \"│\")\n",
    "    print(\"└\" + \"─\" * 78 + \"┘\")\n",
    "    \n",
    "    # Pre-flight checks\n",
    "    print(f\"\\n Pre-flight Checks:\")\n",
    "    \n",
    "    # Check dataset file\n",
    "    if os.path.exists(DATASET_PATH):\n",
    "        print(f\"    Dataset file found: {DATASET_PATH}\")\n",
    "        try:\n",
    "            with open(DATASET_PATH, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            print(f\"    Dataset file is valid JSON\")\n",
    "        except:\n",
    "            print(f\"   Dataset file has invalid JSON format\")\n",
    "            return\n",
    "    else:\n",
    "        print(f\"    Dataset file not found: {DATASET_PATH}\")\n",
    "        print(f\"    Please ensure the dataset file exists in the current directory\")\n",
    "        return\n",
    "    \n",
    "    # Check API key\n",
    "    if API_KEY and len(API_KEY) > 20:\n",
    "        print(f\"    API key configured\")\n",
    "    else:\n",
    "        print(f\"    API key not properly configured\")\n",
    "        return\n",
    "    \n",
    "    # Check output path\n",
    "    output_dir = os.path.dirname(OUTPUT_PATH) if os.path.dirname(OUTPUT_PATH) else \".\"\n",
    "    if os.access(output_dir, os.W_OK):\n",
    "        print(f\"    Output directory is writable\")\n",
    "    else:\n",
    "        print(f\"    Cannot write to output directory: {output_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n Configuration:\")\n",
    "    print(f\"    Model: {MODEL_NAME}\")\n",
    "    print(f\"    Dataset: {DATASET_PATH}\")\n",
    "    print(f\"    Output: {OUTPUT_PATH}\")\n",
    "    print(f\"    API: Together.ai\")\n",
    "    \n",
    "    # User confirmation\n",
    "    print(f\"\\n\" + \"─\" * 60)\n",
    "    response = input(\" Ready to start evaluation? (y/n): \").strip().lower()\n",
    "    if response != 'y':\n",
    "        print(\" Evaluation cancelled.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    try:\n",
    "        print(f\"\\n Initializing evaluator...\")\n",
    "        evaluator = KGOnlyLLMJudgeEvaluator(api_key=API_KEY, model_name=MODEL_NAME)\n",
    "        print(f\" Evaluator initialized successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\" Failed to initialize evaluator: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Run evaluation\n",
    "    start_time = time.time()\n",
    "    print(f\"\\n Starting evaluation at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    try:\n",
    "        results_df = evaluator.evaluate_dataset(\n",
    "            dataset_path=DATASET_PATH,\n",
    "            output_path=OUTPUT_PATH,\n",
    "            sample_size=None,  # Process all items\n",
    "            delay_seconds=1.0,  # 1 second delay for stability\n",
    "            checkpoint_interval=25  # Save every 25 evaluations\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "        \n",
    "        if not results_df.empty:\n",
    "            print(f\"\\n EVALUATION COMPLETED SUCCESSFULLY!\")\n",
    "            print(f\"  Total time: {duration/60:.1f} minutes\")\n",
    "            print(f\" Items evaluated: {len(results_df)}\")\n",
    "            print(f\" Results saved to: {OUTPUT_PATH}\")\n",
    "            print(f\" Average score: {results_df['Overall_Score'].mean():.2f}/5.0\")\n",
    "            \n",
    "            # Performance stats\n",
    "            items_per_minute = len(results_df) / (duration / 60) if duration > 0 else 0\n",
    "            print(f\"⚡ Processing rate: {items_per_minute:.1f} items/minute\")\n",
    "            \n",
    "            # Quick quality check\n",
    "            high_quality = len(results_df[results_df['Overall_Score'] >= 4.0])\n",
    "            quality_rate = (high_quality / len(results_df)) * 100\n",
    "            print(f\" High quality responses (≥4.0): {quality_rate:.1f}%\")\n",
    "            \n",
    "            print(f\"\\n✨ Evaluation complete! Check the detailed summary above for insights.\")\n",
    "            \n",
    "        else:\n",
    "            print(f\" Evaluation failed - no results generated.\")\n",
    "            print(f\" Check the logs for detailed error information.\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"\\n  Evaluation interrupted by user\")\n",
    "        print(f\" Partial results may be saved in checkpoint file\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n Evaluation failed with error: {e}\")\n",
    "        print(f\" Check the logs for detailed error information\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Entry point with additional safety checks.\"\"\"\n",
    "    try:\n",
    "        # Ensure required packages are available\n",
    "        required_packages = ['pandas', 'tqdm', 'together']\n",
    "        missing_packages = []\n",
    "        \n",
    "        for package in required_packages:\n",
    "            try:\n",
    "                __import__(package)\n",
    "            except ImportError:\n",
    "                missing_packages.append(package)\n",
    "        \n",
    "        if missing_packages:\n",
    "            print(f\" Installing missing packages: {', '.join(missing_packages)}\")\n",
    "            for package in missing_packages:\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "            print(f\" All packages installed successfully!\")\n",
    "        \n",
    "        # Run the evaluation\n",
    "        run_kg_only_evaluation()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Fatal error: {e}\")\n",
    "        print(f\" Please check your Python environment and try again\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6ee916-a227-4a5e-9983-31823ac1abf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
