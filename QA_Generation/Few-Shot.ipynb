{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93904010-1ae3-4942-bdc9-edf8aedae54d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Few-Shot QA Generator with Enhanced Quality Filtering\n",
    "\n",
    "Features:\n",
    "- Multiple manually constructed exemplars per question type (3 high-quality examples)\n",
    "- Enhanced prompt engineering with diverse exemplar patterns\n",
    "- Same essential filtering as one-shot: length requirements + duplicate detection\n",
    "- Improved generation guidance through pattern diversity\n",
    "- Maintains all statistical tracking and quality controls\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import time\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from collections import OrderedDict\n",
    "\n",
    "# OpenAI import fix with proper error handling\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    OPENAI_V1 = True\n",
    "    logger_msg = \"Using OpenAI v1.x API\"\n",
    "except ImportError:\n",
    "    try:\n",
    "        import openai\n",
    "        OPENAI_V1 = False\n",
    "        logger_msg = \"Using OpenAI legacy API\"\n",
    "    except ImportError:\n",
    "        print(\"ERROR: OpenAI library not installed. Run: pip install openai\")\n",
    "        sys.exit(1)\n",
    "\n",
    "# Configure environment and logging\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('few_shot_qa_generation.log'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Device configuration with proper torch import\n",
    "try:\n",
    "    import torch\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \n",
    "                         \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "except ImportError:\n",
    "    print(\"ERROR: PyTorch not installed. Run: pip install torch\")\n",
    "    sys.exit(1)\n",
    "\n",
    "logger.info(logger_msg)\n",
    "\n",
    "\n",
    "class QuestionType(Enum):\n",
    "    \"\"\"Enumeration of supported question types for few-shot generation.\"\"\"\n",
    "    FACTUAL = \"factual\"\n",
    "    RELATIONSHIP = \"relationship\" \n",
    "    COMPARATIVE = \"comparative\"\n",
    "    INFERENTIAL = \"inferential\"\n",
    "\n",
    "\n",
    "class FilteringDecision(Enum):\n",
    "    \"\"\"Basic filtering decision outcomes.\"\"\"\n",
    "    ACCEPTED = \"accepted\"\n",
    "    REJECTED_LENGTH = \"rejected_length\"\n",
    "    REJECTED_DUPLICATE = \"rejected_duplicate\"\n",
    "    REJECTED_PARSING = \"rejected_parsing\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class QualityThresholds:\n",
    "    \"\"\"Basic quality thresholds for essential filtering only.\"\"\"\n",
    "    # Length requirements - ensures meaningful content\n",
    "    min_question_words: int = 8      # Ensures meaningful questions\n",
    "    min_answer_words: int = 20       # Ensures informative answers\n",
    "    \n",
    "    # Duplicate detection thresholds - more lenient for variety\n",
    "    duplicate_similarity_threshold: float = 0.85  \n",
    "    batch_similarity_threshold: float = 0.85     \n",
    "    \n",
    "    # Semantic similarity weights for relevance scoring (logging only)\n",
    "    context_weight: float = 0.6\n",
    "    triple_weight: float = 0.4\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FilteringResult:\n",
    "    \"\"\"Data class representing a basic filtering result with reasoning.\"\"\"\n",
    "    accepted: bool\n",
    "    decision: FilteringDecision\n",
    "    relevance_score: float  # For logging/analysis only\n",
    "    context_similarity: float\n",
    "    triple_similarity: float\n",
    "    reasoning: str\n",
    "    metadata: Dict[str, float]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class QAPair:\n",
    "    \"\"\"Data class representing a question-answer pair with complete metadata.\"\"\"\n",
    "    id: str\n",
    "    question: str\n",
    "    answer: str\n",
    "    question_type: QuestionType\n",
    "    qa_metadata: Dict[str, List[str]]\n",
    "    filtering_result: FilteringResult\n",
    "    generation_method: str = \"few_shot\"\n",
    "    # Add ground truth information\n",
    "    source_context: str = \"\"\n",
    "    source_triples: List[Tuple] = None\n",
    "    chunk_id: str = \"\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.source_triples is None:\n",
    "            self.source_triples = []\n",
    "\n",
    "\n",
    "# FEW-SHOT EXEMPLARS - Multiple high-quality examples for pattern learning\n",
    "FEW_SHOT_EXEMPLARS = [\n",
    "    {\n",
    "        \"context\": \"In determining student eligibility, institutions must also satisfy themselves that there is a reasonable likelihood that the student will be able to complete their study programme before seeking funding for the student. This should include the practicality of providing a place for a student who may be unable to complete their programme if they are likely to leave the country permanently during their study programme. For the purposes of this paragraph, institutions must assume that all European Economic Area students resident in the UK before 1 January 2022 have the legal right to remain in the UK for the duration of their study programme. Once a student is enrolled, the institution is expected to take all reasonable steps to ensure that the student can complete their programme.\",\n",
    "        \"knowledge_graph\": [\n",
    "            (\"institution\", \"has_legal_duty\", \"verify_student_eligibility\"),\n",
    "            (\"institution\", \"is_expected_to_ensure\", \"student_program_completion\"),\n",
    "            (\"student\", \"enrolled_in\", \"study_programme\"),\n",
    "            (\"institution\", \"must_evaluate\", \"student_completion_likelihood\"),\n",
    "            (\"student\", \"potential_withdrawal_reason\", \"likely_permanent_departure\"),\n",
    "            (\"eea_student\", \"has_legal_right\", \"remain_in_uk_during_study\"),\n",
    "            (\"eea_student\", \"has_status\", \"resident_in_uk_before_20220101\"),\n",
    "            (\"institution\", \"provides_assistance\", \"student_completion_support\")\n",
    "        ],\n",
    "        \"exemplar_questions\": {\n",
    "            QuestionType.FACTUAL: {\n",
    "                \"question\": \"What must institutions assume about EEA students who were resident in the UK before 1 January 2022?\"\n",
    "            },\n",
    "            QuestionType.RELATIONSHIP: {\n",
    "                \"question\": \"How is a student's likelihood of permanent departure related to their eligibility for funding?\"\n",
    "            },\n",
    "            QuestionType.COMPARATIVE: {\n",
    "                \"question\": \"How does the institution's responsibility differ before and after a student is enrolled in a study programme?\"\n",
    "            },\n",
    "            QuestionType.INFERENTIAL: {\n",
    "                \"question\": \"Why might institutions be discouraged from enrolling students who are likely to leave the UK permanently during their study programme?\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"context\": \"Students who are attending programmes of more than one term's duration, and are eligible for funding at the start of their programme, will usually be eligible for funding for the whole duration of their study programme as well as subsequent funded study programmes studied immediately end-on to their initial funded programme. This includes students studying consecutive study programmes with no break in studies other than normal holiday periods. Similarly, students who are not eligible for funding at the start of their study programme are very unlikely to become eligible for funding during the period of their study programme.\",\n",
    "        \"knowledge_graph\": [\n",
    "            (\"student\", \"has_funding_start_status\", \"eligible_at_programme_start\"),\n",
    "            (\"student\", \"enrolled_in\", \"study_programme\"),\n",
    "            (\"study_programme\", \"has_duration_value\", \"more_than_one_term\"),\n",
    "            (\"study_programme\", \"has_funding\", \"funded_for_duration\"),\n",
    "            (\"funded_for_duration\", \"includes\", \"subsequent_funded_programmes\"),\n",
    "            (\"subsequent_funded_programmes\", \"has_temporal_value\", \"immediately_end_on\"),\n",
    "            (\"student\", \"participates_in\", \"consecutive_study_programmes\"),\n",
    "            (\"consecutive_study_programmes\", \"has_time_period\", \"no_break_other_than_holidays\"),\n",
    "            (\"student\", \"has_funding_start_status\", \"not_eligible_at_programme_start\"),\n",
    "            (\"student\", \"related_to_funding_status\", \"unlikely_to_become_eligible_during_programme\")\n",
    "        ],\n",
    "        \"exemplar_questions\": {\n",
    "            QuestionType.FACTUAL: {\n",
    "                \"question\": \"Which students are usually eligible for funding throughout the duration of their study programme?\"\n",
    "            },\n",
    "            QuestionType.RELATIONSHIP: {\n",
    "                \"question\": \"How does the start-of-programme funding status relate to a student's funding eligibility during their studies?\"\n",
    "            },\n",
    "            QuestionType.COMPARATIVE: {\n",
    "                \"question\": \"What is the difference in funding eligibility between students with and without a break between consecutive study programmes?\"\n",
    "            },\n",
    "            QuestionType.INFERENTIAL: {\n",
    "                \"question\": \"What does the policy imply about ESFA's approach to students who begin their programme ineligible for funding?\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"context\": \"For the Prince's Trust Team Programme, the institution overhead rate (management fee) should be no more than a maximum of 15 per cent of the total ESFA funding. Any figure above 15 per cent will require prior approval from ESFA in collaboration with the Prince's Trust. For the purpose of the condition of funding, ESFA recognise that the Team Programme will support young people to progress towards General Certificate of Secondary Education standard and has been approved as a stepping stone towards a General Certificate of Secondary Education in these subjects.\",\n",
    "        \"knowledge_graph\": [\n",
    "            (\"esfa\", \"provides_funding\", \"princes_trust_team_programme\"),\n",
    "            (\"princes_trust_team_programme\", \"has_funding_condition\", \"maximum_management_fee_15_percent\"),\n",
    "            (\"maximum_management_fee_15_percent\", \"requires\", \"prior_approval_from_esfa_above_15_percent\"),\n",
    "            (\"prior_approval_from_esfa_above_15_percent\", \"involves\", \"collaboration_with_princes_trust\"),\n",
    "            (\"esfa\", \"recognizes\", \"princes_trust_team_programme_as_stepping_stone\"),\n",
    "            (\"princes_trust_team_programme\", \"supports_learning\", \"general_certificate_of_secondary_education_standard\"),\n",
    "            (\"princes_trust_team_programme\", \"has_progression\", \"general_certificate_of_secondary_education\")\n",
    "        ],\n",
    "        \"exemplar_questions\": {\n",
    "            QuestionType.FACTUAL: {\n",
    "                \"question\": \"What is the maximum management fee allowed for the Prince's Trust Team Programme without requiring ESFA approval?\"\n",
    "            },\n",
    "            QuestionType.RELATIONSHIP: {\n",
    "                \"question\": \"How is the Prince's Trust involved in the approval process when the management fee exceeds 15%?\"\n",
    "            },\n",
    "            QuestionType.COMPARATIVE: {\n",
    "                \"question\": \"How does the recognition of the Team Programme differ from a full General Certificate of Secondary Education?\"\n",
    "            },\n",
    "            QuestionType.INFERENTIAL: {\n",
    "                \"question\": \"Why might ESFA recognize the Team Programme as a stepping stone towards GCSE standard?\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Enhanced few-shot question generation templates\n",
    "FEW_SHOT_QUESTION_TEMPLATES = {\n",
    "    QuestionType.FACTUAL: {\n",
    "        \"task_description\": (\n",
    "            \"Factual questions seek specific, concrete information that can be directly extracted from \"\n",
    "            \"the text or inferred from the knowledge graph relationships. They typically start with \"\n",
    "            \"'What', 'When', 'Where', 'Who', or 'How much/many' and ask for precise details.\"\n",
    "        ),\n",
    "        \"generation_guidance\": (\n",
    "            \"Generate factual questions that require **specific** information from the context. \"\n",
    "            \"These questions should ask for concrete details, numbers, dates, names, or specific requirements \"\n",
    "            \"mentioned in the text. Focus on extracting precise information that can be directly answered \"\n",
    "            \"from the provided context and knowledge graph triples.\"\n",
    "        )\n",
    "    },\n",
    "    \n",
    "    QuestionType.RELATIONSHIP: {\n",
    "        \"task_description\": (\n",
    "            \"Relationship questions explore connections between entities. They examine how entities \"\n",
    "            \"interact, depend on each other, or influence one another through the relationships \"\n",
    "            \"defined in the knowledge graph.\"\n",
    "        ),\n",
    "        \"generation_guidance\": (\n",
    "            \"Generate questions about specific relationships or interactions between entities in the context. \"\n",
    "            \"Focus on how different entities, organizations, processes, or concepts connect, influence, or \"\n",
    "            \"interact with each other. Each question must reference at least two entities and explore \"\n",
    "            \"their connection through the knowledge graph relationships.\"\n",
    "        )\n",
    "    },\n",
    "    \n",
    "    QuestionType.COMPARATIVE: {\n",
    "        \"task_description\": (\n",
    "            \"Comparative questions examine differences and similarities between entities or concepts. \"\n",
    "            \"They help understand distinctions in requirements, processes, amounts, or characteristics \"\n",
    "            \"across different categories or instances.\"\n",
    "        ),\n",
    "        \"generation_guidance\": (\n",
    "            \"Generate questions comparing different aspects, entities, or concepts from the context. \"\n",
    "            \"These questions should highlight differences, similarities, or contrasts between multiple \"\n",
    "            \"items such as funding types, requirements, processes, or organizational structures. \"\n",
    "            \"Use the knowledge graph to identify comparable entities.\"\n",
    "        )\n",
    "    },\n",
    "    \n",
    "    QuestionType.INFERENTIAL: {\n",
    "        \"task_description\": (\n",
    "            \"Inferential questions require reasoning and synthesis of multiple pieces of information. \"\n",
    "            \"They ask for conclusions, implications, or predictions that must be derived by combining \"\n",
    "            \"various facts and relationships from the context and knowledge graph.\"\n",
    "        ),\n",
    "        \"generation_guidance\": (\n",
    "            \"Generate questions that require analysis, reasoning, or inference based on the context \"\n",
    "            \"and knowledge graph. These questions should combine multiple pieces of information to \"\n",
    "            \"draw conclusions, identify implications, or predict outcomes. They require synthesizing \"\n",
    "            \"information from multiple knowledge graph triples.\"\n",
    "        )\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "class FewShotQAGenerator:\n",
    "    \"\"\"\n",
    "    Few-Shot QA Generator with Enhanced Quality Filtering.\n",
    "    \n",
    "    Features:\n",
    "    - Uses multiple high-quality exemplars per question type to guide generation\n",
    "    - Pattern diversity through varied exemplar structures\n",
    "    - Length requirements for meaningful content\n",
    "    - Duplicate detection to prevent redundancy \n",
    "    - Basic relevance scoring for analysis (not filtering)\n",
    "    - Simplified decision logic with clear justification\n",
    "    - Centralized statistics tracking \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 openai_api_key: str,\n",
    "                 reset_duplicates_per_chunk: bool = True,\n",
    "                 embedding_model: str = 'BAAI/bge-large-en-v1.5',\n",
    "                 max_embedding_cache: int = 1000):\n",
    "        \"\"\"\n",
    "        Initialize the Few-Shot QA Generator.\n",
    "        \n",
    "        Args:\n",
    "            openai_api_key: OpenAI API key for GPT-4 access\n",
    "            reset_duplicates_per_chunk: Whether to reset duplicate tracking per chunk\n",
    "            max_embedding_cache: Maximum number of embeddings to cache (memory management)\n",
    "        \"\"\"\n",
    "        if not openai_api_key or openai_api_key == \"your-openai-api-key-here\":\n",
    "            raise ValueError(\"Please provide a valid OpenAI API key\")\n",
    "            \n",
    "        self.openai_api_key = openai_api_key\n",
    "        self.max_embedding_cache = max_embedding_cache\n",
    "        \n",
    "        # Initialize OpenAI client based on version\n",
    "        if OPENAI_V1:\n",
    "            self.openai_client = OpenAI(api_key=openai_api_key)\n",
    "        else:\n",
    "            openai.api_key = openai_api_key\n",
    "            self.openai_client = None\n",
    "        \n",
    "        # Initialize quality thresholds\n",
    "        self.thresholds = QualityThresholds()\n",
    "        self.reset_duplicates_per_chunk = reset_duplicates_per_chunk\n",
    "        \n",
    "        # Initialize embedding model\n",
    "        logger.info(f\"Loading embedding model: {embedding_model}\")\n",
    "        try:\n",
    "            self.embedding_model = SentenceTransformer(embedding_model, device=device)\n",
    "            logger.info(f\"Successfully loaded {embedding_model}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading {embedding_model}: {e}\")\n",
    "            raise RuntimeError(f\"Failed to load the specified embedding model '{embedding_model}'. Please check your internet connection and ensure the model name is correct.\")\n",
    "        \n",
    "        # Use OrderedDict for memory management and better duplicate tracking\n",
    "        self.all_questions: set = set()\n",
    "        self.question_embeddings: OrderedDict[str, torch.Tensor] = OrderedDict()\n",
    "        self.batch_questions: set = set()\n",
    "        self.batch_embeddings: Dict[str, torch.Tensor] = {}\n",
    "        \n",
    "        # Enhanced statistics tracking with detailed breakdown\n",
    "        self.generation_stats = {\n",
    "            \"total_api_calls\": 0,\n",
    "            \"total_qa_attempts\": 0,\n",
    "            \"successful_generations\": 0,\n",
    "            \"filtering_decisions\": {\n",
    "                \"accepted\": 0,\n",
    "                \"rejected_length\": 0,\n",
    "                \"rejected_duplicate\": 0,\n",
    "                \"rejected_parsing\": 0\n",
    "            },\n",
    "            \"all_attempts\": []\n",
    "        }\n",
    "        \n",
    "        logger.info(\"Few-shot QA Generator initialized successfully\")\n",
    "        logger.info(\"Using multi-exemplar guidance with essential filters: length requirements + duplicate detection\")\n",
    "\n",
    "    def _filtering_result_to_dict(self, result: FilteringResult) -> Dict:\n",
    "        \"\"\"Convert FilteringResult to dictionary for JSON serialization with enum handling.\"\"\"\n",
    "        return {\n",
    "            \"accepted\": result.accepted,\n",
    "            \"decision\": result.decision.value if hasattr(result.decision, 'value') else str(result.decision),\n",
    "            \"relevance_score\": result.relevance_score,\n",
    "            \"context_similarity\": result.context_similarity,\n",
    "            \"triple_similarity\": result.triple_similarity,\n",
    "            \"reasoning\": result.reasoning,\n",
    "            \"metadata\": result.metadata\n",
    "        }\n",
    "\n",
    "    def _manage_embedding_cache(self):\n",
    "        \"\"\"Manage embedding cache size to prevent memory issues.\"\"\"\n",
    "        if len(self.question_embeddings) > self.max_embedding_cache:\n",
    "            # Remove oldest embeddings (FIFO)\n",
    "            removed_count = len(self.question_embeddings) - self.max_embedding_cache\n",
    "            for _ in range(removed_count):\n",
    "                oldest_key = next(iter(self.question_embeddings))\n",
    "                del self.question_embeddings[oldest_key]\n",
    "                self.all_questions.discard(oldest_key)\n",
    "            logger.debug(f\"Removed {removed_count} old embeddings from cache\")\n",
    "\n",
    "    def calculate_relevance_score(self, question: str, answer: str, context: str, triples: List[Tuple]) -> Tuple[float, float, float]:\n",
    "        \"\"\"\n",
    "        Calculate semantic relevance score for logging/analysis purposes only.\n",
    "        This is NOT used for filtering decisions.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            qa_combined = f\"{question} {answer}\"\n",
    "            qa_embedding = self.embedding_model.encode(qa_combined, convert_to_tensor=True)\n",
    "            \n",
    "            # Context similarity\n",
    "            context_embedding = self.embedding_model.encode(context, convert_to_tensor=True)\n",
    "            context_similarity = util.cos_sim(context_embedding, qa_embedding).item()\n",
    "            \n",
    "            # Triple similarities\n",
    "            triple_similarities = []\n",
    "            for triple in triples:\n",
    "                try:\n",
    "                    if len(triple) >= 3:\n",
    "                        n1, edge, n2 = str(triple[0]), str(triple[1]), str(triple[2])\n",
    "                        triple_text = f\"{n1} {edge} {n2}\"\n",
    "                        triple_embedding = self.embedding_model.encode(triple_text, convert_to_tensor=True)\n",
    "                        triple_similarity = util.cos_sim(triple_embedding, qa_embedding).item()\n",
    "                        triple_similarities.append(triple_similarity)\n",
    "                except (IndexError, TypeError) as e:\n",
    "                    logger.debug(f\"Invalid triple format: {triple}, error: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            max_triple_similarity = max(triple_similarities) if triple_similarities else 0.0\n",
    "            \n",
    "            # Weighted overall relevance (for logging only)\n",
    "            overall_relevance = (context_similarity * self.thresholds.context_weight + \n",
    "                               max_triple_similarity * self.thresholds.triple_weight)\n",
    "            \n",
    "            return overall_relevance, context_similarity, max_triple_similarity\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error calculating relevance score: {e}\")\n",
    "            return 0.0, 0.0, 0.0\n",
    "\n",
    "    def passes_length_filter(self, question: str, answer: str) -> bool:\n",
    "        \"\"\"Check if QA pair meets minimum length requirements.\"\"\"\n",
    "        try:\n",
    "            q_words = len(question.split())\n",
    "            a_words = len(answer.split())\n",
    "            \n",
    "            if q_words < self.thresholds.min_question_words or a_words < self.thresholds.min_answer_words:\n",
    "                logger.debug(f\"Length filter failed: Q={q_words} words (min {self.thresholds.min_question_words}), \"\n",
    "                            f\"A={a_words} words (min {self.thresholds.min_answer_words})\")\n",
    "                return False\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error in length filter: {e}\")\n",
    "            return False\n",
    "\n",
    "    def is_semantic_duplicate(self, new_question: str) -> Tuple[bool, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Check if question is a semantic duplicate and return embedding.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (is_duplicate, question_embedding)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            new_embedding = self.embedding_model.encode(new_question, convert_to_tensor=True)\n",
    "            \n",
    "            # Check against historical questions\n",
    "            for existing_question, existing_embedding in self.question_embeddings.items():\n",
    "                similarity = util.cos_sim(new_embedding, existing_embedding).item()\n",
    "                if similarity > self.thresholds.duplicate_similarity_threshold:\n",
    "                    logger.debug(f\"Historical duplicate detected: '{new_question}' ~ '{existing_question}' \"\n",
    "                               f\"(similarity={similarity:.3f})\")\n",
    "                    return True, new_embedding\n",
    "            \n",
    "            # Check against current batch using stored embeddings\n",
    "            for batch_question, batch_embedding in self.batch_embeddings.items():\n",
    "                similarity = util.cos_sim(new_embedding, batch_embedding).item()\n",
    "                if similarity > self.thresholds.batch_similarity_threshold:\n",
    "                    logger.debug(f\"Batch duplicate detected: '{new_question}' ~ '{batch_question}' \"\n",
    "                               f\"(similarity={similarity:.3f})\")\n",
    "                    return True, new_embedding\n",
    "            \n",
    "            return False, new_embedding\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error in duplicate detection: {e}\")\n",
    "            # Return False with dummy embedding on error\n",
    "            dummy_embedding = torch.zeros(384)  # Default embedding size\n",
    "            return False, dummy_embedding\n",
    "\n",
    "    def filter_qa_pair(self, \n",
    "                      question: str, \n",
    "                      answer: str, \n",
    "                      context: str, \n",
    "                      triples: List[Tuple],\n",
    "                      qa_metadata: Dict) -> Tuple[bool, FilteringResult]:\n",
    "        \"\"\"\n",
    "        Apply basic filtering pipeline to QA pair with centralized statistics tracking.\n",
    "        \n",
    "        Args:\n",
    "            question: Question text\n",
    "            answer: Answer text\n",
    "            context: Original text chunk\n",
    "            triples: Knowledge graph triples\n",
    "            qa_metadata: QA metadata including entities and relationships\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (acceptance_decision, filtering_result_details)\n",
    "        \"\"\"\n",
    "        # Centralized statistics tracking - increment total attempts here\n",
    "        self.generation_stats[\"total_qa_attempts\"] += 1\n",
    "        \n",
    "        # Calculate relevance score first (for all attempts)\n",
    "        relevance_score, context_sim, triple_sim = self.calculate_relevance_score(\n",
    "            question, answer, context, triples\n",
    "        )\n",
    "        \n",
    "        # Create base metadata\n",
    "        base_metadata = {\n",
    "            \"answer_length\": len(answer.split()),\n",
    "            \"question_length\": len(question.split()),\n",
    "            \"mentioned_entities\": len(qa_metadata.get(\"mentioned_entities\", [])),\n",
    "            \"mentioned_relationships\": len(qa_metadata.get(\"mentioned_relationships\", []))\n",
    "        }\n",
    "        \n",
    "        # Filter 1: Length requirements\n",
    "        if not self.passes_length_filter(question, answer):\n",
    "            result = FilteringResult(\n",
    "                accepted=False,\n",
    "                decision=FilteringDecision.REJECTED_LENGTH,\n",
    "                relevance_score=relevance_score,\n",
    "                context_similarity=context_sim,\n",
    "                triple_similarity=triple_sim,\n",
    "                reasoning=f\"Failed length requirements: Q={len(question.split())} words, A={len(answer.split())} words\",\n",
    "                metadata=base_metadata\n",
    "            )\n",
    "            \n",
    "            # Update statistics\n",
    "            self.generation_stats[\"filtering_decisions\"][\"rejected_length\"] += 1\n",
    "            \n",
    "            # Store attempt for analysis\n",
    "            self.generation_stats[\"all_attempts\"].append({\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"filtering_result\": self._filtering_result_to_dict(result),\n",
    "                \"relevance_score\": relevance_score\n",
    "            })\n",
    "            \n",
    "            return False, result\n",
    "        \n",
    "        # Filter 2: Duplicate detection \n",
    "        is_duplicate, question_embedding = self.is_semantic_duplicate(question)\n",
    "        if is_duplicate:\n",
    "            result = FilteringResult(\n",
    "                accepted=False,\n",
    "                decision=FilteringDecision.REJECTED_DUPLICATE,\n",
    "                relevance_score=relevance_score,\n",
    "                context_similarity=context_sim,\n",
    "                triple_similarity=triple_sim,\n",
    "                reasoning=\"Semantic duplicate detected\",\n",
    "                metadata=base_metadata\n",
    "            )\n",
    "            \n",
    "            # Update statistics\n",
    "            self.generation_stats[\"filtering_decisions\"][\"rejected_duplicate\"] += 1\n",
    "            \n",
    "            # Store attempt for analysis\n",
    "            self.generation_stats[\"all_attempts\"].append({\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"filtering_result\": self._filtering_result_to_dict(result),\n",
    "                \"relevance_score\": relevance_score\n",
    "            })\n",
    "            \n",
    "            return False, result\n",
    "        \n",
    "        # Accept QA pair - passed all filters\n",
    "        result = FilteringResult(\n",
    "            accepted=True,\n",
    "            decision=FilteringDecision.ACCEPTED,\n",
    "            relevance_score=relevance_score,\n",
    "            context_similarity=context_sim,\n",
    "            triple_similarity=triple_sim,\n",
    "            reasoning=f\"Passed basic quality filters (relevance: {relevance_score:.3f})\",\n",
    "            metadata=base_metadata\n",
    "        )\n",
    "        \n",
    "        # Register accepted question with proper embedding storage\n",
    "        self.all_questions.add(question)\n",
    "        self.question_embeddings[question] = question_embedding\n",
    "        self.batch_questions.add(question)\n",
    "        self.batch_embeddings[question] = question_embedding\n",
    "        \n",
    "        # Manage cache size\n",
    "        self._manage_embedding_cache()\n",
    "        \n",
    "        # Update statistics\n",
    "        self.generation_stats[\"successful_generations\"] += 1\n",
    "        self.generation_stats[\"filtering_decisions\"][\"accepted\"] += 1\n",
    "        \n",
    "        # Store attempt for analysis\n",
    "        self.generation_stats[\"all_attempts\"].append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"filtering_result\": self._filtering_result_to_dict(result),\n",
    "            \"relevance_score\": relevance_score\n",
    "        })\n",
    "        \n",
    "        return True, result\n",
    "\n",
    "    def load_and_merge_data(self, chunks_file: str, triples_file: str) -> Dict:\n",
    "        \"\"\"Load and merge chunks and triples data.\"\"\"\n",
    "        try:\n",
    "            # Load chunks data\n",
    "            chunks_df = pd.read_csv(chunks_file)\n",
    "            logger.info(f\"Loaded {len(chunks_df)} chunks from {chunks_file}\")\n",
    "            logger.info(f\"Chunks columns: {list(chunks_df.columns)}\")\n",
    "            \n",
    "            # Load triples data\n",
    "            triples_df = pd.read_csv(triples_file)\n",
    "            logger.info(f\"Loaded {len(triples_df)} triples from {triples_file}\")\n",
    "            logger.info(f\"Triples columns: {list(triples_df.columns)}\")\n",
    "            \n",
    "            # Check for required columns\n",
    "            if 'chunk_id' not in chunks_df.columns:\n",
    "                raise ValueError(\"chunks_file must contain 'chunk_id' column\")\n",
    "            if 'text' not in chunks_df.columns:\n",
    "                raise ValueError(\"chunks_file must contain 'text' column\")\n",
    "            if 'chunk_id' not in triples_df.columns:\n",
    "                raise ValueError(\"triples_file must contain 'chunk_id' column\")\n",
    "            \n",
    "            # Try to identify triple columns\n",
    "            triple_columns = None\n",
    "            possible_combinations = [\n",
    "                ('subject', 'predicate', 'object'),\n",
    "                ('node1', 'edge', 'node2'),\n",
    "                ('head', 'relation', 'tail'),\n",
    "                ('entity1', 'relationship', 'entity2')\n",
    "            ]\n",
    "            \n",
    "            for combo in possible_combinations:\n",
    "                if all(col in triples_df.columns for col in combo):\n",
    "                    triple_columns = combo\n",
    "                    break\n",
    "            \n",
    "            if triple_columns is None:\n",
    "                available_cols = list(triples_df.columns)\n",
    "                raise ValueError(f\"Could not identify triple columns. Available columns: {available_cols}. \"\n",
    "                               f\"Expected one of: {possible_combinations}\")\n",
    "            \n",
    "            logger.info(f\"Using triple columns: {triple_columns}\")\n",
    "            \n",
    "            # Merge on chunk_id\n",
    "            merged_df = pd.merge(chunks_df, triples_df, on='chunk_id', how='inner')\n",
    "            logger.info(f\"Merged data contains {len(merged_df)} records\")\n",
    "            \n",
    "            if len(merged_df) == 0:\n",
    "                raise ValueError(\"No matching chunk_ids found between chunks and triples files\")\n",
    "            \n",
    "            # Group by chunk_id\n",
    "            grouped_data = {}\n",
    "            for chunk_id, group in merged_df.groupby('chunk_id'):\n",
    "                grouped_data[chunk_id] = {\n",
    "                    'text': group['text'].iloc[0],\n",
    "                    'triples': [(row[triple_columns[0]], row[triple_columns[1]], row[triple_columns[2]]) \n",
    "                              for _, row in group.iterrows()]\n",
    "                }\n",
    "            \n",
    "            logger.info(f\"Grouped data contains {len(grouped_data)} unique chunks\")\n",
    "            return grouped_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading and merging data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_few_shot_prompt(self, context: str, triples: List[Tuple], question_type: QuestionType, num_questions: int) -> str:\n",
    "        \"\"\"Generate few-shot prompt with multiple exemplars for enhanced pattern learning.\"\"\"\n",
    "        template = FEW_SHOT_QUESTION_TEMPLATES.get(question_type, {})\n",
    "        task_description = template.get(\"task_description\", \"\")\n",
    "        generation_guidance = template.get(\"generation_guidance\", \"\")\n",
    "        \n",
    "        # Format current context triples\n",
    "        formatted_triples = []\n",
    "        for triple in triples[:20]:  # Limit to prevent prompt overflow\n",
    "            try:\n",
    "                if len(triple) >= 3:\n",
    "                    n1, e, n2 = str(triple[0]), str(triple[1]), str(triple[2])\n",
    "                    formatted_triples.append(f\"- {n1} → '{e}' → {n2}\")\n",
    "            except (IndexError, TypeError):\n",
    "                continue\n",
    "        \n",
    "        triple_text = \"\\n\".join(formatted_triples) if formatted_triples else \"No valid triples available\"\n",
    "\n",
    "        # Build multiple exemplar demonstrations\n",
    "        exemplar_demonstrations = []\n",
    "        for i, exemplar in enumerate(FEW_SHOT_EXEMPLARS, 1):\n",
    "            # Format exemplar triples\n",
    "            exemplar_formatted_triples = []\n",
    "            for triple in exemplar[\"knowledge_graph\"]:\n",
    "                n1, e, n2 = str(triple[0]), str(triple[1]), str(triple[2])\n",
    "                exemplar_formatted_triples.append(f\"- {n1} → '{e}' → {n2}\")\n",
    "            exemplar_triple_text = \"\\n\".join(exemplar_formatted_triples)\n",
    "            \n",
    "            # Get the exemplar question for this type\n",
    "            exemplar_question = exemplar[\"exemplar_questions\"][question_type][\"question\"]\n",
    "            \n",
    "            demonstration = f\"\"\"\n",
    "EXAMPLE {i} CONTEXT:\n",
    "{exemplar[\"context\"][:800]}{\"...\" if len(exemplar[\"context\"]) > 800 else \"\"}\n",
    "\n",
    "EXAMPLE {i} KNOWLEDGE GRAPH TRIPLES:\n",
    "{exemplar_triple_text}\n",
    "\n",
    "EXAMPLE {i} {question_type.value.upper()} QUESTION:\n",
    "{exemplar_question}\n",
    "\"\"\"\n",
    "            exemplar_demonstrations.append(demonstration)\n",
    "\n",
    "        # Combine all demonstrations\n",
    "        all_demonstrations = \"\\n\".join(exemplar_demonstrations)\n",
    "\n",
    "        # Create comprehensive few-shot prompt\n",
    "        prompt = f\"\"\"\n",
    "FEW-SHOT QA GENERATION TASK\n",
    "\n",
    "TASK TYPE: {question_type.value.upper()}\n",
    "{task_description}\n",
    "\n",
    "MULTIPLE EXEMPLAR DEMONSTRATIONS:\n",
    "{all_demonstrations}\n",
    "\n",
    "\n",
    "NOW GENERATE FOR NEW CONTEXT:\n",
    "\n",
    "TARGET CONTEXT:\n",
    "{context[:1200]}{\"...\" if len(context) > 1200 else \"\"}\n",
    "\n",
    "TARGET KNOWLEDGE GRAPH TRIPLES:\n",
    "{triple_text}\n",
    "\n",
    "GENERATION INSTRUCTIONS: {generation_guidance}\n",
    "\n",
    "ENHANCED DIVERSITY REQUIREMENTS:\n",
    "- Study the patterns from ALL {len(FEW_SHOT_EXEMPLARS)} examples above\n",
    "- Each question must be UNIQUE and ask about DIFFERENT aspects\n",
    "- Use VARIED question starters and phrasing patterns inspired by the examples\n",
    "- Focus on DIFFERENT entities, relationships, or information types\n",
    "- Avoid repetitive structures or similar wordings\n",
    "- Make each question distinctly different from others and from ALL exemplars\n",
    "\n",
    "REQUIRED OUTPUT FORMAT:\n",
    "[\n",
    "  {{\n",
    "    \"id\": \"1\",\n",
    "    \"question\": \"Your detailed question here?\",\n",
    "    \"answer\": \"Your comprehensive answer here.\",\n",
    "    \"type\": \"{question_type.value}\",\n",
    "    \"qa_metadata\": {{\n",
    "      \"mentioned_entities\": [\"entity1\", \"entity2\"],\n",
    "      \"mentioned_relationships\": [\"relationship1\", \"relationship2\"]\n",
    "    }}\n",
    "  }}\n",
    "]\n",
    "\n",
    "Generate {num_questions} {question_type.value} questions with answers, drawing inspiration from the diverse patterns shown in the {len(FEW_SHOT_EXEMPLARS)} examples above.\n",
    "\"\"\"\n",
    "        return prompt.strip()\n",
    "    \n",
    "    def parse_json_response(self, text_response: str) -> List[Dict]:\n",
    "        \"\"\"Parse JSON response from GPT-4 with improved error handling.\"\"\"\n",
    "        # Remove common markdown formatting that can break JSON parsing\n",
    "        text_response = text_response.strip()\n",
    "        if text_response.startswith('```json'):\n",
    "            text_response = text_response[7:]  # Remove ```json\n",
    "        if text_response.endswith('```'):\n",
    "            text_response = text_response[:-3]  # Remove trailing ```\n",
    "        text_response = text_response.strip()\n",
    "        \n",
    "        try:\n",
    "            parsed = json.loads(text_response)\n",
    "            if isinstance(parsed, list):\n",
    "                return parsed\n",
    "            elif isinstance(parsed, dict) and \"question\" in parsed:\n",
    "                return [parsed]\n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.debug(f\"Initial JSON parse failed: {e}\")\n",
    "        \n",
    "        # Fallback parsing strategies\n",
    "        try:\n",
    "            json_start = text_response.find('[')\n",
    "            json_end = text_response.rfind(']') + 1\n",
    "            if json_start >= 0 and json_end > json_start:\n",
    "                json_str = text_response[json_start:json_end]\n",
    "                parsed = json.loads(json_str)\n",
    "                if isinstance(parsed, list):\n",
    "                    return parsed\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Array extraction failed: {e}\")\n",
    "        \n",
    "        try:\n",
    "            json_start = text_response.find('{')\n",
    "            json_end = text_response.rfind('}') + 1\n",
    "            if json_start >= 0 and json_end > json_start:\n",
    "                json_str = text_response[json_start:json_end]\n",
    "                parsed = json.loads(json_str)\n",
    "                if isinstance(parsed, dict) and \"question\" in parsed:\n",
    "                    return [parsed]\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Object extraction failed: {e}\")\n",
    "        \n",
    "        logger.warning(f\"Failed to parse JSON response: {text_response[:200]}...\")\n",
    "        return []\n",
    "\n",
    "    def call_openai_api(self, system_message: str, user_message: str, max_tokens: int = 4000, temperature: float = 0.7) -> str:\n",
    "        \"\"\"Call OpenAI API with proper version handling and temperature bounds.\"\"\"\n",
    "        try:\n",
    "            # Ensure temperature is within valid bounds\n",
    "            temperature = min(max(temperature, 0.0), 1.0)\n",
    "            \n",
    "            if OPENAI_V1:\n",
    "                response = self.openai_client.chat.completions.create(\n",
    "                    model=\"gpt-4-turbo-preview\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_message},\n",
    "                        {\"role\": \"user\", \"content\": user_message}\n",
    "                    ],\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=temperature,\n",
    "                    presence_penalty=0.3,\n",
    "                    frequency_penalty=0.3\n",
    "                )\n",
    "                return response.choices[0].message.content.strip()\n",
    "            else:\n",
    "                response = openai.ChatCompletion.create(\n",
    "                    model=\"gpt-4-turbo-preview\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_message},\n",
    "                        {\"role\": \"user\", \"content\": user_message}\n",
    "                    ],\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=temperature,\n",
    "                    presence_penalty=0.3,\n",
    "                    frequency_penalty=0.3\n",
    "                )\n",
    "                return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"OpenAI API call failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_qa_pairs(self, \n",
    "                         context: str, \n",
    "                         triples: List[Tuple], \n",
    "                         question_type: QuestionType, \n",
    "                         num_questions: int,\n",
    "                         chunk_id: str) -> List[QAPair]:\n",
    "        \"\"\"Generate QA pairs using few-shot learning approach with basic filtering system.\"\"\"\n",
    "        qa_pairs = []\n",
    "        max_retries = 3\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Count API calls (separate from QA attempts)\n",
    "                self.generation_stats[\"total_api_calls\"] += 1\n",
    "                \n",
    "                prompt = self.generate_few_shot_prompt(context, triples, question_type, num_questions)\n",
    "                \n",
    "                system_message = \"\"\"You are an expert at generating domain-specific QA pairs using few-shot learning with multiple exemplar guidance. \n",
    "\n",
    "You have been provided with multiple high-quality exemplars that demonstrate diverse patterns and approaches for each question type. Use these exemplars as guides to understand the range of possibilities and generate similar high-quality questions for the new context provided.\n",
    "\n",
    "Key principles:\n",
    "- Study ALL provided examples to understand pattern diversity\n",
    "- Generate questions that demonstrate understanding of the domain relationships and entities\n",
    "- Follow the varied patterns established by the multiple exemplars\n",
    "- Ensure each question explores different aspects of the context\n",
    "- Maintain the quality and depth shown in the examples\n",
    "\n",
    "Your output MUST be valid JSON in the exact format specified. Generate questions that demonstrate understanding of the domain relationships and entities present in the provided knowledge graph, following the diverse patterns established by the exemplars.\n",
    "\n",
    "IMPORTANT: In the qa_metadata field, accurately list the specific entities and relationships that you mention in your question and answer.\"\"\"\n",
    "\n",
    "                logger.debug(f\"Generating {num_questions} {question_type.value} questions using few-shot learning (attempt {attempt + 1})\")\n",
    "                \n",
    "                # Temperature progression with bounds\n",
    "                temperature = min(0.8 + (attempt * 0.1), 1.0)\n",
    "                \n",
    "                text_response = self.call_openai_api(\n",
    "                    system_message, \n",
    "                    prompt, \n",
    "                    temperature=temperature\n",
    "                )\n",
    "                \n",
    "                parsed_items = self.parse_json_response(text_response)\n",
    "                \n",
    "                if parsed_items:\n",
    "                    logger.info(f\"Successfully parsed {len(parsed_items)} QA pairs from few-shot response\")\n",
    "                    \n",
    "                    for item in parsed_items:\n",
    "                        q_text = item.get(\"question\", \"\").strip()\n",
    "                        a_text = item.get(\"answer\", \"\").strip()\n",
    "                        qa_metadata = item.get(\"qa_metadata\", {\n",
    "                            \"mentioned_entities\": [],\n",
    "                            \"mentioned_relationships\": []\n",
    "                        })\n",
    "\n",
    "                        if q_text and a_text:\n",
    "                            # Apply filtering (statistics are handled inside filter_qa_pair)\n",
    "                            accepted, filtering_result = self.filter_qa_pair(\n",
    "                                q_text, a_text, context, triples, qa_metadata\n",
    "                            )\n",
    "                            \n",
    "                            if accepted:\n",
    "                                unique_id = f\"{question_type.value}_fewshot_{chunk_id}_{int(time.time())}_{str(uuid.uuid4())[:8]}\"\n",
    "                                qa_pair = QAPair(\n",
    "                                    id=unique_id,\n",
    "                                    question=q_text,\n",
    "                                    answer=a_text,\n",
    "                                    question_type=question_type,  # Keep as enum for internal use\n",
    "                                    qa_metadata=qa_metadata,\n",
    "                                    filtering_result=filtering_result,\n",
    "                                    generation_method=\"few_shot\",\n",
    "                                    source_context=context,\n",
    "                                    source_triples=triples.copy(),\n",
    "                                    chunk_id=chunk_id\n",
    "                                )\n",
    "                                \n",
    "                                qa_pairs.append(qa_pair)\n",
    "                                \n",
    "                                logger.debug(f\"Accepted QA pair: {filtering_result.decision.value} \"\n",
    "                                           f\"(relevance={filtering_result.relevance_score:.3f})\")\n",
    "                                \n",
    "                                if len(qa_pairs) >= num_questions:\n",
    "                                    return qa_pairs[:num_questions]\n",
    "                            else:\n",
    "                                logger.debug(f\"Rejected QA pair: {filtering_result.reasoning}\")\n",
    "                        else:\n",
    "                            # Handle parsing failures - empty question or answer\n",
    "                            self.generation_stats[\"total_qa_attempts\"] += 1\n",
    "                            self.generation_stats[\"filtering_decisions\"][\"rejected_parsing\"] += 1\n",
    "                            \n",
    "                            # Store failed parsing attempt\n",
    "                            dummy_result = FilteringResult(\n",
    "                                accepted=False,\n",
    "                                decision=FilteringDecision.REJECTED_PARSING,\n",
    "                                relevance_score=0.0,\n",
    "                                context_similarity=0.0,\n",
    "                                triple_similarity=0.0,\n",
    "                                reasoning=\"Empty question or answer from parsing\",\n",
    "                                metadata={}\n",
    "                            )\n",
    "                            \n",
    "                            self.generation_stats[\"all_attempts\"].append({\n",
    "                                \"question\": q_text,\n",
    "                                \"answer\": a_text,\n",
    "                                \"filtering_result\": self._filtering_result_to_dict(dummy_result),\n",
    "                                \"relevance_score\": 0.0\n",
    "                            })\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in generation attempt {attempt + 1}: {str(e)}\")\n",
    "                time.sleep(min(2 ** attempt, 10))  # Exponential backoff with cap\n",
    "                continue\n",
    "\n",
    "        return qa_pairs\n",
    "\n",
    "    def create_few_shot_dataset(self, \n",
    "                               merged_grouped: Dict, \n",
    "                               output_file: str, \n",
    "                               limit: Optional[int] = None) -> None:\n",
    "        \"\"\"Create a complete few-shot QA dataset with basic filtering.\"\"\"\n",
    "        logger.info(\"Starting few-shot QA dataset creation with multi-exemplar guidance\")\n",
    "        \n",
    "        all_qa_pairs = []\n",
    "        question_types = list(QuestionType)\n",
    "        \n",
    "        # Process chunks\n",
    "        chunks_to_process = list(merged_grouped.items())\n",
    "        if limit:\n",
    "            chunks_to_process = chunks_to_process[:limit]\n",
    "            \n",
    "        total_chunks = len(chunks_to_process)\n",
    "        logger.info(f\"Processing {total_chunks} chunks with few-shot learning\")\n",
    "        \n",
    "        for i, (chunk_id, chunk_data) in enumerate(chunks_to_process, 1):\n",
    "            logger.info(f\"Processing chunk {i}/{total_chunks}: {chunk_id}\")\n",
    "            \n",
    "            # Reset batch tracking for each chunk if specified\n",
    "            if self.reset_duplicates_per_chunk:\n",
    "                self.batch_questions.clear()\n",
    "                self.batch_embeddings.clear()\n",
    "            \n",
    "            context = chunk_data['text']\n",
    "            triples = chunk_data['triples']\n",
    "            \n",
    "            if not triples:\n",
    "                logger.warning(f\"No triples found for chunk {chunk_id}, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Generate QA pairs for each question type using few-shot learning\n",
    "            for question_type in question_types:\n",
    "                try:\n",
    "                    logger.debug(f\"Generating {question_type.value} questions for chunk {chunk_id} using few-shot learning\")\n",
    "                    \n",
    "                    qa_pairs = self.generate_qa_pairs(\n",
    "                        context=context,\n",
    "                        triples=triples,\n",
    "                        question_type=question_type,\n",
    "                        num_questions=2,\n",
    "                        chunk_id=chunk_id\n",
    "                    )\n",
    "                    \n",
    "                    all_qa_pairs.extend(qa_pairs)\n",
    "                    logger.info(f\"Generated {len(qa_pairs)} {question_type.value} QA pairs for chunk {chunk_id}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error generating {question_type.value} questions for chunk {chunk_id}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Add delay between chunks to avoid rate limiting\n",
    "            if i < total_chunks:\n",
    "                time.sleep(2)\n",
    "        \n",
    "        # Get embedding model name safely\n",
    "        try:\n",
    "            if hasattr(self.embedding_model, 'model_name'):\n",
    "                embedding_model_name = self.embedding_model.model_name\n",
    "            elif hasattr(self.embedding_model, '_model_name'):\n",
    "                embedding_model_name = self.embedding_model._model_name\n",
    "            elif hasattr(self.embedding_model, 'config') and hasattr(self.embedding_model.config, 'name_or_path'):\n",
    "                embedding_model_name = self.embedding_model.config.name_or_path\n",
    "            else:\n",
    "                try:\n",
    "                    embedding_model_name = self.embedding_model._modules['0'].auto_model.config.name_or_path\n",
    "                except:\n",
    "                    embedding_model_name = 'BAAI/bge-large-en-v1.5'\n",
    "        except:\n",
    "            embedding_model_name = 'BAAI/bge-large-en-v1.5'\n",
    "        \n",
    "        # Calculate question type statistics with proper enum handling\n",
    "        question_type_stats = {}\n",
    "        question_type_quality = {}\n",
    "        \n",
    "        for qa_pair in all_qa_pairs:\n",
    "            q_type = qa_pair.question_type.value  # Convert enum to string here\n",
    "            \n",
    "            # Count by type\n",
    "            if q_type not in question_type_stats:\n",
    "                question_type_stats[q_type] = {\n",
    "                    \"count\": 0,\n",
    "                    \"avg_relevance_score\": 0.0,\n",
    "                    \"avg_question_length\": 0.0,\n",
    "                    \"avg_answer_length\": 0.0,\n",
    "                    \"avg_context_similarity\": 0.0,\n",
    "                    \"avg_triple_similarity\": 0.0\n",
    "                }\n",
    "                question_type_quality[q_type] = {\n",
    "                    \"relevance_scores\": [],\n",
    "                    \"question_lengths\": [],\n",
    "                    \"answer_lengths\": [],\n",
    "                    \"context_similarities\": [],\n",
    "                    \"triple_similarities\": []\n",
    "                }\n",
    "            \n",
    "            question_type_stats[q_type][\"count\"] += 1\n",
    "            \n",
    "            # Collect quality metrics\n",
    "            question_type_quality[q_type][\"relevance_scores\"].append(qa_pair.filtering_result.relevance_score)\n",
    "            question_type_quality[q_type][\"question_lengths\"].append(len(qa_pair.question.split()))\n",
    "            question_type_quality[q_type][\"answer_lengths\"].append(len(qa_pair.answer.split()))\n",
    "            question_type_quality[q_type][\"context_similarities\"].append(qa_pair.filtering_result.context_similarity)\n",
    "            question_type_quality[q_type][\"triple_similarities\"].append(qa_pair.filtering_result.triple_similarity)\n",
    "        \n",
    "        # Calculate averages for each question type\n",
    "        for q_type in question_type_stats:\n",
    "            metrics = question_type_quality[q_type]\n",
    "            question_type_stats[q_type][\"avg_relevance_score\"] = np.mean(metrics[\"relevance_scores\"]) if metrics[\"relevance_scores\"] else 0.0\n",
    "            question_type_stats[q_type][\"avg_question_length\"] = np.mean(metrics[\"question_lengths\"]) if metrics[\"question_lengths\"] else 0.0\n",
    "            question_type_stats[q_type][\"avg_answer_length\"] = np.mean(metrics[\"answer_lengths\"]) if metrics[\"answer_lengths\"] else 0.0\n",
    "            question_type_stats[q_type][\"avg_context_similarity\"] = np.mean(metrics[\"context_similarities\"]) if metrics[\"context_similarities\"] else 0.0\n",
    "            question_type_stats[q_type][\"avg_triple_similarity\"] = np.mean(metrics[\"triple_similarities\"]) if metrics[\"triple_similarities\"] else 0.0\n",
    "\n",
    "        # Create final dataset structure with enhanced metadata\n",
    "        dataset = {\n",
    "            \"metadata\": {\n",
    "                \"creation_date\": pd.Timestamp.now().isoformat(),\n",
    "                \"total_queries\": len(all_qa_pairs),\n",
    "                \"generation_method\": \"few_shot\",\n",
    "                \"model_version\": \"1.0\",\n",
    "                \"embedding_model\": embedding_model_name,\n",
    "                \"filtering_approach\": \"essential_quality_filters\",\n",
    "                \"exemplar_info\": {\n",
    "                    \"num_exemplars\": len(FEW_SHOT_EXEMPLARS),\n",
    "                    \"exemplar_contexts_length\": [len(ex[\"context\"]) for ex in FEW_SHOT_EXEMPLARS],\n",
    "                    \"exemplar_triples_count\": [len(ex[\"knowledge_graph\"]) for ex in FEW_SHOT_EXEMPLARS],\n",
    "                    \"pattern_diversity\": \"Multiple contexts covering different policy aspects\",\n",
    "                    \"question_type_coverage\": list(FEW_SHOT_EXEMPLARS[0][\"exemplar_questions\"].keys())\n",
    "                },\n",
    "                \"quality_thresholds\": {\n",
    "                    \"min_question_words\": self.thresholds.min_question_words,\n",
    "                    \"min_answer_words\": self.thresholds.min_answer_words,\n",
    "                    \"duplicate_similarity_threshold\": self.thresholds.duplicate_similarity_threshold,\n",
    "                    \"batch_similarity_threshold\": self.thresholds.batch_similarity_threshold\n",
    "                },\n",
    "                \"generation_statistics\": self.get_filtering_statistics(),\n",
    "                \"question_type_breakdown\": question_type_stats,\n",
    "                \"processing_summary\": {\n",
    "                    \"chunks_processed\": total_chunks,\n",
    "                    \"questions_per_chunk\": 2 * len(question_types),  # 2 per question type\n",
    "                    \"question_types_generated\": [q_type.value for q_type in question_types],  # Convert enums to strings\n",
    "                    \"overall_quality\": {\n",
    "                        \"avg_relevance_score\": np.mean([qa.filtering_result.relevance_score for qa in all_qa_pairs]) if all_qa_pairs else 0.0,\n",
    "                        \"avg_question_length\": np.mean([len(qa.question.split()) for qa in all_qa_pairs]) if all_qa_pairs else 0.0,\n",
    "                        \"avg_answer_length\": np.mean([len(qa.answer.split()) for qa in all_qa_pairs]) if all_qa_pairs else 0.0,\n",
    "                        \"avg_context_similarity\": np.mean([qa.filtering_result.context_similarity for qa in all_qa_pairs]) if all_qa_pairs else 0.0,\n",
    "                        \"avg_triple_similarity\": np.mean([qa.filtering_result.triple_similarity for qa in all_qa_pairs]) if all_qa_pairs else 0.0\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"queries\": []\n",
    "        }\n",
    "        \n",
    "        # Convert QA pairs to dictionary format with proper enum serialization\n",
    "        for qa_pair in all_qa_pairs:\n",
    "            query_dict = {\n",
    "                \"id\": qa_pair.id,\n",
    "                \"question\": qa_pair.question,\n",
    "                \"answer\": qa_pair.answer,\n",
    "                \"question_type\": qa_pair.question_type.value,  # Convert enum to string\n",
    "                \"qa_metadata\": qa_pair.qa_metadata,\n",
    "                \"filtering_result\": {\n",
    "                    \"accepted\": qa_pair.filtering_result.accepted,\n",
    "                    \"decision\": qa_pair.filtering_result.decision.value,  # Convert enum to string\n",
    "                    \"relevance_score\": qa_pair.filtering_result.relevance_score,\n",
    "                    \"context_similarity\": qa_pair.filtering_result.context_similarity,\n",
    "                    \"triple_similarity\": qa_pair.filtering_result.triple_similarity,\n",
    "                    \"reasoning\": qa_pair.filtering_result.reasoning,\n",
    "                    \"metadata\": qa_pair.filtering_result.metadata\n",
    "                },\n",
    "                \"generation_method\": qa_pair.generation_method,\n",
    "                # Add ground truth information for evaluation\n",
    "                \"ground_truth\": {\n",
    "                    \"source_context\": qa_pair.source_context,\n",
    "                    \"source_triples\": [\n",
    "                        {\n",
    "                            \"subject\": str(triple[0]),  # Ensure string conversion\n",
    "                            \"predicate\": str(triple[1]), \n",
    "                            \"object\": str(triple[2])\n",
    "                        } for triple in qa_pair.source_triples if len(triple) >= 3\n",
    "                    ],\n",
    "                    \"chunk_id\": qa_pair.chunk_id,\n",
    "                    \"context_length_chars\": len(qa_pair.source_context),\n",
    "                    \"num_source_triples\": len(qa_pair.source_triples)\n",
    "                }\n",
    "            }\n",
    "            dataset[\"queries\"].append(query_dict)\n",
    "        \n",
    "        # Ensure all data is JSON serializable before saving\n",
    "        dataset = self._ensure_json_serializable(dataset)\n",
    "        \n",
    "        # Save dataset\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(dataset, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        logger.info(f\"Few-shot dataset creation completed!\")\n",
    "        logger.info(f\"Total QA pairs generated: {len(all_qa_pairs)}\")\n",
    "        logger.info(f\"Dataset saved to: {output_file}\")\n",
    "        \n",
    "        # Log detailed statistics\n",
    "        stats = self.get_filtering_statistics()\n",
    "        logger.info(\"\\nFew-Shot Basic Filtering Statistics:\")\n",
    "        logger.info(f\"Total API calls: {stats.get('total_api_calls', 0)}\")\n",
    "        logger.info(f\"Total QA attempts: {stats.get('total_qa_attempts', 0)}\")\n",
    "        logger.info(f\"Successful generations: {stats.get('successful_generations', 0)}\")\n",
    "        \n",
    "        logger.info(\"\\nFiltering Decision Breakdown:\")\n",
    "        for decision_type, count in stats[\"filtering_decisions\"].items():\n",
    "            if count > 0:\n",
    "                percentage = (count / stats['total_qa_attempts']) * 100 if stats['total_qa_attempts'] > 0 else 0\n",
    "                logger.info(f\"  {decision_type}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        if \"acceptance_rate\" in stats:\n",
    "            logger.info(f\"\\nOverall Acceptance Rate: {stats['acceptance_rate']:.1f}%\")\n",
    "        \n",
    "        # Log question type breakdown\n",
    "        logger.info(\"\\nQuestion Type Breakdown:\")\n",
    "        for q_type, type_stats in question_type_stats.items():\n",
    "            count = type_stats[\"count\"]\n",
    "            avg_relevance = type_stats[\"avg_relevance_score\"]\n",
    "            percentage = (count / len(all_qa_pairs)) * 100 if all_qa_pairs else 0\n",
    "            logger.info(f\"  {q_type}: {count} questions ({percentage:.1f}%) - avg relevance: {avg_relevance:.3f}\")\n",
    "        \n",
    "        # Validation check\n",
    "        total_decisions = sum(stats[\"filtering_decisions\"].values())\n",
    "        if total_decisions == stats[\"total_qa_attempts\"]:\n",
    "            logger.info(\"\\n Statistics validation: PASSED\")\n",
    "        else:\n",
    "            logger.warning(f\"\\n Statistics validation: FAILED ({total_decisions} decisions vs {stats['total_qa_attempts']} attempts)\")\n",
    "\n",
    "    def _ensure_json_serializable(self, obj):\n",
    "        \"\"\"Recursively ensure all objects in nested structure are JSON serializable.\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            return {key: self._ensure_json_serializable(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self._ensure_json_serializable(item) for item in obj]\n",
    "        elif isinstance(obj, tuple):\n",
    "            return [self._ensure_json_serializable(item) for item in obj]\n",
    "        elif isinstance(obj, (QuestionType, FilteringDecision)):\n",
    "            return obj.value\n",
    "        elif hasattr(obj, '__dict__'):\n",
    "            # Handle custom objects by converting to dict\n",
    "            return self._ensure_json_serializable(obj.__dict__)\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "    def get_filtering_statistics(self) -> Dict:\n",
    "        \"\"\"Get comprehensive statistics about filtering decisions with JSON serialization safety.\"\"\"\n",
    "        stats = self.generation_stats.copy()\n",
    "        \n",
    "        # Calculate acceptance rate based on QA pairs processed\n",
    "        total_qa_attempts = stats[\"total_qa_attempts\"]\n",
    "        if total_qa_attempts > 0:\n",
    "            acceptance_rate = (stats[\"successful_generations\"] / total_qa_attempts) * 100\n",
    "            stats[\"acceptance_rate\"] = acceptance_rate\n",
    "        else:\n",
    "            stats[\"acceptance_rate\"] = 0.0\n",
    "        \n",
    "        # Add validation check\n",
    "        total_decisions = sum(stats[\"filtering_decisions\"].values())\n",
    "        if total_decisions != total_qa_attempts:\n",
    "            logger.warning(f\"Statistics mismatch: {total_decisions} decisions vs {total_qa_attempts} attempts\")\n",
    "            stats[\"validation_status\"] = \"FAILED\"\n",
    "        else:\n",
    "            stats[\"validation_status\"] = \"PASSED\"\n",
    "        \n",
    "        # Ensure all nested objects are JSON serializable\n",
    "        return self._ensure_json_serializable(stats)\n",
    "\n",
    "\n",
    "# Analysis and utility functions\n",
    "\n",
    "def analyze_few_shot_filtering_results(dataset_path: str, generator_stats: Optional[Dict] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze the results of few-shot filtering using both dataset and generation statistics.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to the generated dataset\n",
    "        generator_stats: Optional generator statistics for complete analysis\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "            dataset = json.load(f)\n",
    "        \n",
    "        queries = dataset.get(\"queries\", [])\n",
    "        metadata = dataset.get(\"metadata\", {})\n",
    "        \n",
    "        if not queries:\n",
    "            return {\"error\": \"No queries found in dataset\"}\n",
    "        \n",
    "        # Get actual generation statistics from metadata if available\n",
    "        generation_stats = metadata.get(\"generation_statistics\", {})\n",
    "        if generator_stats:\n",
    "            generation_stats = generator_stats\n",
    "        \n",
    "        # Analyze accepted queries (those in the final dataset)\n",
    "        accepted_count = len(queries)\n",
    "        \n",
    "        # Calculate quality metrics for accepted queries\n",
    "        quality_metrics = {\n",
    "            \"relevance_scores\": [],\n",
    "            \"question_lengths\": [],\n",
    "            \"answer_lengths\": [],\n",
    "            \"context_similarities\": [],\n",
    "            \"triple_similarities\": []\n",
    "        }\n",
    "        \n",
    "        question_type_distribution = {}\n",
    "        \n",
    "        for query in queries:\n",
    "            filtering_info = query.get(\"filtering_result\", {})\n",
    "            quality_metrics[\"relevance_scores\"].append(filtering_info.get(\"relevance_score\", 0.0))\n",
    "            quality_metrics[\"question_lengths\"].append(len(query[\"question\"].split()))\n",
    "            quality_metrics[\"answer_lengths\"].append(len(query[\"answer\"].split()))\n",
    "            quality_metrics[\"context_similarities\"].append(filtering_info.get(\"context_similarity\", 0.0))\n",
    "            quality_metrics[\"triple_similarities\"].append(filtering_info.get(\"triple_similarity\", 0.0))\n",
    "            \n",
    "            # Count question types\n",
    "            q_type = query.get(\"question_type\", \"unknown\")\n",
    "            question_type_distribution[q_type] = question_type_distribution.get(q_type, 0) + 1\n",
    "        \n",
    "        # Use generation statistics for complete picture\n",
    "        total_attempts = generation_stats.get(\"total_qa_attempts\", accepted_count)\n",
    "        rejected_count = total_attempts - accepted_count\n",
    "        \n",
    "        # Calculate comprehensive statistics\n",
    "        analysis_results = {\n",
    "            \"total_qa_attempts\": total_attempts,\n",
    "            \"final_dataset_queries\": accepted_count,\n",
    "            \"decision_distribution\": {\n",
    "                \"accepted_count\": accepted_count,\n",
    "                \"rejected_count\": rejected_count,\n",
    "                \"acceptance_rate\": (accepted_count / total_attempts) * 100 if total_attempts > 0 else 0\n",
    "            },\n",
    "            \"detailed_filtering_breakdown\": generation_stats.get(\"filtering_decisions\", {}),\n",
    "            \"question_type_distribution\": question_type_distribution,\n",
    "            \"quality_statistics\": {\n",
    "                \"avg_relevance_score\": np.mean(quality_metrics[\"relevance_scores\"]) if quality_metrics[\"relevance_scores\"] else 0,\n",
    "                \"min_relevance_score\": min(quality_metrics[\"relevance_scores\"]) if quality_metrics[\"relevance_scores\"] else 0,\n",
    "                \"max_relevance_score\": max(quality_metrics[\"relevance_scores\"]) if quality_metrics[\"relevance_scores\"] else 0,\n",
    "                \"std_relevance_score\": np.std(quality_metrics[\"relevance_scores\"]) if quality_metrics[\"relevance_scores\"] else 0,\n",
    "                \"avg_question_length\": np.mean(quality_metrics[\"question_lengths\"]) if quality_metrics[\"question_lengths\"] else 0,\n",
    "                \"avg_answer_length\": np.mean(quality_metrics[\"answer_lengths\"]) if quality_metrics[\"answer_lengths\"] else 0,\n",
    "                \"avg_context_similarity\": np.mean(quality_metrics[\"context_similarities\"]) if quality_metrics[\"context_similarities\"] else 0,\n",
    "                \"avg_triple_similarity\": np.mean(quality_metrics[\"triple_similarities\"]) if quality_metrics[\"triple_similarities\"] else 0\n",
    "            },\n",
    "            \"generation_efficiency\": {\n",
    "                \"api_calls\": generation_stats.get(\"total_api_calls\", 0),\n",
    "                \"qa_per_api_call\": accepted_count / generation_stats.get(\"total_api_calls\", 1),\n",
    "                \"acceptance_rate\": generation_stats.get(\"acceptance_rate\", 0)\n",
    "            },\n",
    "            \"exemplar_effectiveness\": metadata.get(\"exemplar_info\", {}),\n",
    "            \"pattern_diversity_analysis\": {\n",
    "                \"num_exemplars_used\": metadata.get(\"exemplar_info\", {}).get(\"num_exemplars\", 0),\n",
    "                \"pattern_coverage\": \"Multiple regulatory contexts with diverse structures\",\n",
    "                \"learning_approach\": \"Few-shot pattern recognition from varied examples\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return analysis_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error analyzing few-shot filtering results: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "def generate_few_shot_report(dataset_path: str, output_report_path: str, generator_stats: Optional[Dict] = None) -> None:\n",
    "    \"\"\"\n",
    "    Generate a comprehensive report on few-shot filtering performance.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to the few-shot filtering dataset\n",
    "        output_report_path: Path to save the filtering report\n",
    "        generator_stats: Optional generator statistics for complete analysis\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Analyze with complete statistics\n",
    "        analysis = analyze_few_shot_filtering_results(dataset_path, generator_stats)\n",
    "        \n",
    "        if \"error\" in analysis:\n",
    "            logger.error(f\"Analysis failed: {analysis['error']}\")\n",
    "            return\n",
    "        \n",
    "        # Generate comprehensive report\n",
    "        report = {\n",
    "            \"report_metadata\": {\n",
    "                \"generation_date\": pd.Timestamp.now().isoformat(),\n",
    "                \"dataset_analyzed\": dataset_path,\n",
    "                \"report_type\": \"Few_Shot_QA_Analysis\",\n",
    "                \"approach\": \"Multi-exemplar guided generation with essential quality filtering\"\n",
    "            },\n",
    "            \"executive_summary\": {\n",
    "                \"total_qa_attempts\": analysis.get(\"total_qa_attempts\", 0),\n",
    "                \"final_questions_generated\": analysis.get(\"final_dataset_queries\", 0),\n",
    "                \"overall_acceptance_rate\": f\"{analysis.get('decision_distribution', {}).get('acceptance_rate', 0):.1f}%\",\n",
    "                \"avg_relevance_score\": f\"{analysis.get('quality_statistics', {}).get('avg_relevance_score', 0):.3f}\",\n",
    "                \"api_efficiency\": f\"{analysis.get('generation_efficiency', {}).get('qa_per_api_call', 0):.1f} QA pairs per API call\"\n",
    "            },\n",
    "            \"detailed_analysis\": analysis,\n",
    "            \"few_shot_approach\": {\n",
    "                \"method\": \"Multi-exemplar guided generation using diverse high-quality template questions\",\n",
    "                \"exemplar_sources\": \"Multiple manually constructed regulatory texts with KG triples\",\n",
    "                \"pattern_diversity\": \"Varied contexts covering different policy aspects and structures\",\n",
    "                \"question_types_covered\": [\"factual\", \"relationship\", \"comparative\", \"inferential\"],\n",
    "                \"filtering_approach\": \"Essential quality filters: length requirements + duplicate detection\"\n",
    "            },\n",
    "            \"advantages_of_few_shot\": [\n",
    "                \"Provides diverse pattern recognition through multiple exemplars\",\n",
    "                \"Enables learning from varied question structures and approaches\",\n",
    "                \"Reduces over-fitting to single exemplar patterns\",\n",
    "                \"Improves generalization across different contexts\",\n",
    "                \"Maintains quality while increasing structural variety\",\n",
    "                \"Better coverage of domain-specific language patterns\",\n",
    "                \"Enhanced robustness through pattern diversity\"\n",
    "            ],\n",
    "            \"pattern_learning_insights\": {\n",
    "                \"exemplar_diversity\": f\"Used {analysis.get('exemplar_effectiveness', {}).get('num_exemplars', 0)} diverse exemplars\",\n",
    "                \"structural_variety\": \"Multiple question formulation approaches learned\",\n",
    "                \"domain_adaptation\": \"Better adaptation to regulatory language patterns\",\n",
    "                \"quality_consistency\": \"Maintained quality across diverse pattern types\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save report\n",
    "        with open(output_report_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        logger.info(f\"Few-shot filtering report generated: {output_report_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating few-shot filtering report: {e}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate the few-shot learning QA generation system.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting Few-Shot QA Generation with Multi-Exemplar Guidance\")\n",
    "    \n",
    "    try:\n",
    "        # Get API key\n",
    "        openai_api_key = input(\"Please enter your OpenAI API key: \").strip()\n",
    "        \n",
    "        if not openai_api_key or openai_api_key == \"your-openai-api-key-here\":\n",
    "            logger.error(\"Please provide a valid OpenAI API key\")\n",
    "            return\n",
    "        \n",
    "        # Initialize generator with few-shot learning\n",
    "        generator = FewShotQAGenerator(\n",
    "            openai_api_key=openai_api_key,\n",
    "            reset_duplicates_per_chunk=True,\n",
    "            embedding_model='BAAI/bge-large-en-v1.5',\n",
    "            max_embedding_cache=1000\n",
    "        )\n",
    "\n",
    "        # File paths\n",
    "        chunks_file = input(\"Enter path to chunks CSV file (or press Enter for 'chunks.csv'): \").strip() or \"chunks.csv\"\n",
    "        triples_file = input(\"Enter path to triples CSV file (or press Enter for 'Ontology_Guided_Triples.csv'): \").strip() or \"Ontology_Guided_Triples.csv\"\n",
    "        output_file = input(\"Enter output file name (or press Enter for 'Few-Shot_qa_dataset.json'): \").strip() or \"Few-Shot_qa_dataset.json\"\n",
    "\n",
    "        # Check if files exist\n",
    "        if not os.path.exists(chunks_file):\n",
    "            logger.error(f\"Chunks file not found: {chunks_file}\")\n",
    "            logger.info(\"Please ensure your chunks file exists and has the correct path\")\n",
    "            return\n",
    "            \n",
    "        if not os.path.exists(triples_file):\n",
    "            logger.error(f\"Triples file not found: {triples_file}\")\n",
    "            logger.info(\"Please ensure your triples file exists and has the correct path\")\n",
    "            return\n",
    "\n",
    "        # Load and merge data\n",
    "        logger.info(\"Loading and merging data files\")\n",
    "        merged_data = generator.load_and_merge_data(chunks_file, triples_file)\n",
    "\n",
    "        if not merged_data:\n",
    "            logger.error(\"No data loaded. Please check your input files.\")\n",
    "            return\n",
    "        \n",
    "        # Get user input for number of chunks to process\n",
    "        try:\n",
    "            limit = input(\"Enter number of chunks to process (or press Enter for all): \").strip()\n",
    "            limit = int(limit) if limit else None\n",
    "        except ValueError:\n",
    "            limit = None\n",
    "            logger.info(\"Using default: processing all chunks\")\n",
    "        \n",
    "        if limit is None:\n",
    "            logger.info(\"Processing all chunks\")\n",
    "        else:\n",
    "            logger.info(f\"Processing {limit} chunks\")\n",
    "        \n",
    "        # Create few-shot learning dataset\n",
    "        logger.info(f\"Creating Few-Shot QA dataset using multi-exemplar guidance\")\n",
    "        generator.create_few_shot_dataset(\n",
    "            merged_grouped=merged_data,\n",
    "            output_file=output_file,\n",
    "            limit=limit\n",
    "        )\n",
    "\n",
    "        # Get final statistics from generator\n",
    "        final_stats = generator.get_filtering_statistics()\n",
    "\n",
    "        # Generate analysis report\n",
    "        logger.info(\"Analyzing few-shot filtering results\")\n",
    "        analysis_results = analyze_few_shot_filtering_results(output_file, final_stats)\n",
    "        \n",
    "        if \"error\" in analysis_results:\n",
    "            logger.error(f\"Analysis failed: {analysis_results['error']}\")\n",
    "        else:\n",
    "            logger.info(\"Few-Shot Filtering Results:\")\n",
    "            logger.info(f\"  Total QA attempts: {analysis_results.get('total_qa_attempts', 0)}\")\n",
    "            logger.info(f\"  Final questions generated: {analysis_results.get('final_dataset_queries', 0)}\")\n",
    "            logger.info(f\"  Overall acceptance rate: {analysis_results.get('decision_distribution', {}).get('acceptance_rate', 0):.1f}%\")\n",
    "            logger.info(f\"  Average relevance score: {analysis_results.get('quality_statistics', {}).get('avg_relevance_score', 0):.3f}\")\n",
    "\n",
    "        # Generate comprehensive report\n",
    "        report_file = \"Few_Shot_QA_analysis_report.json\"\n",
    "        generate_few_shot_report(output_file, report_file, final_stats)\n",
    "        \n",
    "        # Display final statistics\n",
    "        logger.info(\"\\nFINAL Few-Shot Statistics:\")\n",
    "        logger.info(f\"Total API calls: {final_stats.get('total_api_calls', 0)}\")\n",
    "        logger.info(f\"Total QA attempts: {final_stats.get('total_qa_attempts', 0)}\")\n",
    "        logger.info(f\"Successful generations: {final_stats.get('successful_generations', 0)}\")\n",
    "\n",
    "        logger.info(\"\\nFiltering Decision Breakdown:\")\n",
    "        for decision_type, count in final_stats[\"filtering_decisions\"].items():\n",
    "            if count > 0:\n",
    "                percentage = (count / final_stats['total_qa_attempts']) * 100 if final_stats['total_qa_attempts'] > 0 else 0\n",
    "                logger.info(f\"  {decision_type}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        if \"acceptance_rate\" in final_stats:\n",
    "            logger.info(f\"\\nOverall Acceptance Rate: {final_stats['acceptance_rate']:.1f}%\")\n",
    "        \n",
    "        # Validation check\n",
    "        validation_status = final_stats.get(\"validation_status\", \"UNKNOWN\")\n",
    "        logger.info(f\"Statistics validation: {validation_status}\")\n",
    "        \n",
    "        logger.info(f\"\\nOutputs generated:\")\n",
    "        logger.info(f\"  Dataset: {output_file}\")\n",
    "        logger.info(f\"  Analysis Report: {report_file}\")\n",
    "        logger.info(\"\\nFew-Shot QA generation completed successfully!\")\n",
    "        logger.info(\"\\nFew-Shot Learning Features:\")\n",
    "        logger.info(f\"   ✓ Multi-exemplar guidance ({len(FEW_SHOT_EXEMPLARS)} diverse examples)\")\n",
    "        logger.info(\"   ✓ Pattern diversity learning from varied contexts\")\n",
    "        logger.info(\"   ✓ Enhanced structural variety in questions\")\n",
    "        logger.info(\"   ✓ Essential filtering: length + duplicate detection\")\n",
    "        logger.info(\"   ✓ Balanced coverage across question types\")\n",
    "        logger.info(\"   ✓ Robust domain-specific pattern learning\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"\\nProcess interrupted by user\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main execution: {str(e)}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34cfb7f-6d38-441a-9f20-9f3fd28859d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd714459-a6b8-4bc2-9641-bb87be0f8435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
