{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2af9e47-8170-4117-8f2f-ba4edd973187",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Zero-Shot QA Generator with Essential Quality Filtering \n",
    "\n",
    "This implementation uses a streamlined approach with only essential filters:\n",
    "- Length Requirements: Ensures meaningful questions and informative answers\n",
    "- Duplicate Detection: Prevents redundant questions using semantic similarity\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import time\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from collections import OrderedDict\n",
    "\n",
    "# OpenAI import fix with proper error handling\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    OPENAI_V1 = True\n",
    "    logger_msg = \"Using OpenAI v1.x API\"\n",
    "except ImportError:\n",
    "    try:\n",
    "        import openai\n",
    "        OPENAI_V1 = False\n",
    "        logger_msg = \"Using OpenAI legacy API\"\n",
    "    except ImportError:\n",
    "        print(\"ERROR: OpenAI library not installed. Run: pip install openai\")\n",
    "        sys.exit(1)\n",
    "\n",
    "# Configure environment and logging\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('basic_qa_generation.log'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Device configuration with proper torch import\n",
    "try:\n",
    "    import torch\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \n",
    "                         \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "except ImportError:\n",
    "    print(\"ERROR: PyTorch not installed. Run: pip install torch\")\n",
    "    sys.exit(1)\n",
    "\n",
    "logger.info(logger_msg)\n",
    "\n",
    "\n",
    "class QuestionType(Enum):\n",
    "    \"\"\"Enumeration of supported question types for zero-shot generation.\"\"\"\n",
    "    FACTUAL = \"factual\"\n",
    "    RELATIONSHIP = \"relationship\" \n",
    "    COMPARATIVE = \"comparative\"\n",
    "    INFERENTIAL = \"inferential\"\n",
    "\n",
    "\n",
    "class FilteringDecision(Enum):\n",
    "    \"\"\"Basic filtering decision outcomes.\"\"\"\n",
    "    ACCEPTED = \"accepted\"\n",
    "    REJECTED_LENGTH = \"rejected_length\"\n",
    "    REJECTED_DUPLICATE = \"rejected_duplicate\"\n",
    "    REJECTED_PARSING = \"rejected_parsing\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class QualityThresholds:\n",
    "    \"\"\"Basic quality thresholds for essential filtering only.\"\"\"\n",
    "    # Length requirements - ensures meaningful content\n",
    "    min_question_words: int = 8      # Ensures meaningful questions\n",
    "    min_answer_words: int = 20       # Ensures informative answers\n",
    "    \n",
    "    # Duplicate detection thresholds - more lenient for variety\n",
    "    duplicate_similarity_threshold: float = 0.85  \n",
    "    batch_similarity_threshold: float = 0.85     \n",
    "    \n",
    "    # Semantic similarity weights for relevance scoring (logging only)\n",
    "    context_weight: float = 0.6\n",
    "    triple_weight: float = 0.4\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FilteringResult:\n",
    "    \"\"\"Data class representing a basic filtering result with reasoning.\"\"\"\n",
    "    accepted: bool\n",
    "    decision: FilteringDecision\n",
    "    relevance_score: float  # For logging/analysis only\n",
    "    context_similarity: float\n",
    "    triple_similarity: float\n",
    "    reasoning: str\n",
    "    metadata: Dict[str, float]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class QAPair:\n",
    "    \"\"\"Data class representing a question-answer pair with complete metadata.\"\"\"\n",
    "    id: str\n",
    "    question: str\n",
    "    answer: str\n",
    "    question_type: QuestionType\n",
    "    qa_metadata: Dict[str, List[str]]\n",
    "    filtering_result: FilteringResult\n",
    "    generation_method: str = \"zero_shot\"\n",
    "    # Add ground truth information\n",
    "    source_context: str = \"\"\n",
    "    source_triples: List[Tuple] = None\n",
    "    chunk_id: str = \"\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.source_triples is None:\n",
    "            self.source_triples = []\n",
    "\n",
    "\n",
    "# Enhanced zero-shot question generation templates with diversity emphasis\n",
    "ZERO_SHOT_QUESTION_TEMPLATES = {\n",
    "    QuestionType.FACTUAL: {\n",
    "        \"prompt\": (\n",
    "            \"Generate {n} detailed factual questions that require **specific** information from the context. \"\n",
    "            \"These questions should ask for concrete details, numbers, dates, names, or specific requirements \"\n",
    "            \"mentioned in the text. Focus on extracting precise information that can be directly answered \"\n",
    "            \"from the provided context and knowledge graph triples. \"\n",
    "        \n",
    "        ),\n",
    "        \"task_description\": (\n",
    "            \"Factual questions seek specific, concrete information that can be directly extracted from \"\n",
    "            \"the text or inferred from the knowledge graph relationships. They typically start with \"\n",
    "            \"'What', 'When', 'Where', 'Who', or 'How much/many'.\"\n",
    "        )\n",
    "    },\n",
    "    \n",
    "    QuestionType.RELATIONSHIP: {\n",
    "        \"prompt\": (\n",
    "            \"Generate {n} questions about specific relationships or interactions between entities in the context. \"\n",
    "            \"Focus on how different entities, organizations, processes, or concepts connect, influence, or \"\n",
    "            \"interact with each other. Each question must reference at least two entities and explore \"\n",
    "            \"their connection through the knowledge graph relationships.\"\n",
    "        ),\n",
    "        \"task_description\": (\n",
    "            \"Relationship questions explore connections between entities. They examine how entities \"\n",
    "            \"interact, depend on each other, or influence one another through the relationships \"\n",
    "            \"defined in the knowledge graph.\"\n",
    "        )\n",
    "    },\n",
    "    \n",
    "    QuestionType.COMPARATIVE: {\n",
    "        \"prompt\": (\n",
    "            \"Generate {n} questions comparing different aspects, entities, or concepts from the context. \"\n",
    "            \"These questions should highlight differences, similarities, or contrasts between multiple \"\n",
    "            \"items such as funding types, requirements, processes, or organizational structures. \"\n",
    "            \"Use the knowledge graph to identify comparable entities.\"\n",
    "        ),\n",
    "        \"task_description\": (\n",
    "            \"Comparative questions examine differences and similarities between entities or concepts. \"\n",
    "            \"They help understand distinctions in requirements, processes, amounts, or characteristics \"\n",
    "            \"across different categories or instances.\"\n",
    "        )\n",
    "    },\n",
    "    \n",
    "    QuestionType.INFERENTIAL: {\n",
    "        \"prompt\": (\n",
    "            \"Generate {n} questions that require analysis, reasoning, or inference based on the context \"\n",
    "            \"and knowledge graph. These questions should combine multiple pieces of information to \"\n",
    "            \"draw conclusions, identify implications, or predict outcomes. They require synthesizing \"\n",
    "            \"information from multiple knowledge graph triples.\"\n",
    "        ),\n",
    "        \"task_description\": (\n",
    "            \"Inferential questions require reasoning and synthesis of multiple pieces of information. \"\n",
    "            \"They ask for conclusions, implications, or predictions that must be derived by combining \"\n",
    "            \"various facts and relationships from the context and knowledge graph.\"\n",
    "        )\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "class BasicQAGenerator:\n",
    "    \"\"\"\n",
    "    Basic Zero-Shot QA Generator with Essential Quality Filtering Only .\n",
    "    \n",
    "    Features:\n",
    "    - Length requirements for meaningful content\n",
    "    - Duplicate detection to prevent redundancy \n",
    "    - Basic relevance scoring for analysis (not filtering)\n",
    "    - Simplified decision logic with clear justification\n",
    "    - Centralized statistics tracking \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 openai_api_key: str,\n",
    "                 reset_duplicates_per_chunk: bool = True,\n",
    "                 embedding_model: str = 'BAAI/bge-large-en-v1.5',\n",
    "                 max_embedding_cache: int = 1000):\n",
    "        \"\"\"\n",
    "        Initialize the Basic QA Generator.\n",
    "        \n",
    "        Args:\n",
    "            openai_api_key: OpenAI API key for GPT-4 access\n",
    "            reset_duplicates_per_chunk: Whether to reset duplicate tracking per chunk\n",
    "            max_embedding_cache: Maximum number of embeddings to cache (memory management)\n",
    "        \"\"\"\n",
    "        if not openai_api_key or openai_api_key == \"your-openai-api-key-here\":\n",
    "            raise ValueError(\"Please provide a valid OpenAI API key\")\n",
    "            \n",
    "        self.openai_api_key = openai_api_key\n",
    "        self.max_embedding_cache = max_embedding_cache\n",
    "        \n",
    "        # Initialize OpenAI client based on version\n",
    "        if OPENAI_V1:\n",
    "            self.openai_client = OpenAI(api_key=openai_api_key)\n",
    "        else:\n",
    "            openai.api_key = openai_api_key\n",
    "            self.openai_client = None\n",
    "        \n",
    "        # Initialize quality thresholds\n",
    "        self.thresholds = QualityThresholds()\n",
    "        self.reset_duplicates_per_chunk = reset_duplicates_per_chunk\n",
    "        \n",
    "        # Initialize embedding model\n",
    "        logger.info(f\"Loading embedding model: {embedding_model}\")\n",
    "        try:\n",
    "            self.embedding_model = SentenceTransformer(embedding_model, device=device)\n",
    "            logger.info(f\"Successfully loaded {embedding_model}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading {embedding_model}: {e}\")\n",
    "            raise RuntimeError(f\"Failed to load the specified embedding model '{embedding_model}'. Please check your internet connection and ensure the model name is correct.\")\n",
    "        \n",
    "        # Use OrderedDict for memory management and better duplicate tracking\n",
    "        self.all_questions: set = set()\n",
    "        self.question_embeddings: OrderedDict[str, torch.Tensor] = OrderedDict()\n",
    "        self.batch_questions: set = set()\n",
    "        self.batch_embeddings: Dict[str, torch.Tensor] = {}  #  Separate batch tracking\n",
    "        \n",
    "        # Enhanced statistics tracking with detailed breakdown\n",
    "        self.generation_stats = {\n",
    "            \"total_api_calls\": 0,\n",
    "            \"total_qa_attempts\": 0,  # Total QA pairs that went through filtering\n",
    "            \"successful_generations\": 0,  # QA pairs that passed all filters\n",
    "            \"filtering_decisions\": {\n",
    "                \"accepted\": 0,\n",
    "                \"rejected_length\": 0,\n",
    "                \"rejected_duplicate\": 0,\n",
    "                \"rejected_parsing\": 0\n",
    "            },\n",
    "            #  Track all attempts for proper analysis\n",
    "            \"all_attempts\": []  # Store all filtering attempts for analysis\n",
    "        }\n",
    "        \n",
    "        logger.info(\"Zero-shot QA Generator initialized successfully\")\n",
    "        logger.info(\"Using only essential filters: length requirements + duplicate detection\")\n",
    "\n",
    "    def _filtering_result_to_dict(self, result: FilteringResult) -> Dict:\n",
    "        \"\"\"Convert FilteringResult to dictionary for JSON serialization.\"\"\"\n",
    "        return {\n",
    "            \"accepted\": result.accepted,\n",
    "            \"decision\": result.decision.value,\n",
    "            \"relevance_score\": result.relevance_score,\n",
    "            \"context_similarity\": result.context_similarity,\n",
    "            \"triple_similarity\": result.triple_similarity,\n",
    "            \"reasoning\": result.reasoning,\n",
    "            \"metadata\": result.metadata\n",
    "        }\n",
    "\n",
    "    def _manage_embedding_cache(self):\n",
    "        \"\"\"Manage embedding cache size to prevent memory issues.\"\"\"\n",
    "        if len(self.question_embeddings) > self.max_embedding_cache:\n",
    "            # Remove oldest embeddings (FIFO)\n",
    "            removed_count = len(self.question_embeddings) - self.max_embedding_cache\n",
    "            for _ in range(removed_count):\n",
    "                oldest_key = next(iter(self.question_embeddings))\n",
    "                del self.question_embeddings[oldest_key]\n",
    "                self.all_questions.discard(oldest_key)\n",
    "            logger.debug(f\"Removed {removed_count} old embeddings from cache\")\n",
    "\n",
    "    def calculate_relevance_score(self, question: str, answer: str, context: str, triples: List[Tuple]) -> Tuple[float, float, float]:\n",
    "        \"\"\"\n",
    "        Calculate semantic relevance score for logging/analysis purposes only.\n",
    "        This is NOT used for filtering decisions.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            qa_combined = f\"{question} {answer}\"\n",
    "            qa_embedding = self.embedding_model.encode(qa_combined, convert_to_tensor=True)\n",
    "            \n",
    "            # Context similarity\n",
    "            context_embedding = self.embedding_model.encode(context, convert_to_tensor=True)\n",
    "            context_similarity = util.cos_sim(context_embedding, qa_embedding).item()\n",
    "            \n",
    "            # Triple similarities\n",
    "            triple_similarities = []\n",
    "            for triple in triples:\n",
    "                try:\n",
    "                    if len(triple) >= 3:\n",
    "                        n1, edge, n2 = str(triple[0]), str(triple[1]), str(triple[2])\n",
    "                        triple_text = f\"{n1} {edge} {n2}\"\n",
    "                        triple_embedding = self.embedding_model.encode(triple_text, convert_to_tensor=True)\n",
    "                        triple_similarity = util.cos_sim(triple_embedding, qa_embedding).item()\n",
    "                        triple_similarities.append(triple_similarity)\n",
    "                except (IndexError, TypeError) as e:\n",
    "                    logger.debug(f\"Invalid triple format: {triple}, error: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            max_triple_similarity = max(triple_similarities) if triple_similarities else 0.0\n",
    "            \n",
    "            # Weighted overall relevance (for logging only)\n",
    "            overall_relevance = (context_similarity * self.thresholds.context_weight + \n",
    "                               max_triple_similarity * self.thresholds.triple_weight)\n",
    "            \n",
    "            return overall_relevance, context_similarity, max_triple_similarity\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error calculating relevance score: {e}\")\n",
    "            return 0.0, 0.0, 0.0\n",
    "\n",
    "    def passes_length_filter(self, question: str, answer: str) -> bool:\n",
    "        \"\"\"Check if QA pair meets minimum length requirements.\"\"\"\n",
    "        try:\n",
    "            q_words = len(question.split())\n",
    "            a_words = len(answer.split())\n",
    "            \n",
    "            if q_words < self.thresholds.min_question_words or a_words < self.thresholds.min_answer_words:\n",
    "                logger.debug(f\"Length filter failed: Q={q_words} words (min {self.thresholds.min_question_words}), \"\n",
    "                            f\"A={a_words} words (min {self.thresholds.min_answer_words})\")\n",
    "                return False\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error in length filter: {e}\")\n",
    "            return False\n",
    "\n",
    "    def is_semantic_duplicate(self, new_question: str) -> Tuple[bool, torch.Tensor]:\n",
    "        \"\"\"\n",
    "         Check if question is a semantic duplicate and return embedding.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (is_duplicate, question_embedding)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            new_embedding = self.embedding_model.encode(new_question, convert_to_tensor=True)\n",
    "            \n",
    "            # Check against historical questions\n",
    "            for existing_question, existing_embedding in self.question_embeddings.items():\n",
    "                similarity = util.cos_sim(new_embedding, existing_embedding).item()\n",
    "                if similarity > self.thresholds.duplicate_similarity_threshold:\n",
    "                    logger.debug(f\"Historical duplicate detected: '{new_question}' ~ '{existing_question}' \"\n",
    "                               f\"(similarity={similarity:.3f})\")\n",
    "                    return True, new_embedding\n",
    "            \n",
    "            # Check against current batch using stored embeddings\n",
    "            for batch_question, batch_embedding in self.batch_embeddings.items():\n",
    "                similarity = util.cos_sim(new_embedding, batch_embedding).item()\n",
    "                if similarity > self.thresholds.batch_similarity_threshold:\n",
    "                    logger.debug(f\"Batch duplicate detected: '{new_question}' ~ '{batch_question}' \"\n",
    "                               f\"(similarity={similarity:.3f})\")\n",
    "                    return True, new_embedding\n",
    "            \n",
    "            return False, new_embedding\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error in duplicate detection: {e}\")\n",
    "            # Return False with dummy embedding on error\n",
    "            dummy_embedding = torch.zeros(384)  # Default embedding size\n",
    "            return False, dummy_embedding\n",
    "\n",
    "    def filter_qa_pair(self, \n",
    "                      question: str, \n",
    "                      answer: str, \n",
    "                      context: str, \n",
    "                      triples: List[Tuple],\n",
    "                      qa_metadata: Dict) -> Tuple[bool, FilteringResult]:\n",
    "        \"\"\"\n",
    "         Apply basic filtering pipeline to QA pair with centralized statistics tracking.\n",
    "        \n",
    "        Args:\n",
    "            question: Question text\n",
    "            answer: Answer text\n",
    "            context: Original text chunk\n",
    "            triples: Knowledge graph triples\n",
    "            qa_metadata: QA metadata including entities and relationships\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (acceptance_decision, filtering_result_details)\n",
    "        \"\"\"\n",
    "        #  Centralized statistics tracking - increment total attempts here\n",
    "        self.generation_stats[\"total_qa_attempts\"] += 1\n",
    "        \n",
    "        # Calculate relevance score first (for all attempts)\n",
    "        relevance_score, context_sim, triple_sim = self.calculate_relevance_score(\n",
    "            question, answer, context, triples\n",
    "        )\n",
    "        \n",
    "        # Create base metadata\n",
    "        base_metadata = {\n",
    "            \"answer_length\": len(answer.split()),\n",
    "            \"question_length\": len(question.split()),\n",
    "            \"mentioned_entities\": len(qa_metadata.get(\"mentioned_entities\", [])),\n",
    "            \"mentioned_relationships\": len(qa_metadata.get(\"mentioned_relationships\", []))\n",
    "        }\n",
    "        \n",
    "        # Filter 1: Length requirements\n",
    "        if not self.passes_length_filter(question, answer):\n",
    "            result = FilteringResult(\n",
    "                accepted=False,\n",
    "                decision=FilteringDecision.REJECTED_LENGTH,\n",
    "                relevance_score=relevance_score,\n",
    "                context_similarity=context_sim,\n",
    "                triple_similarity=triple_sim,\n",
    "                reasoning=f\"Failed length requirements: Q={len(question.split())} words, A={len(answer.split())} words\",\n",
    "                metadata=base_metadata\n",
    "            )\n",
    "            \n",
    "            #  statistics\n",
    "            self.generation_stats[\"filtering_decisions\"][\"rejected_length\"] += 1\n",
    "            \n",
    "            # Store attempt for analysis (convert to dict for JSON serialization)\n",
    "            self.generation_stats[\"all_attempts\"].append({\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"filtering_result\": self._filtering_result_to_dict(result),\n",
    "                \"relevance_score\": relevance_score\n",
    "            })\n",
    "            \n",
    "            return False, result\n",
    "        \n",
    "        # Filter 2: Duplicate detection \n",
    "        is_duplicate, question_embedding = self.is_semantic_duplicate(question)\n",
    "        if is_duplicate:\n",
    "            result = FilteringResult(\n",
    "                accepted=False,\n",
    "                decision=FilteringDecision.REJECTED_DUPLICATE,\n",
    "                relevance_score=relevance_score,\n",
    "                context_similarity=context_sim,\n",
    "                triple_similarity=triple_sim,\n",
    "                reasoning=\"Semantic duplicate detected\",\n",
    "                metadata=base_metadata\n",
    "            )\n",
    "            \n",
    "            # Update statistics\n",
    "            self.generation_stats[\"filtering_decisions\"][\"rejected_duplicate\"] += 1\n",
    "            \n",
    "            # Store attempt for analysis (convert to dict for JSON serialization)\n",
    "            self.generation_stats[\"all_attempts\"].append({\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"filtering_result\": self._filtering_result_to_dict(result),\n",
    "                \"relevance_score\": relevance_score\n",
    "            })\n",
    "            \n",
    "            return False, result\n",
    "        \n",
    "        # Accept QA pair - passed all filters\n",
    "        result = FilteringResult(\n",
    "            accepted=True,\n",
    "            decision=FilteringDecision.ACCEPTED,\n",
    "            relevance_score=relevance_score,\n",
    "            context_similarity=context_sim,\n",
    "            triple_similarity=triple_sim,\n",
    "            reasoning=f\"Passed basic quality filters (relevance: {relevance_score:.3f})\",\n",
    "            metadata=base_metadata\n",
    "        )\n",
    "        \n",
    "        # Register accepted question with proper embedding storage\n",
    "        self.all_questions.add(question)\n",
    "        self.question_embeddings[question] = question_embedding\n",
    "        self.batch_questions.add(question)\n",
    "        self.batch_embeddings[question] = question_embedding  #  Store in batch embeddings too\n",
    "        \n",
    "        # Manage cache size\n",
    "        self._manage_embedding_cache()\n",
    "        \n",
    "        #  Update statistics\n",
    "        self.generation_stats[\"successful_generations\"] += 1\n",
    "        self.generation_stats[\"filtering_decisions\"][\"accepted\"] += 1\n",
    "        \n",
    "        # Store attempt for analysis (convert to dict for JSON serialization)\n",
    "        self.generation_stats[\"all_attempts\"].append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"filtering_result\": self._filtering_result_to_dict(result),\n",
    "            \"relevance_score\": relevance_score\n",
    "        })\n",
    "        \n",
    "        return True, result\n",
    "\n",
    "    def load_and_merge_data(self, chunks_file: str, triples_file: str) -> Dict:\n",
    "        \"\"\"Load and merge chunks and triples data.\"\"\"\n",
    "        try:\n",
    "            # Load chunks data\n",
    "            chunks_df = pd.read_csv(chunks_file)\n",
    "            logger.info(f\"Loaded {len(chunks_df)} chunks from {chunks_file}\")\n",
    "            logger.info(f\"Chunks columns: {list(chunks_df.columns)}\")\n",
    "            \n",
    "            # Load triples data\n",
    "            triples_df = pd.read_csv(triples_file)\n",
    "            logger.info(f\"Loaded {len(triples_df)} triples from {triples_file}\")\n",
    "            logger.info(f\"Triples columns: {list(triples_df.columns)}\")\n",
    "            \n",
    "            # Check for required columns\n",
    "            if 'chunk_id' not in chunks_df.columns:\n",
    "                raise ValueError(\"chunks_file must contain 'chunk_id' column\")\n",
    "            if 'text' not in chunks_df.columns:\n",
    "                raise ValueError(\"chunks_file must contain 'text' column\")\n",
    "            if 'chunk_id' not in triples_df.columns:\n",
    "                raise ValueError(\"triples_file must contain 'chunk_id' column\")\n",
    "            \n",
    "            # Try to identify triple columns\n",
    "            triple_columns = None\n",
    "            possible_combinations = [\n",
    "                ('subject', 'predicate', 'object'),\n",
    "                ('node1', 'edge', 'node2'),\n",
    "                ('head', 'relation', 'tail'),\n",
    "                ('entity1', 'relationship', 'entity2')\n",
    "            ]\n",
    "            \n",
    "            for combo in possible_combinations:\n",
    "                if all(col in triples_df.columns for col in combo):\n",
    "                    triple_columns = combo\n",
    "                    break\n",
    "            \n",
    "            if triple_columns is None:\n",
    "                available_cols = list(triples_df.columns)\n",
    "                raise ValueError(f\"Could not identify triple columns. Available columns: {available_cols}. \"\n",
    "                               f\"Expected one of: {possible_combinations}\")\n",
    "            \n",
    "            logger.info(f\"Using triple columns: {triple_columns}\")\n",
    "            \n",
    "            # Merge on chunk_id\n",
    "            merged_df = pd.merge(chunks_df, triples_df, on='chunk_id', how='inner')\n",
    "            logger.info(f\"Merged data contains {len(merged_df)} records\")\n",
    "            \n",
    "            if len(merged_df) == 0:\n",
    "                raise ValueError(\"No matching chunk_ids found between chunks and triples files\")\n",
    "            \n",
    "            # Group by chunk_id\n",
    "            grouped_data = {}\n",
    "            for chunk_id, group in merged_df.groupby('chunk_id'):\n",
    "                grouped_data[chunk_id] = {\n",
    "                    'text': group['text'].iloc[0],\n",
    "                    'triples': [(row[triple_columns[0]], row[triple_columns[1]], row[triple_columns[2]]) \n",
    "                              for _, row in group.iterrows()]\n",
    "                }\n",
    "            \n",
    "            logger.info(f\"Grouped data contains {len(grouped_data)} unique chunks\")\n",
    "            return grouped_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading and merging data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_zero_shot_prompt(self, context: str, triples: List[Tuple], question_type: QuestionType, num_questions: int) -> str:\n",
    "        \"\"\"Generate zero-shot prompt for GPT-4 with diversity emphasis.\"\"\"\n",
    "        template = ZERO_SHOT_QUESTION_TEMPLATES.get(question_type, {})\n",
    "        base_prompt_text = template.get(\"prompt\", \"Generate {n} questions.\").format(n=num_questions)\n",
    "        task_description = template.get(\"task_description\", \"\")\n",
    "        \n",
    "        formatted_triples = []\n",
    "        for triple in triples[:20]:  # Limit to prevent prompt overflow\n",
    "            try:\n",
    "                if len(triple) >= 3:\n",
    "                    n1, e, n2 = str(triple[0]), str(triple[1]), str(triple[2])\n",
    "                    formatted_triples.append(f\"- {n1} → '{e}' → {n2}\")\n",
    "            except (IndexError, TypeError):\n",
    "                continue\n",
    "        \n",
    "        triple_text = \"\\n\".join(formatted_triples) if formatted_triples else \"No valid triples available\"\n",
    "\n",
    "        # Add diversity instructions\n",
    "        diversity_instructions = f\"\"\"\n",
    "DIVERSITY REQUIREMENTS:\n",
    "- Each question must be UNIQUE and ask about DIFFERENT aspects\n",
    "- Use VARIED question starters and phrasing patterns\n",
    "- Focus on DIFFERENT entities, relationships, or information types\n",
    "- Avoid repetitive structures or similar wordings\n",
    "- Make each question distinctly different from others\n",
    "\"\"\"\n",
    "\n",
    "        json_format_instructions = f\"\"\"\n",
    "REQUIRED OUTPUT FORMAT:\n",
    "[\n",
    "  {{\n",
    "    \"id\": \"1\",\n",
    "    \"question\": \"Your detailed question here?\",\n",
    "    \"answer\": \"Your comprehensive answer here.\",\n",
    "    \"type\": \"{question_type.value}\",\n",
    "    \"qa_metadata\": {{\n",
    "      \"mentioned_entities\": [\"entity1\", \"entity2\"],\n",
    "      \"mentioned_relationships\": [\"relationship1\", \"relationship2\"]\n",
    "    }}\n",
    "  }}\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "ZERO-SHOT QA GENERATION TASK\n",
    "\n",
    "TASK TYPE: {question_type.value.upper()}\n",
    "{task_description}\n",
    "\n",
    "{diversity_instructions}\n",
    "\n",
    "INSTRUCTIONS: {base_prompt_text}\n",
    "\n",
    "CONTEXT TEXT:\n",
    "{context[:1200]}{\"...\" if len(context) > 1200 else \"\"}\n",
    "\n",
    "ONTOLOGY-GUIDED KNOWLEDGE GRAPH TRIPLES:\n",
    "{triple_text}\n",
    "\n",
    "{json_format_instructions}\n",
    "\n",
    "Generate exactly {num_questions} DIVERSE, high-quality {question_type.value} questions.\n",
    "\"\"\"\n",
    "        return prompt.strip()\n",
    "    \n",
    "    def parse_json_response(self, text_response: str) -> List[Dict]:\n",
    "        \"\"\"Parse JSON response from GPT-4 with improved error handling.\"\"\"\n",
    "        # Remove common markdown formatting that can break JSON parsing\n",
    "        text_response = text_response.strip()\n",
    "        if text_response.startswith('```json'):\n",
    "            text_response = text_response[7:]  # Remove ```json\n",
    "        if text_response.endswith('```'):\n",
    "            text_response = text_response[:-3]  # Remove trailing ```\n",
    "        text_response = text_response.strip()\n",
    "        \n",
    "        try:\n",
    "            parsed = json.loads(text_response)\n",
    "            if isinstance(parsed, list):\n",
    "                return parsed\n",
    "            elif isinstance(parsed, dict) and \"question\" in parsed:\n",
    "                return [parsed]\n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.debug(f\"Initial JSON parse failed: {e}\")\n",
    "        \n",
    "        # Fallback parsing strategies\n",
    "        try:\n",
    "            json_start = text_response.find('[')\n",
    "            json_end = text_response.rfind(']') + 1\n",
    "            if json_start >= 0 and json_end > json_start:\n",
    "                json_str = text_response[json_start:json_end]\n",
    "                parsed = json.loads(json_str)\n",
    "                if isinstance(parsed, list):\n",
    "                    return parsed\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Array extraction failed: {e}\")\n",
    "        \n",
    "        try:\n",
    "            json_start = text_response.find('{')\n",
    "            json_end = text_response.rfind('}') + 1\n",
    "            if json_start >= 0 and json_end > json_start:\n",
    "                json_str = text_response[json_start:json_end]\n",
    "                parsed = json.loads(json_str)\n",
    "                if isinstance(parsed, dict) and \"question\" in parsed:\n",
    "                    return [parsed]\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Object extraction failed: {e}\")\n",
    "        \n",
    "        logger.warning(f\"Failed to parse JSON response: {text_response[:200]}...\")\n",
    "        return []\n",
    "\n",
    "    def call_openai_api(self, system_message: str, user_message: str, max_tokens: int = 4000, temperature: float = 0.7) -> str:\n",
    "        \"\"\"Call OpenAI API with proper version handling and temperature bounds.\"\"\"\n",
    "        try:\n",
    "            #  Ensure temperature is within valid bounds\n",
    "            temperature = min(max(temperature, 0.0), 1.0)\n",
    "            \n",
    "            if OPENAI_V1:\n",
    "                response = self.openai_client.chat.completions.create(\n",
    "                    model=\"gpt-4-turbo-preview\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_message},\n",
    "                        {\"role\": \"user\", \"content\": user_message}\n",
    "                    ],\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=temperature,\n",
    "                    presence_penalty=0.3,\n",
    "                    frequency_penalty=0.3\n",
    "                )\n",
    "                return response.choices[0].message.content.strip()\n",
    "            else:\n",
    "                response = openai.ChatCompletion.create(\n",
    "                    model=\"gpt-4-turbo-preview\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_message},\n",
    "                        {\"role\": \"user\", \"content\": user_message}\n",
    "                    ],\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=temperature,\n",
    "                    presence_penalty=0.3,\n",
    "                    frequency_penalty=0.3\n",
    "                )\n",
    "                return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"OpenAI API call failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_qa_pairs(self, \n",
    "                         context: str, \n",
    "                         triples: List[Tuple], \n",
    "                         question_type: QuestionType, \n",
    "                         num_questions: int,\n",
    "                         chunk_id: str) -> List[QAPair]:\n",
    "        \"\"\"Generate QA pairs using basic filtering system - \"\"\"\n",
    "        qa_pairs = []\n",
    "        max_retries = 3\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Count API calls (separate from QA attempts)\n",
    "                self.generation_stats[\"total_api_calls\"] += 1\n",
    "                \n",
    "                prompt = self.generate_zero_shot_prompt(context, triples, question_type, num_questions)\n",
    "                \n",
    "                system_message = \"\"\"You are an expert at generating domain-specific QA pairs using only task instructions and ontology-guided knowledge graphs. \n",
    "\n",
    "Your output MUST be valid JSON in the exact format specified. Generate questions that demonstrate understanding of the domain relationships and entities present in the provided knowledge graph.\n",
    "\n",
    "IMPORTANT: In the qa_metadata field, accurately list the specific entities and relationships that you mention in your question and answer.\"\"\"\n",
    "\n",
    "                logger.debug(f\"Generating {num_questions} {question_type.value} questions (attempt {attempt + 1})\")\n",
    "                \n",
    "                #  Temperature progression with bounds\n",
    "                temperature = min(0.8 + (attempt * 0.1), 1.0)\n",
    "                \n",
    "                text_response = self.call_openai_api(\n",
    "                    system_message, \n",
    "                    prompt, \n",
    "                    temperature=temperature\n",
    "                )\n",
    "                \n",
    "                parsed_items = self.parse_json_response(text_response)\n",
    "                \n",
    "                if parsed_items:\n",
    "                    logger.info(f\"Successfully parsed {len(parsed_items)} QA pairs from zero-shot response\")\n",
    "                    \n",
    "                    for item in parsed_items:\n",
    "                        q_text = item.get(\"question\", \"\").strip()\n",
    "                        a_text = item.get(\"answer\", \"\").strip()\n",
    "                        qa_metadata = item.get(\"qa_metadata\", {\n",
    "                            \"mentioned_entities\": [],\n",
    "                            \"mentioned_relationships\": []\n",
    "                        })\n",
    "\n",
    "                        if q_text and a_text:\n",
    "                            # Apply filtering (statistics are handled inside filter_qa_pair)\n",
    "                            accepted, filtering_result = self.filter_qa_pair(\n",
    "                                q_text, a_text, context, triples, qa_metadata\n",
    "                            )\n",
    "                            \n",
    "                            if accepted:\n",
    "                                unique_id = f\"{question_type.value}_basic_{chunk_id}_{int(time.time())}_{str(uuid.uuid4())[:8]}\"\n",
    "                                qa_pair = QAPair(\n",
    "                                    id=unique_id,\n",
    "                                    question=q_text,\n",
    "                                    answer=a_text,\n",
    "                                    question_type=question_type,\n",
    "                                    qa_metadata=qa_metadata,\n",
    "                                    filtering_result=filtering_result,\n",
    "                                    generation_method=\"zero_shot\",\n",
    "                                    source_context=context,\n",
    "                                    source_triples=triples.copy(),\n",
    "                                    chunk_id=chunk_id\n",
    "                                )\n",
    "                                \n",
    "                                qa_pairs.append(qa_pair)\n",
    "                                \n",
    "                                logger.debug(f\"Accepted QA pair: {filtering_result.decision.value} \"\n",
    "                                           f\"(relevance={filtering_result.relevance_score:.3f})\")\n",
    "                                \n",
    "                                if len(qa_pairs) >= num_questions:\n",
    "                                    return qa_pairs[:num_questions]\n",
    "                            else:\n",
    "                                logger.debug(f\"Rejected QA pair: {filtering_result.reasoning}\")\n",
    "                        else:\n",
    "                            # Handle parsing failures - empty question or answer\n",
    "                            self.generation_stats[\"total_qa_attempts\"] += 1\n",
    "                            self.generation_stats[\"filtering_decisions\"][\"rejected_parsing\"] += 1\n",
    "                            \n",
    "                            # Store failed parsing attempt\n",
    "                            dummy_result = FilteringResult(\n",
    "                                accepted=False,\n",
    "                                decision=FilteringDecision.REJECTED_PARSING,\n",
    "                                relevance_score=0.0,\n",
    "                                context_similarity=0.0,\n",
    "                                triple_similarity=0.0,\n",
    "                                reasoning=\"Empty question or answer from parsing\",\n",
    "                                metadata={}\n",
    "                            )\n",
    "                            \n",
    "                            self.generation_stats[\"all_attempts\"].append({\n",
    "                                \"question\": q_text,\n",
    "                                \"answer\": a_text,\n",
    "                                \"filtering_result\": dummy_result,\n",
    "                                \"relevance_score\": 0.0\n",
    "                            })\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in generation attempt {attempt + 1}: {str(e)}\")\n",
    "                time.sleep(min(2 ** attempt, 10))  # Exponential backoff with cap\n",
    "                continue\n",
    "\n",
    "        return qa_pairs\n",
    "\n",
    "    def create_zero_shot_dataset(self, \n",
    "                               merged_grouped: Dict, \n",
    "                               output_file: str, \n",
    "                               limit: Optional[int] = None) -> None:\n",
    "        \"\"\"Create a complete zero-shot QA dataset with basic filtering.\"\"\"\n",
    "        logger.info(\"Starting zero-shot QA dataset creation with filtering\")\n",
    "        \n",
    "        all_qa_pairs = []\n",
    "        question_types = list(QuestionType)\n",
    "        \n",
    "        # Process chunks\n",
    "        chunks_to_process = list(merged_grouped.items())\n",
    "        if limit:\n",
    "            chunks_to_process = chunks_to_process[:limit]\n",
    "            \n",
    "        total_chunks = len(chunks_to_process)\n",
    "        logger.info(f\"Processing {total_chunks} chunks\")\n",
    "        \n",
    "        for i, (chunk_id, chunk_data) in enumerate(chunks_to_process, 1):\n",
    "            logger.info(f\"Processing chunk {i}/{total_chunks}: {chunk_id}\")\n",
    "            \n",
    "            # Reset batch tracking for each chunk if specified\n",
    "            if self.reset_duplicates_per_chunk:\n",
    "                self.batch_questions.clear()\n",
    "                self.batch_embeddings.clear()  # Clear batch embeddings too\n",
    "            \n",
    "            context = chunk_data['text']\n",
    "            triples = chunk_data['triples']\n",
    "            \n",
    "            if not triples:\n",
    "                logger.warning(f\"No triples found for chunk {chunk_id}, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Generate QA pairs for each question type\n",
    "            for question_type in question_types:\n",
    "                try:\n",
    "                    logger.debug(f\"Generating {question_type.value} questions for chunk {chunk_id}\")\n",
    "                    \n",
    "                    qa_pairs = self.generate_qa_pairs(\n",
    "                        context=context,\n",
    "                        triples=triples,\n",
    "                        question_type=question_type,\n",
    "                        num_questions=2,\n",
    "                        chunk_id=chunk_id\n",
    "                    )\n",
    "                    \n",
    "                    all_qa_pairs.extend(qa_pairs)\n",
    "                    logger.info(f\"Generated {len(qa_pairs)} {question_type.value} QA pairs for chunk {chunk_id}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error generating {question_type.value} questions for chunk {chunk_id}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Add delay between chunks to avoid rate limiting\n",
    "            if i < total_chunks:\n",
    "                time.sleep(2)\n",
    "        \n",
    "        # Get embedding model name safely\n",
    "        try:\n",
    "            if hasattr(self.embedding_model, 'model_name'):\n",
    "                embedding_model_name = self.embedding_model.model_name\n",
    "            elif hasattr(self.embedding_model, '_model_name'):\n",
    "                embedding_model_name = self.embedding_model._model_name\n",
    "            elif hasattr(self.embedding_model, 'config') and hasattr(self.embedding_model.config, 'name_or_path'):\n",
    "                embedding_model_name = self.embedding_model.config.name_or_path\n",
    "            else:\n",
    "                try:\n",
    "                    embedding_model_name = self.embedding_model._modules['0'].auto_model.config.name_or_path\n",
    "                except:\n",
    "                    embedding_model_name = 'BAAI/bge-large-en-v1.5'\n",
    "        except:\n",
    "            embedding_model_name = 'BAAI/bge-large-en-v1.5'\n",
    "        \n",
    "        # Calculate question type statistics\n",
    "        question_type_stats = {}\n",
    "        question_type_quality = {}\n",
    "        \n",
    "        for qa_pair in all_qa_pairs:\n",
    "            q_type = qa_pair.question_type.value\n",
    "            \n",
    "            # Count by type\n",
    "            if q_type not in question_type_stats:\n",
    "                question_type_stats[q_type] = {\n",
    "                    \"count\": 0,\n",
    "                    \"avg_relevance_score\": 0.0,\n",
    "                    \"avg_question_length\": 0.0,\n",
    "                    \"avg_answer_length\": 0.0,\n",
    "                    \"avg_context_similarity\": 0.0,\n",
    "                    \"avg_triple_similarity\": 0.0\n",
    "                }\n",
    "                question_type_quality[q_type] = {\n",
    "                    \"relevance_scores\": [],\n",
    "                    \"question_lengths\": [],\n",
    "                    \"answer_lengths\": [],\n",
    "                    \"context_similarities\": [],\n",
    "                    \"triple_similarities\": []\n",
    "                }\n",
    "            \n",
    "            question_type_stats[q_type][\"count\"] += 1\n",
    "            \n",
    "            # Collect quality metrics\n",
    "            question_type_quality[q_type][\"relevance_scores\"].append(qa_pair.filtering_result.relevance_score)\n",
    "            question_type_quality[q_type][\"question_lengths\"].append(len(qa_pair.question.split()))\n",
    "            question_type_quality[q_type][\"answer_lengths\"].append(len(qa_pair.answer.split()))\n",
    "            question_type_quality[q_type][\"context_similarities\"].append(qa_pair.filtering_result.context_similarity)\n",
    "            question_type_quality[q_type][\"triple_similarities\"].append(qa_pair.filtering_result.triple_similarity)\n",
    "        \n",
    "        # Calculate averages for each question type\n",
    "        for q_type in question_type_stats:\n",
    "            metrics = question_type_quality[q_type]\n",
    "            question_type_stats[q_type][\"avg_relevance_score\"] = np.mean(metrics[\"relevance_scores\"]) if metrics[\"relevance_scores\"] else 0.0\n",
    "            question_type_stats[q_type][\"avg_question_length\"] = np.mean(metrics[\"question_lengths\"]) if metrics[\"question_lengths\"] else 0.0\n",
    "            question_type_stats[q_type][\"avg_answer_length\"] = np.mean(metrics[\"answer_lengths\"]) if metrics[\"answer_lengths\"] else 0.0\n",
    "            question_type_stats[q_type][\"avg_context_similarity\"] = np.mean(metrics[\"context_similarities\"]) if metrics[\"context_similarities\"] else 0.0\n",
    "            question_type_stats[q_type][\"avg_triple_similarity\"] = np.mean(metrics[\"triple_similarities\"]) if metrics[\"triple_similarities\"] else 0.0\n",
    "\n",
    "        # Create final dataset structure with enhanced metadata\n",
    "        dataset = {\n",
    "            \"metadata\": {\n",
    "                \"creation_date\": pd.Timestamp.now().isoformat(),\n",
    "                \"total_queries\": len(all_qa_pairs),\n",
    "                \"generation_method\": \"zero_shot\",\n",
    "                \"model_version\": \"1.0\",\n",
    "                \"embedding_model\": embedding_model_name,\n",
    "                \"filtering_approach\": \"essential_quality_filters\",\n",
    "                \"quality_thresholds\": {\n",
    "                    \"min_question_words\": self.thresholds.min_question_words,\n",
    "                    \"min_answer_words\": self.thresholds.min_answer_words,\n",
    "                    \"duplicate_similarity_threshold\": self.thresholds.duplicate_similarity_threshold,\n",
    "                    \"batch_similarity_threshold\": self.thresholds.batch_similarity_threshold\n",
    "                },\n",
    "                \"generation_statistics\": self.get_filtering_statistics(),\n",
    "                \"question_type_breakdown\": question_type_stats,\n",
    "                \"processing_summary\": {\n",
    "                    \"chunks_processed\": total_chunks,\n",
    "                    \"questions_per_chunk\": 2 * len(question_types),  # 2 per question type\n",
    "                    \"question_types_generated\": list(question_type_stats.keys()),\n",
    "                    \"overall_quality\": {\n",
    "                        \"avg_relevance_score\": np.mean([qa.filtering_result.relevance_score for qa in all_qa_pairs]) if all_qa_pairs else 0.0,\n",
    "                        \"avg_question_length\": np.mean([len(qa.question.split()) for qa in all_qa_pairs]) if all_qa_pairs else 0.0,\n",
    "                        \"avg_answer_length\": np.mean([len(qa.answer.split()) for qa in all_qa_pairs]) if all_qa_pairs else 0.0,\n",
    "                        \"avg_context_similarity\": np.mean([qa.filtering_result.context_similarity for qa in all_qa_pairs]) if all_qa_pairs else 0.0,\n",
    "                        \"avg_triple_similarity\": np.mean([qa.filtering_result.triple_similarity for qa in all_qa_pairs]) if all_qa_pairs else 0.0\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"queries\": []\n",
    "        }\n",
    "        \n",
    "        # Convert QA pairs to dictionary format\n",
    "        for qa_pair in all_qa_pairs:\n",
    "            query_dict = {\n",
    "                \"id\": qa_pair.id,\n",
    "                \"question\": qa_pair.question,\n",
    "                \"answer\": qa_pair.answer,\n",
    "                \"question_type\": qa_pair.question_type.value,\n",
    "                \"qa_metadata\": qa_pair.qa_metadata,\n",
    "                \"filtering_result\": {\n",
    "                    \"accepted\": qa_pair.filtering_result.accepted,\n",
    "                    \"decision\": qa_pair.filtering_result.decision.value,\n",
    "                    \"relevance_score\": qa_pair.filtering_result.relevance_score,\n",
    "                    \"context_similarity\": qa_pair.filtering_result.context_similarity,\n",
    "                    \"triple_similarity\": qa_pair.filtering_result.triple_similarity,\n",
    "                    \"reasoning\": qa_pair.filtering_result.reasoning,\n",
    "                    \"metadata\": qa_pair.filtering_result.metadata\n",
    "                },\n",
    "                \"generation_method\": qa_pair.generation_method,\n",
    "                # Add ground truth information for evaluation\n",
    "                \"ground_truth\": {\n",
    "                    \"source_context\": qa_pair.source_context,\n",
    "                    \"source_triples\": [\n",
    "                        {\n",
    "                            \"subject\": triple[0],\n",
    "                            \"predicate\": triple[1], \n",
    "                            \"object\": triple[2]\n",
    "                        } for triple in qa_pair.source_triples if len(triple) >= 3\n",
    "                    ],\n",
    "                    \"chunk_id\": qa_pair.chunk_id,\n",
    "                    \"context_length_chars\": len(qa_pair.source_context),\n",
    "                    \"num_source_triples\": len(qa_pair.source_triples)\n",
    "                }\n",
    "            }\n",
    "            dataset[\"queries\"].append(query_dict)\n",
    "        \n",
    "        # Save dataset\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(dataset, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        logger.info(f\"Dataset creation completed!\")\n",
    "        logger.info(f\"Total QA pairs generated: {len(all_qa_pairs)}\")\n",
    "        logger.info(f\"Dataset saved to: {output_file}\")\n",
    "        \n",
    "        # Log detailed statistics\n",
    "        stats = self.get_filtering_statistics()\n",
    "        logger.info(\"\\ Basic Filtering Statistics:\")\n",
    "        logger.info(f\"Total API calls: {stats.get('total_api_calls', 0)}\")\n",
    "        logger.info(f\"Total QA attempts: {stats.get('total_qa_attempts', 0)}\")\n",
    "        logger.info(f\"Successful generations: {stats.get('successful_generations', 0)}\")\n",
    "        \n",
    "        logger.info(\"\\nFiltering Decision Breakdown:\")\n",
    "        for decision_type, count in stats[\"filtering_decisions\"].items():\n",
    "            if count > 0:\n",
    "                percentage = (count / stats['total_qa_attempts']) * 100 if stats['total_qa_attempts'] > 0 else 0\n",
    "                logger.info(f\"  {decision_type}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        if \"acceptance_rate\" in stats:\n",
    "            logger.info(f\"\\nOverall Acceptance Rate: {stats['acceptance_rate']:.1f}%\")\n",
    "        \n",
    "        # Log question type breakdown\n",
    "        logger.info(\"\\nQuestion Type Breakdown:\")\n",
    "        for q_type, type_stats in question_type_stats.items():\n",
    "            count = type_stats[\"count\"]\n",
    "            avg_relevance = type_stats[\"avg_relevance_score\"]\n",
    "            percentage = (count / len(all_qa_pairs)) * 100 if all_qa_pairs else 0\n",
    "            logger.info(f\"  {q_type}: {count} questions ({percentage:.1f}%) - avg relevance: {avg_relevance:.3f}\")\n",
    "        \n",
    "        # Validation check\n",
    "        total_decisions = sum(stats[\"filtering_decisions\"].values())\n",
    "        if total_decisions == stats[\"total_qa_attempts\"]:\n",
    "            logger.info(\"\\n Statistics validation: PASSED\")\n",
    "        else:\n",
    "            logger.warning(f\"\\n Statistics validation: FAILED ({total_decisions} decisions vs {stats['total_qa_attempts']} attempts)\")\n",
    "\n",
    "    def get_filtering_statistics(self) -> Dict:\n",
    "        \"\"\"Get comprehensive statistics about filtering decisions.\"\"\"\n",
    "        stats = self.generation_stats.copy()\n",
    "        \n",
    "        # Calculate acceptance rate based on QA pairs processed\n",
    "        total_qa_attempts = stats[\"total_qa_attempts\"]\n",
    "        if total_qa_attempts > 0:\n",
    "            acceptance_rate = (stats[\"successful_generations\"] / total_qa_attempts) * 100\n",
    "            stats[\"acceptance_rate\"] = acceptance_rate\n",
    "        else:\n",
    "            stats[\"acceptance_rate\"] = 0.0\n",
    "        \n",
    "        # Add validation check\n",
    "        total_decisions = sum(stats[\"filtering_decisions\"].values())\n",
    "        if total_decisions != total_qa_attempts:\n",
    "            logger.warning(f\"Statistics mismatch: {total_decisions} decisions vs {total_qa_attempts} attempts\")\n",
    "            stats[\"validation_status\"] = \"FAILED\"\n",
    "        else:\n",
    "            stats[\"validation_status\"] = \"PASSED\"\n",
    "        \n",
    "        return stats\n",
    "\n",
    "\n",
    "#  Analysis and utility functions\n",
    "\n",
    "def analyze_basic_filtering_results(dataset_path: str, generator_stats: Optional[Dict] = None) -> Dict:\n",
    "    \"\"\"\n",
    "     Analyze the results of basic filtering using both dataset and generation statistics.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to the generated dataset\n",
    "        generator_stats: Optional generator statistics for complete analysis\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "            dataset = json.load(f)\n",
    "        \n",
    "        queries = dataset.get(\"queries\", [])\n",
    "        metadata = dataset.get(\"metadata\", {})\n",
    "        \n",
    "        if not queries:\n",
    "            return {\"error\": \"No queries found in dataset\"}\n",
    "        \n",
    "        #  Get actual generation statistics from metadata if available\n",
    "        generation_stats = metadata.get(\"generation_statistics\", {})\n",
    "        if generator_stats:\n",
    "            generation_stats = generator_stats\n",
    "        \n",
    "        # Analyze accepted queries (those in the final dataset)\n",
    "        accepted_count = len(queries)\n",
    "        \n",
    "        # Calculate quality metrics for accepted queries\n",
    "        quality_metrics = {\n",
    "            \"relevance_scores\": [],\n",
    "            \"question_lengths\": [],\n",
    "            \"answer_lengths\": [],\n",
    "            \"context_similarities\": [],\n",
    "            \"triple_similarities\": []\n",
    "        }\n",
    "        \n",
    "        question_type_distribution = {}\n",
    "        \n",
    "        for query in queries:\n",
    "            filtering_info = query.get(\"filtering_result\", {})\n",
    "            quality_metrics[\"relevance_scores\"].append(filtering_info.get(\"relevance_score\", 0.0))\n",
    "            quality_metrics[\"question_lengths\"].append(len(query[\"question\"].split()))\n",
    "            quality_metrics[\"answer_lengths\"].append(len(query[\"answer\"].split()))\n",
    "            quality_metrics[\"context_similarities\"].append(filtering_info.get(\"context_similarity\", 0.0))\n",
    "            quality_metrics[\"triple_similarities\"].append(filtering_info.get(\"triple_similarity\", 0.0))\n",
    "            \n",
    "            # Count question types\n",
    "            q_type = query.get(\"question_type\", \"unknown\")\n",
    "            question_type_distribution[q_type] = question_type_distribution.get(q_type, 0) + 1\n",
    "        \n",
    "        #  Use generation statistics for complete picture\n",
    "        total_attempts = generation_stats.get(\"total_qa_attempts\", accepted_count)\n",
    "        rejected_count = total_attempts - accepted_count\n",
    "        \n",
    "        # Calculate comprehensive statistics\n",
    "        analysis_results = {\n",
    "            \"total_qa_attempts\": total_attempts,\n",
    "            \"final_dataset_queries\": accepted_count,\n",
    "            \"decision_distribution\": {\n",
    "                \"accepted_count\": accepted_count,\n",
    "                \"rejected_count\": rejected_count,\n",
    "                \"acceptance_rate\": (accepted_count / total_attempts) * 100 if total_attempts > 0 else 0\n",
    "            },\n",
    "            \"detailed_filtering_breakdown\": generation_stats.get(\"filtering_decisions\", {}),\n",
    "            \"question_type_distribution\": question_type_distribution,\n",
    "            \"quality_statistics\": {\n",
    "                \"avg_relevance_score\": np.mean(quality_metrics[\"relevance_scores\"]) if quality_metrics[\"relevance_scores\"] else 0,\n",
    "                \"min_relevance_score\": min(quality_metrics[\"relevance_scores\"]) if quality_metrics[\"relevance_scores\"] else 0,\n",
    "                \"max_relevance_score\": max(quality_metrics[\"relevance_scores\"]) if quality_metrics[\"relevance_scores\"] else 0,\n",
    "                \"std_relevance_score\": np.std(quality_metrics[\"relevance_scores\"]) if quality_metrics[\"relevance_scores\"] else 0,\n",
    "                \"avg_question_length\": np.mean(quality_metrics[\"question_lengths\"]) if quality_metrics[\"question_lengths\"] else 0,\n",
    "                \"avg_answer_length\": np.mean(quality_metrics[\"answer_lengths\"]) if quality_metrics[\"answer_lengths\"] else 0,\n",
    "                \"avg_context_similarity\": np.mean(quality_metrics[\"context_similarities\"]) if quality_metrics[\"context_similarities\"] else 0,\n",
    "                \"avg_triple_similarity\": np.mean(quality_metrics[\"triple_similarities\"]) if quality_metrics[\"triple_similarities\"] else 0\n",
    "            },\n",
    "            \"generation_efficiency\": {\n",
    "                \"api_calls\": generation_stats.get(\"total_api_calls\", 0),\n",
    "                \"qa_per_api_call\": accepted_count / generation_stats.get(\"total_api_calls\", 1),\n",
    "                \"acceptance_rate\": generation_stats.get(\"acceptance_rate\", 0)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return analysis_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error analyzing basic filtering results: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "def generate_basic_report(dataset_path: str, output_report_path: str, generator_stats: Optional[Dict] = None) -> None:\n",
    "    \"\"\"\n",
    "     Generate a comprehensive report on basic filtering performance.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to the basic filtering dataset\n",
    "        output_report_path: Path to save the filtering report\n",
    "        generator_stats: Optional generator statistics for complete analysis\n",
    "    \"\"\"\n",
    "    try:\n",
    "        #  Analyze with complete statistics\n",
    "        analysis = analyze_basic_filtering_results(dataset_path, generator_stats)\n",
    "        \n",
    "        if \"error\" in analysis:\n",
    "            logger.error(f\"Analysis failed: {analysis['error']}\")\n",
    "            return\n",
    "        \n",
    "        # Generate comprehensive report\n",
    "        report = {\n",
    "            \"report_metadata\": {\n",
    "                \"Generation_date\": pd.Timestamp.now().isoformat(),\n",
    "                \"Dataset_analyzed\": dataset_path,\n",
    "                \"Report_type\": \"QA_analysis\",\n",
    "                \"System_enhancements\": [\n",
    "                    \"Statistical consistency between generation and analysis\",\n",
    "                    \"Duplicate detection\",\n",
    "                    \"Centralised statistics tracking\",\n",
    "                    \"Memory management for embeddings\",\n",
    "                    \"Error handling\"\n",
    "                ]\n",
    "            },\n",
    "            \"executive_summary\": {\n",
    "                \"total_qa_attempts\": analysis.get(\"total_qa_attempts\", 0),\n",
    "                \"final_questions_generated\": analysis.get(\"final_dataset_queries\", 0),\n",
    "                \"overall_acceptance_rate\": f\"{analysis.get('decision_distribution', {}).get('acceptance_rate', 0):.1f}%\",\n",
    "                \"avg_relevance_score\": f\"{analysis.get('quality_statistics', {}).get('avg_relevance_score', 0):.3f}\",\n",
    "                \"api_efficiency\": f\"{analysis.get('generation_efficiency', {}).get('qa_per_api_call', 0):.1f} QA pairs per API call\"\n",
    "            },\n",
    "            \"detailed_analysis\": analysis,\n",
    "            \"filtering_approach\": \"Essential quality filters only: length requirements + duplicate detection\",\n",
    "\n",
    "        }\n",
    "        \n",
    "        # Save report\n",
    "        with open(output_report_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        logger.info(f\" basic filtering report generated: {output_report_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating basic filtering report: {e}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate the  basic filtering QA generation system.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting  Basic Zero-Shot QA Generation\")\n",
    "    \n",
    "    try:\n",
    "        # Get API key\n",
    "        openai_api_key = input(\"Please enter your OpenAI API key: \").strip()\n",
    "        \n",
    "        if not openai_api_key or openai_api_key == \"your-openai-api-key-here\":\n",
    "            logger.error(\"Please provide a valid OpenAI API key\")\n",
    "            return\n",
    "        \n",
    "        # Initialize generator with  basic filtering\n",
    "        generator = BasicQAGenerator(\n",
    "            openai_api_key=openai_api_key,\n",
    "            reset_duplicates_per_chunk=True,\n",
    "            embedding_model='BAAI/bge-large-en-v1.5',\n",
    "            max_embedding_cache=1000\n",
    "        )\n",
    "\n",
    "        # File paths\n",
    "        chunks_file = input(\"Enter path to chunks CSV file (or press Enter for 'chunks.csv'): \").strip() or \"chunks.csv\"\n",
    "        triples_file = input(\"Enter path to triples CSV file (or press Enter for 'Ontology_Guided_Triples.csv'): \").strip() or \"Ontology_Guided_Triples.csv\"\n",
    "        output_file = input(\"Enter output file name (or press Enter for 'Zero-Shot_qa_dataset.json'): \").strip() or \"Zero-Shot_qa_dataset.json\"\n",
    "\n",
    "        # Check if files exist\n",
    "        if not os.path.exists(chunks_file):\n",
    "            logger.error(f\"Chunks file not found: {chunks_file}\")\n",
    "            logger.info(\"Please ensure your chunks file exists and has the correct path\")\n",
    "            return\n",
    "            \n",
    "        if not os.path.exists(triples_file):\n",
    "            logger.error(f\"Triples file not found: {triples_file}\")\n",
    "            logger.info(\"Please ensure your triples file exists and has the correct path\")\n",
    "            return\n",
    "\n",
    "        # Load and merge data\n",
    "        logger.info(\"Loading and merging data files\")\n",
    "        merged_data = generator.load_and_merge_data(chunks_file, triples_file)\n",
    "\n",
    "        if not merged_data:\n",
    "            logger.error(\"No data loaded. Please check your input files.\")\n",
    "            return\n",
    "\n",
    "        # Chunks processing \n",
    "        limit = None\n",
    "        logger.info(\"Processing all chunks\")\n",
    "\n",
    "        # Create  basic filtering dataset\n",
    "        logger.info(f\"Creating Zero shot QA dataset (processing {limit} chunks)\")\n",
    "        generator.create_zero_shot_dataset(\n",
    "            merged_grouped=merged_data,\n",
    "            output_file=output_file,\n",
    "            limit=limit\n",
    "        )\n",
    "\n",
    "        # Get final statistics from generator\n",
    "        final_stats = generator.get_filtering_statistics()\n",
    "\n",
    "        # Generate  analysis report\n",
    "        logger.info(\"Analyzing  filtering results\")\n",
    "        analysis_results = analyze_basic_filtering_results(output_file, final_stats)\n",
    "        \n",
    "        if \"error\" in analysis_results:\n",
    "            logger.error(f\"Analysis failed: {analysis_results['error']}\")\n",
    "        else:\n",
    "            logger.info(\" Filtering Results:\")\n",
    "            logger.info(f\"  Total QA attempts: {analysis_results.get('total_qa_attempts', 0)}\")\n",
    "            logger.info(f\"  Final questions generated: {analysis_results.get('final_dataset_queries', 0)}\")\n",
    "            logger.info(f\"  Overall acceptance rate: {analysis_results.get('decision_distribution', {}).get('acceptance_rate', 0):.1f}%\")\n",
    "            logger.info(f\"  Average relevance score: {analysis_results.get('quality_statistics', {}).get('avg_relevance_score', 0):.3f}\")\n",
    "\n",
    "        # Generate comprehensive  report\n",
    "        report_file = \"Zero_Shot_QA_analysis_report.json\"\n",
    "        generate_basic_report(output_file, report_file, final_stats)\n",
    "        \n",
    "        # Display final statistics\n",
    "        logger.info(\"\\nFINAL Filtering Statistics:\")\n",
    "        logger.info(f\"Total API calls: {final_stats.get('total_api_calls', 0)}\")\n",
    "        logger.info(f\"Total QA attempts: {final_stats.get('total_qa_attempts', 0)}\")\n",
    "        logger.info(f\"Successful generations: {final_stats.get('successful_generations', 0)}\")\n",
    "\n",
    "        logger.info(\"\\nFiltering Decision Breakdown:\")\n",
    "        for decision_type, count in final_stats[\"filtering_decisions\"].items():\n",
    "            if count > 0:\n",
    "                percentage = (count / final_stats['total_qa_attempts']) * 100 if final_stats['total_qa_attempts'] > 0 else 0\n",
    "                logger.info(f\"  {decision_type}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        if \"acceptance_rate\" in final_stats:\n",
    "            logger.info(f\"\\nOverall Acceptance Rate: {final_stats['acceptance_rate']:.1f}%\")\n",
    "        \n",
    "        # Validation check\n",
    "        validation_status = final_stats.get(\"validation_status\", \"UNKNOWN\")\n",
    "        logger.info(f\"Statistics validation: {validation_status}\")\n",
    "        \n",
    "        logger.info(f\"\\nOutputs generated:\")\n",
    "        logger.info(f\"  Dataset: {output_file}\")\n",
    "        logger.info(f\"  Analysis Report: {report_file}\")\n",
    "        logger.info(\"\\nZero Shot QA generation completed successfully!\")\n",
    "        logger.info(\"\\nKey fixes applied:\")\n",
    "        logger.info(\"   Statistical consistency resolved\")\n",
    "        logger.info(\"   Duplicate detection timing\")\n",
    "        logger.info(\"   Centralized statistics tracking\")\n",
    "        logger.info(\"   Memory management added\")\n",
    "        logger.info(\"   Error handling\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"\\nProcess interrupted by user\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main execution: {str(e)}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a716dd1-937a-43e9-ba9f-01aad0063d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8b10a8-e63c-4c8e-a016-045fde0b46c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cd18bb-f3a2-4572-ace3-7e704bf54d83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
