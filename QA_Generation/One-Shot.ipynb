{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5defca36-9680-46b5-81ef-757c358ce6da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "One-Shot QA Generator with Essential Quality Filtering \n",
    "\n",
    "\n",
    "\n",
    "Features:\n",
    "- One manually constructed exemplar per question type (factual, relationship, comparative, inferential)\n",
    "- Same essential filtering as zero-shot: length requirements + duplicate detection\n",
    "- Enhanced prompt engineering with exemplar-guided generation\n",
    "- Maintains all statistical tracking and quality controls from zero-shot version\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import time\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from collections import OrderedDict\n",
    "\n",
    "# OpenAI import fix with proper error handling\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    OPENAI_V1 = True\n",
    "    logger_msg = \"Using OpenAI v1.x API\"\n",
    "except ImportError:\n",
    "    try:\n",
    "        import openai\n",
    "        OPENAI_V1 = False\n",
    "        logger_msg = \"Using OpenAI legacy API\"\n",
    "    except ImportError:\n",
    "        print(\"ERROR: OpenAI library not installed. Run: pip install openai\")\n",
    "        sys.exit(1)\n",
    "\n",
    "# Configure environment and logging\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('one_shot_qa_generation.log'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Device configuration with proper torch import\n",
    "try:\n",
    "    import torch\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \n",
    "                         \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "except ImportError:\n",
    "    print(\"ERROR: PyTorch not installed. Run: pip install torch\")\n",
    "    sys.exit(1)\n",
    "\n",
    "logger.info(logger_msg)\n",
    "\n",
    "\n",
    "class QuestionType(Enum):\n",
    "    \"\"\"Enumeration of supported question types for one-shot generation.\"\"\"\n",
    "    FACTUAL = \"factual\"\n",
    "    RELATIONSHIP = \"relationship\" \n",
    "    COMPARATIVE = \"comparative\"\n",
    "    INFERENTIAL = \"inferential\"\n",
    "\n",
    "\n",
    "class FilteringDecision(Enum):\n",
    "    \"\"\"Basic filtering decision outcomes.\"\"\"\n",
    "    ACCEPTED = \"accepted\"\n",
    "    REJECTED_LENGTH = \"rejected_length\"\n",
    "    REJECTED_DUPLICATE = \"rejected_duplicate\"\n",
    "    REJECTED_PARSING = \"rejected_parsing\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class QualityThresholds:\n",
    "    \"\"\"Basic quality thresholds for essential filtering only.\"\"\"\n",
    "    # Length requirements - ensures meaningful content\n",
    "    min_question_words: int = 8      # Ensures meaningful questions\n",
    "    min_answer_words: int = 20       # Ensures informative answers\n",
    "    \n",
    "    # Duplicate detection thresholds - more lenient for variety\n",
    "    duplicate_similarity_threshold: float = 0.85  \n",
    "    batch_similarity_threshold: float = 0.85     \n",
    "    \n",
    "    # Semantic similarity weights for relevance scoring (logging only)\n",
    "    context_weight: float = 0.6\n",
    "    triple_weight: float = 0.4\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FilteringResult:\n",
    "    \"\"\"Data class representing a basic filtering result with reasoning.\"\"\"\n",
    "    accepted: bool\n",
    "    decision: FilteringDecision\n",
    "    relevance_score: float  # For logging/analysis only\n",
    "    context_similarity: float\n",
    "    triple_similarity: float\n",
    "    reasoning: str\n",
    "    metadata: Dict[str, float]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class QAPair:\n",
    "    \"\"\"Data class representing a question-answer pair with complete metadata.\"\"\"\n",
    "    id: str\n",
    "    question: str\n",
    "    answer: str\n",
    "    question_type: QuestionType\n",
    "    qa_metadata: Dict[str, List[str]]\n",
    "    filtering_result: FilteringResult\n",
    "    generation_method: str = \"one_shot\"\n",
    "    # Add ground truth information\n",
    "    source_context: str = \"\"\n",
    "    source_triples: List[Tuple] = None\n",
    "    chunk_id: str = \"\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.source_triples is None:\n",
    "            self.source_triples = []\n",
    "\n",
    "\n",
    "# ONE-SHOT EXEMPLAR - The manually constructed high-quality example\n",
    "ONE_SHOT_EXEMPLAR = {\n",
    "    \"context\": \"ESFA funds children who are currently electively home educated (EHE) who attend general further education (Further Education) and sixth form colleges. EHE children who attend schools and academies are not eligible for ESFA young people's funding. You can find more information in the Funding rates and formula guide. Colleges may claim ESFA young people's funding for children of compulsory school age who have completed their statutory education, have achieved qualifications at least equivalent to a full level 2, and who want to enrol on a level 3 course. Colleges do not need to seek approval from ESFA, as we will count these students for lagged funding purposes. This advice also applies to schools and academies placing students in their sixth forms earlier than usual. In exceptional circumstances, for example, students arriving in the UK for the first time during school year 11, ESFA will consider provision for individual students of compulsory school age to be eligible for ESFA young people's funding in colleges. Groups of students would not be eligible for funding, since by inference such circumstances are unlikely to be exceptional.\",\n",
    "\n",
    "    \"knowledge_graph\": [\n",
    "        (\"esfa\", \"funds\", \"ehe_children_further_education\"),\n",
    "        (\"ehe_children_further_education\", \"enrolled_in\", \"further_education_colleges\"),\n",
    "        (\"ehe_children_further_education\", \"enrolled_in\", \"sixth_form_colleges\"),\n",
    "        (\"ehe_children_schools_academies\", \"not_eligible_for\", \"esfa_young_people_funding\"),\n",
    "        (\"funding_rates_and_formula_guide\", \"provides_information_on\", \"esfa_funding_details\"),\n",
    "        (\"colleges\", \"can_claim\", \"esfa_young_people_funding\"),\n",
    "        (\"esfa_young_people_funding\", \"for_programme\", \"level_3_course\"),\n",
    "        (\"children_compulsory_school_age\", \"has_achievement_status\", \"full_level_2_qualification\"),\n",
    "        (\"esfa\", \"does_not_require_approval_from\", \"colleges_for_lagged_funding\"),\n",
    "        (\"schools_and_academies\", \"applies_same_advice_as\", \"colleges_for_early_sixth_form_placement\"),\n",
    "        (\"esfa\", \"considers_funding_eligibility_for\", \"individual_students_compulsory_school_age\"),\n",
    "        (\"individual_students_compulsory_school_age\", \"has_reason\", \"arriving_in_uk_during_school_year_11\"),\n",
    "        (\"groups_of_students\", \"not_eligible_for\", \"esfa_young_people_funding_due_to_non_exceptional_circumstances\")\n",
    "    ],\n",
    "    \n",
    "    \"exemplar_questions\": {\n",
    "        QuestionType.FACTUAL: {\n",
    "            \"question\": \"What types of institutions can claim ESFA young people's funding for electively home educated (EHE) children?\"\n",
    "        },\n",
    "        \n",
    "        QuestionType.RELATIONSHIP: {\n",
    "            \"question\": \"How is a full Level 2 qualification related to eligibility for Level 3 ESFA-funded programmes for compulsory school-age students?\"\n",
    "        },\n",
    "        \n",
    "        QuestionType.COMPARATIVE: {\n",
    "            \"question\": \"How does ESFA funding eligibility differ between individual and groups of students arriving in the UK during school year 11?\"\n",
    "        },\n",
    "        \n",
    "        QuestionType.INFERENTIAL: {\n",
    "            \"question\": \"Based on the information provided, what is the underlying reason ESFA distinguishes between individual and groups of students when considering funding for those arriving in the UK during school year 11?\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Enhanced one-shot question generation templates\n",
    "ONE_SHOT_QUESTION_TEMPLATES = {\n",
    "    QuestionType.FACTUAL: {\n",
    "        \"task_description\": (\n",
    "            \"Factual questions seek specific, concrete information that can be directly extracted from \"\n",
    "            \"the text or inferred from the knowledge graph relationships. They typically start with \"\n",
    "            \"'What', 'When', 'Where', 'Who', or 'How much/many' and ask for precise details.\"\n",
    "        ),\n",
    "        \"generation_guidance\": (\n",
    "            \"Generate factual questions that require **specific** information from the context. \"\n",
    "            \"These questions should ask for concrete details, numbers, dates, names, or specific requirements \"\n",
    "            \"mentioned in the text. Focus on extracting precise information that can be directly answered \"\n",
    "            \"from the provided context and knowledge graph triples.\"\n",
    "        )\n",
    "    },\n",
    "    \n",
    "    QuestionType.RELATIONSHIP: {\n",
    "        \"task_description\": (\n",
    "            \"Relationship questions explore connections between entities. They examine how entities \"\n",
    "            \"interact, depend on each other, or influence one another through the relationships \"\n",
    "            \"defined in the knowledge graph.\"\n",
    "        ),\n",
    "        \"generation_guidance\": (\n",
    "            \"Generate questions about specific relationships or interactions between entities in the context. \"\n",
    "            \"Focus on how different entities, organizations, processes, or concepts connect, influence, or \"\n",
    "            \"interact with each other. Each question must reference at least two entities and explore \"\n",
    "            \"their connection through the knowledge graph relationships.\"\n",
    "        )\n",
    "    },\n",
    "    \n",
    "    QuestionType.COMPARATIVE: {\n",
    "        \"task_description\": (\n",
    "            \"Comparative questions examine differences and similarities between entities or concepts. \"\n",
    "            \"They help understand distinctions in requirements, processes, amounts, or characteristics \"\n",
    "            \"across different categories or instances.\"\n",
    "        ),\n",
    "        \"generation_guidance\": (\n",
    "            \"Generate questions comparing different aspects, entities, or concepts from the context. \"\n",
    "            \"These questions should highlight differences, similarities, or contrasts between multiple \"\n",
    "            \"items such as funding types, requirements, processes, or organizational structures. \"\n",
    "            \"Use the knowledge graph to identify comparable entities.\"\n",
    "        )\n",
    "    },\n",
    "    \n",
    "    QuestionType.INFERENTIAL: {\n",
    "        \"task_description\": (\n",
    "            \"Inferential questions require reasoning and synthesis of multiple pieces of information. \"\n",
    "            \"They ask for conclusions, implications, or predictions that must be derived by combining \"\n",
    "            \"various facts and relationships from the context and knowledge graph.\"\n",
    "        ),\n",
    "        \"generation_guidance\": (\n",
    "            \"Generate questions that require analysis, reasoning, or inference based on the context \"\n",
    "            \"and knowledge graph. These questions should combine multiple pieces of information to \"\n",
    "            \"draw conclusions, identify implications, or predict outcomes. They require synthesizing \"\n",
    "            \"information from multiple knowledge graph triples.\"\n",
    "        )\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "class OneShotQAGenerator:\n",
    "    \"\"\"\n",
    "    One-Shot QA Generator with Essential Quality Filtering.\n",
    "    \n",
    "    Features:\n",
    "    - Uses one high-quality exemplar per question type to guide generation\n",
    "    - Length requirements for meaningful content\n",
    "    - Duplicate detection to prevent redundancy \n",
    "    - Basic relevance scoring for analysis (not filtering)\n",
    "    - Simplified decision logic with clear justification\n",
    "    - Centralized statistics tracking \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 openai_api_key: str,\n",
    "                 reset_duplicates_per_chunk: bool = True,\n",
    "                 embedding_model: str = 'BAAI/bge-large-en-v1.5',\n",
    "                 max_embedding_cache: int = 1000):\n",
    "        \"\"\"\n",
    "        Initialize the One-Shot QA Generator.\n",
    "        \n",
    "        Args:\n",
    "            openai_api_key: OpenAI API key for GPT-4 access\n",
    "            reset_duplicates_per_chunk: Whether to reset duplicate tracking per chunk\n",
    "            max_embedding_cache: Maximum number of embeddings to cache (memory management)\n",
    "        \"\"\"\n",
    "        if not openai_api_key or openai_api_key == \"your-openai-api-key-here\":\n",
    "            raise ValueError(\"Please provide a valid OpenAI API key\")\n",
    "            \n",
    "        self.openai_api_key = openai_api_key\n",
    "        self.max_embedding_cache = max_embedding_cache\n",
    "        \n",
    "        # Initialize OpenAI client based on version\n",
    "        if OPENAI_V1:\n",
    "            self.openai_client = OpenAI(api_key=openai_api_key)\n",
    "        else:\n",
    "            openai.api_key = openai_api_key\n",
    "            self.openai_client = None\n",
    "        \n",
    "        # Initialize quality thresholds\n",
    "        self.thresholds = QualityThresholds()\n",
    "        self.reset_duplicates_per_chunk = reset_duplicates_per_chunk\n",
    "        \n",
    "        # Initialize embedding model\n",
    "        logger.info(f\"Loading embedding model: {embedding_model}\")\n",
    "        try:\n",
    "            self.embedding_model = SentenceTransformer(embedding_model, device=device)\n",
    "            logger.info(f\"Successfully loaded {embedding_model}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading {embedding_model}: {e}\")\n",
    "            raise RuntimeError(f\"Failed to load the specified embedding model '{embedding_model}'. Please check your internet connection and ensure the model name is correct.\")\n",
    "        \n",
    "        # Use OrderedDict for memory management and better duplicate tracking\n",
    "        self.all_questions: set = set()\n",
    "        self.question_embeddings: OrderedDict[str, torch.Tensor] = OrderedDict()\n",
    "        self.batch_questions: set = set()\n",
    "        self.batch_embeddings: Dict[str, torch.Tensor] = {}\n",
    "        \n",
    "        # Enhanced statistics tracking with detailed breakdown\n",
    "        self.generation_stats = {\n",
    "            \"total_api_calls\": 0,\n",
    "            \"total_qa_attempts\": 0,\n",
    "            \"successful_generations\": 0,\n",
    "            \"filtering_decisions\": {\n",
    "                \"accepted\": 0,\n",
    "                \"rejected_length\": 0,\n",
    "                \"rejected_duplicate\": 0,\n",
    "                \"rejected_parsing\": 0\n",
    "            },\n",
    "            \"all_attempts\": []\n",
    "        }\n",
    "        \n",
    "        logger.info(\"One-shot QA Generator initialized successfully\")\n",
    "        logger.info(\"Using exemplar-guided generation with essential filters: length requirements + duplicate detection\")\n",
    "\n",
    "    def _filtering_result_to_dict(self, result: FilteringResult) -> Dict:\n",
    "        \"\"\"Convert FilteringResult to dictionary for JSON serialization.\"\"\"\n",
    "        return {\n",
    "            \"accepted\": result.accepted,\n",
    "            \"decision\": result.decision.value,\n",
    "            \"relevance_score\": result.relevance_score,\n",
    "            \"context_similarity\": result.context_similarity,\n",
    "            \"triple_similarity\": result.triple_similarity,\n",
    "            \"reasoning\": result.reasoning,\n",
    "            \"metadata\": result.metadata\n",
    "        }\n",
    "\n",
    "    def _manage_embedding_cache(self):\n",
    "        \"\"\"Manage embedding cache size to prevent memory issues.\"\"\"\n",
    "        if len(self.question_embeddings) > self.max_embedding_cache:\n",
    "            # Remove oldest embeddings (FIFO)\n",
    "            removed_count = len(self.question_embeddings) - self.max_embedding_cache\n",
    "            for _ in range(removed_count):\n",
    "                oldest_key = next(iter(self.question_embeddings))\n",
    "                del self.question_embeddings[oldest_key]\n",
    "                self.all_questions.discard(oldest_key)\n",
    "            logger.debug(f\"Removed {removed_count} old embeddings from cache\")\n",
    "\n",
    "    def calculate_relevance_score(self, question: str, answer: str, context: str, triples: List[Tuple]) -> Tuple[float, float, float]:\n",
    "        \"\"\"\n",
    "        Calculate semantic relevance score for logging/analysis purposes only.\n",
    "        This is NOT used for filtering decisions.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            qa_combined = f\"{question} {answer}\"\n",
    "            qa_embedding = self.embedding_model.encode(qa_combined, convert_to_tensor=True)\n",
    "            \n",
    "            # Context similarity\n",
    "            context_embedding = self.embedding_model.encode(context, convert_to_tensor=True)\n",
    "            context_similarity = util.cos_sim(context_embedding, qa_embedding).item()\n",
    "            \n",
    "            # Triple similarities\n",
    "            triple_similarities = []\n",
    "            for triple in triples:\n",
    "                try:\n",
    "                    if len(triple) >= 3:\n",
    "                        n1, edge, n2 = str(triple[0]), str(triple[1]), str(triple[2])\n",
    "                        triple_text = f\"{n1} {edge} {n2}\"\n",
    "                        triple_embedding = self.embedding_model.encode(triple_text, convert_to_tensor=True)\n",
    "                        triple_similarity = util.cos_sim(triple_embedding, qa_embedding).item()\n",
    "                        triple_similarities.append(triple_similarity)\n",
    "                except (IndexError, TypeError) as e:\n",
    "                    logger.debug(f\"Invalid triple format: {triple}, error: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            max_triple_similarity = max(triple_similarities) if triple_similarities else 0.0\n",
    "            \n",
    "            # Weighted overall relevance (for logging only)\n",
    "            overall_relevance = (context_similarity * self.thresholds.context_weight + \n",
    "                               max_triple_similarity * self.thresholds.triple_weight)\n",
    "            \n",
    "            return overall_relevance, context_similarity, max_triple_similarity\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error calculating relevance score: {e}\")\n",
    "            return 0.0, 0.0, 0.0\n",
    "\n",
    "    def passes_length_filter(self, question: str, answer: str) -> bool:\n",
    "        \"\"\"Check if QA pair meets minimum length requirements.\"\"\"\n",
    "        try:\n",
    "            q_words = len(question.split())\n",
    "            a_words = len(answer.split())\n",
    "            \n",
    "            if q_words < self.thresholds.min_question_words or a_words < self.thresholds.min_answer_words:\n",
    "                logger.debug(f\"Length filter failed: Q={q_words} words (min {self.thresholds.min_question_words}), \"\n",
    "                            f\"A={a_words} words (min {self.thresholds.min_answer_words})\")\n",
    "                return False\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error in length filter: {e}\")\n",
    "            return False\n",
    "\n",
    "    def is_semantic_duplicate(self, new_question: str) -> Tuple[bool, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Check if question is a semantic duplicate and return embedding.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (is_duplicate, question_embedding)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            new_embedding = self.embedding_model.encode(new_question, convert_to_tensor=True)\n",
    "            \n",
    "            # Check against historical questions\n",
    "            for existing_question, existing_embedding in self.question_embeddings.items():\n",
    "                similarity = util.cos_sim(new_embedding, existing_embedding).item()\n",
    "                if similarity > self.thresholds.duplicate_similarity_threshold:\n",
    "                    logger.debug(f\"Historical duplicate detected: '{new_question}' ~ '{existing_question}' \"\n",
    "                               f\"(similarity={similarity:.3f})\")\n",
    "                    return True, new_embedding\n",
    "            \n",
    "            # Check against current batch using stored embeddings\n",
    "            for batch_question, batch_embedding in self.batch_embeddings.items():\n",
    "                similarity = util.cos_sim(new_embedding, batch_embedding).item()\n",
    "                if similarity > self.thresholds.batch_similarity_threshold:\n",
    "                    logger.debug(f\"Batch duplicate detected: '{new_question}' ~ '{batch_question}' \"\n",
    "                               f\"(similarity={similarity:.3f})\")\n",
    "                    return True, new_embedding\n",
    "            \n",
    "            return False, new_embedding\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error in duplicate detection: {e}\")\n",
    "            # Return False with dummy embedding on error\n",
    "            dummy_embedding = torch.zeros(384)  # Default embedding size\n",
    "            return False, dummy_embedding\n",
    "\n",
    "    def filter_qa_pair(self, \n",
    "                      question: str, \n",
    "                      answer: str, \n",
    "                      context: str, \n",
    "                      triples: List[Tuple],\n",
    "                      qa_metadata: Dict) -> Tuple[bool, FilteringResult]:\n",
    "        \"\"\"\n",
    "        Apply basic filtering pipeline to QA pair with centralized statistics tracking.\n",
    "        \n",
    "        Args:\n",
    "            question: Question text\n",
    "            answer: Answer text\n",
    "            context: Original text chunk\n",
    "            triples: Knowledge graph triples\n",
    "            qa_metadata: QA metadata including entities and relationships\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (acceptance_decision, filtering_result_details)\n",
    "        \"\"\"\n",
    "        # Centralized statistics tracking - increment total attempts here\n",
    "        self.generation_stats[\"total_qa_attempts\"] += 1\n",
    "        \n",
    "        # Calculate relevance score first (for all attempts)\n",
    "        relevance_score, context_sim, triple_sim = self.calculate_relevance_score(\n",
    "            question, answer, context, triples\n",
    "        )\n",
    "        \n",
    "        # Create base metadata\n",
    "        base_metadata = {\n",
    "            \"answer_length\": len(answer.split()),\n",
    "            \"question_length\": len(question.split()),\n",
    "            \"mentioned_entities\": len(qa_metadata.get(\"mentioned_entities\", [])),\n",
    "            \"mentioned_relationships\": len(qa_metadata.get(\"mentioned_relationships\", []))\n",
    "        }\n",
    "        \n",
    "        # Filter 1: Length requirements\n",
    "        if not self.passes_length_filter(question, answer):\n",
    "            result = FilteringResult(\n",
    "                accepted=False,\n",
    "                decision=FilteringDecision.REJECTED_LENGTH,\n",
    "                relevance_score=relevance_score,\n",
    "                context_similarity=context_sim,\n",
    "                triple_similarity=triple_sim,\n",
    "                reasoning=f\"Failed length requirements: Q={len(question.split())} words, A={len(answer.split())} words\",\n",
    "                metadata=base_metadata\n",
    "            )\n",
    "            \n",
    "            # Update statistics\n",
    "            self.generation_stats[\"filtering_decisions\"][\"rejected_length\"] += 1\n",
    "            \n",
    "            # Store attempt for analysis\n",
    "            self.generation_stats[\"all_attempts\"].append({\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"filtering_result\": self._filtering_result_to_dict(result),\n",
    "                \"relevance_score\": relevance_score\n",
    "            })\n",
    "            \n",
    "            return False, result\n",
    "        \n",
    "        # Filter 2: Duplicate detection \n",
    "        is_duplicate, question_embedding = self.is_semantic_duplicate(question)\n",
    "        if is_duplicate:\n",
    "            result = FilteringResult(\n",
    "                accepted=False,\n",
    "                decision=FilteringDecision.REJECTED_DUPLICATE,\n",
    "                relevance_score=relevance_score,\n",
    "                context_similarity=context_sim,\n",
    "                triple_similarity=triple_sim,\n",
    "                reasoning=\"Semantic duplicate detected\",\n",
    "                metadata=base_metadata\n",
    "            )\n",
    "            \n",
    "            # Update statistics\n",
    "            self.generation_stats[\"filtering_decisions\"][\"rejected_duplicate\"] += 1\n",
    "            \n",
    "            # Store attempt for analysis\n",
    "            self.generation_stats[\"all_attempts\"].append({\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"filtering_result\": self._filtering_result_to_dict(result),\n",
    "                \"relevance_score\": relevance_score\n",
    "            })\n",
    "            \n",
    "            return False, result\n",
    "        \n",
    "        # Accept QA pair - passed all filters\n",
    "        result = FilteringResult(\n",
    "            accepted=True,\n",
    "            decision=FilteringDecision.ACCEPTED,\n",
    "            relevance_score=relevance_score,\n",
    "            context_similarity=context_sim,\n",
    "            triple_similarity=triple_sim,\n",
    "            reasoning=f\"Passed basic quality filters (relevance: {relevance_score:.3f})\",\n",
    "            metadata=base_metadata\n",
    "        )\n",
    "        \n",
    "        # Register accepted question with proper embedding storage\n",
    "        self.all_questions.add(question)\n",
    "        self.question_embeddings[question] = question_embedding\n",
    "        self.batch_questions.add(question)\n",
    "        self.batch_embeddings[question] = question_embedding\n",
    "        \n",
    "        # Manage cache size\n",
    "        self._manage_embedding_cache()\n",
    "        \n",
    "        # Update statistics\n",
    "        self.generation_stats[\"successful_generations\"] += 1\n",
    "        self.generation_stats[\"filtering_decisions\"][\"accepted\"] += 1\n",
    "        \n",
    "        # Store attempt for analysis\n",
    "        self.generation_stats[\"all_attempts\"].append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"filtering_result\": self._filtering_result_to_dict(result),\n",
    "            \"relevance_score\": relevance_score\n",
    "        })\n",
    "        \n",
    "        return True, result\n",
    "\n",
    "    def load_and_merge_data(self, chunks_file: str, triples_file: str) -> Dict:\n",
    "        \"\"\"Load and merge chunks and triples data.\"\"\"\n",
    "        try:\n",
    "            # Load chunks data\n",
    "            chunks_df = pd.read_csv(chunks_file)\n",
    "            logger.info(f\"Loaded {len(chunks_df)} chunks from {chunks_file}\")\n",
    "            logger.info(f\"Chunks columns: {list(chunks_df.columns)}\")\n",
    "            \n",
    "            # Load triples data\n",
    "            triples_df = pd.read_csv(triples_file)\n",
    "            logger.info(f\"Loaded {len(triples_df)} triples from {triples_file}\")\n",
    "            logger.info(f\"Triples columns: {list(triples_df.columns)}\")\n",
    "            \n",
    "            # Check for required columns\n",
    "            if 'chunk_id' not in chunks_df.columns:\n",
    "                raise ValueError(\"chunks_file must contain 'chunk_id' column\")\n",
    "            if 'text' not in chunks_df.columns:\n",
    "                raise ValueError(\"chunks_file must contain 'text' column\")\n",
    "            if 'chunk_id' not in triples_df.columns:\n",
    "                raise ValueError(\"triples_file must contain 'chunk_id' column\")\n",
    "            \n",
    "            # Try to identify triple columns\n",
    "            triple_columns = None\n",
    "            possible_combinations = [\n",
    "                ('subject', 'predicate', 'object'),\n",
    "                ('node1', 'edge', 'node2'),\n",
    "                ('head', 'relation', 'tail'),\n",
    "                ('entity1', 'relationship', 'entity2')\n",
    "            ]\n",
    "            \n",
    "            for combo in possible_combinations:\n",
    "                if all(col in triples_df.columns for col in combo):\n",
    "                    triple_columns = combo\n",
    "                    break\n",
    "            \n",
    "            if triple_columns is None:\n",
    "                available_cols = list(triples_df.columns)\n",
    "                raise ValueError(f\"Could not identify triple columns. Available columns: {available_cols}. \"\n",
    "                               f\"Expected one of: {possible_combinations}\")\n",
    "            \n",
    "            logger.info(f\"Using triple columns: {triple_columns}\")\n",
    "            \n",
    "            # Merge on chunk_id\n",
    "            merged_df = pd.merge(chunks_df, triples_df, on='chunk_id', how='inner')\n",
    "            logger.info(f\"Merged data contains {len(merged_df)} records\")\n",
    "            \n",
    "            if len(merged_df) == 0:\n",
    "                raise ValueError(\"No matching chunk_ids found between chunks and triples files\")\n",
    "            \n",
    "            # Group by chunk_id\n",
    "            grouped_data = {}\n",
    "            for chunk_id, group in merged_df.groupby('chunk_id'):\n",
    "                grouped_data[chunk_id] = {\n",
    "                    'text': group['text'].iloc[0],\n",
    "                    'triples': [(row[triple_columns[0]], row[triple_columns[1]], row[triple_columns[2]]) \n",
    "                              for _, row in group.iterrows()]\n",
    "                }\n",
    "            \n",
    "            logger.info(f\"Grouped data contains {len(grouped_data)} unique chunks\")\n",
    "            return grouped_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading and merging data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_one_shot_prompt(self, context: str, triples: List[Tuple], question_type: QuestionType, num_questions: int) -> str:\n",
    "        \"\"\"Generate one-shot prompt with question-only exemplars.\"\"\"\n",
    "        template = ONE_SHOT_QUESTION_TEMPLATES.get(question_type, {})\n",
    "        task_description = template.get(\"task_description\", \"\")\n",
    "        generation_guidance = template.get(\"generation_guidance\", \"\")\n",
    "        \n",
    "        # Get the exemplar question for this type\n",
    "        exemplar_question = ONE_SHOT_EXEMPLAR[\"exemplar_questions\"][question_type][\"question\"]\n",
    "        \n",
    "        # Format current context triples\n",
    "        formatted_triples = []\n",
    "        for triple in triples[:20]:  # Limit to prevent prompt overflow\n",
    "            try:\n",
    "                if len(triple) >= 3:\n",
    "                    n1, e, n2 = str(triple[0]), str(triple[1]), str(triple[2])\n",
    "                    formatted_triples.append(f\"- {n1} → '{e}' → {n2}\")\n",
    "            except (IndexError, TypeError):\n",
    "                continue\n",
    "        \n",
    "        triple_text = \"\\n\".join(formatted_triples) if formatted_triples else \"No valid triples available\"\n",
    "    \n",
    "        # Format exemplar triples\n",
    "        exemplar_formatted_triples = []\n",
    "        for triple in ONE_SHOT_EXEMPLAR[\"knowledge_graph\"]:\n",
    "            n1, e, n2 = str(triple[0]), str(triple[1]), str(triple[2])\n",
    "            exemplar_formatted_triples.append(f\"- {n1} → '{e}' → {n2}\")\n",
    "        exemplar_triple_text = \"\\n\".join(exemplar_formatted_triples)\n",
    "    \n",
    "        # Create question-only one-shot prompt (following literature pattern)\n",
    "        prompt = f\"\"\"\n",
    "    ONE-SHOT QA GENERATION TASK\n",
    "    \n",
    "    TASK TYPE: {question_type.value.upper()}\n",
    "    {task_description}\n",
    "    \n",
    "    EXEMPLAR DEMONSTRATION:\n",
    "    \n",
    "    EXAMPLE CONTEXT:\n",
    "    {ONE_SHOT_EXEMPLAR[\"context\"][:1200]}{\"...\" if len(ONE_SHOT_EXEMPLAR[\"context\"]) > 1200 else \"\"}\n",
    "    \n",
    "    EXAMPLE KNOWLEDGE GRAPH TRIPLES:\n",
    "    {exemplar_triple_text}\n",
    "    \n",
    "    EXAMPLE {question_type.value.upper()} QUESTION:\n",
    "    {exemplar_question}\n",
    "    \n",
    "    NOW GENERATE FOR NEW CONTEXT:\n",
    "    \n",
    "    TARGET CONTEXT:\n",
    "    {context[:1200]}{\"...\" if len(context) > 1200 else \"\"}\n",
    "    \n",
    "    TARGET KNOWLEDGE GRAPH TRIPLES:\n",
    "    {triple_text}\n",
    "    \n",
    "    GENERATION INSTRUCTIONS: {generation_guidance}\n",
    "    \n",
    "    DIVERSITY REQUIREMENTS:\n",
    "    - Each question must be UNIQUE and ask about DIFFERENT aspects\n",
    "    - Use VARIED question starters and phrasing patterns  \n",
    "    - Focus on DIFFERENT entities, relationships, or information types\n",
    "    - Avoid repetitive structures or similar wordings\n",
    "    - Make each question distinctly different from others and from the exemplar\n",
    "    \n",
    "    REQUIRED OUTPUT FORMAT:\n",
    "    [\n",
    "      {{\n",
    "        \"id\": \"1\",\n",
    "        \"question\": \"Your detailed question here?\",\n",
    "        \"answer\": \"Your comprehensive answer here.\",\n",
    "        \"type\": \"{question_type.value}\",\n",
    "        \"qa_metadata\": {{\n",
    "          \"mentioned_entities\": [\"entity1\", \"entity2\"],\n",
    "          \"mentioned_relationships\": [\"relationship1\", \"relationship2\"]\n",
    "        }}\n",
    "      }}\n",
    "    ]\n",
    "    \n",
    "    f\"Generate {num_questions} {question_type.value} questions with answers similar to the question: {exemplar_question}\"\n",
    "\"\"\"\n",
    "        return prompt.strip()\n",
    "    \n",
    "    def parse_json_response(self, text_response: str) -> List[Dict]:\n",
    "        \"\"\"Parse JSON response from GPT-4 with improved error handling.\"\"\"\n",
    "        # Remove common markdown formatting that can break JSON parsing\n",
    "        text_response = text_response.strip()\n",
    "        if text_response.startswith('```json'):\n",
    "            text_response = text_response[7:]  # Remove ```json\n",
    "        if text_response.endswith('```'):\n",
    "            text_response = text_response[:-3]  # Remove trailing ```\n",
    "        text_response = text_response.strip()\n",
    "        \n",
    "        try:\n",
    "            parsed = json.loads(text_response)\n",
    "            if isinstance(parsed, list):\n",
    "                return parsed\n",
    "            elif isinstance(parsed, dict) and \"question\" in parsed:\n",
    "                return [parsed]\n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.debug(f\"Initial JSON parse failed: {e}\")\n",
    "        \n",
    "        # Fallback parsing strategies\n",
    "        try:\n",
    "            json_start = text_response.find('[')\n",
    "            json_end = text_response.rfind(']') + 1\n",
    "            if json_start >= 0 and json_end > json_start:\n",
    "                json_str = text_response[json_start:json_end]\n",
    "                parsed = json.loads(json_str)\n",
    "                if isinstance(parsed, list):\n",
    "                    return parsed\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Array extraction failed: {e}\")\n",
    "        \n",
    "        try:\n",
    "            json_start = text_response.find('{')\n",
    "            json_end = text_response.rfind('}') + 1\n",
    "            if json_start >= 0 and json_end > json_start:\n",
    "                json_str = text_response[json_start:json_end]\n",
    "                parsed = json.loads(json_str)\n",
    "                if isinstance(parsed, dict) and \"question\" in parsed:\n",
    "                    return [parsed]\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Object extraction failed: {e}\")\n",
    "        \n",
    "        logger.warning(f\"Failed to parse JSON response: {text_response[:200]}...\")\n",
    "        return []\n",
    "\n",
    "    def call_openai_api(self, system_message: str, user_message: str, max_tokens: int = 4000, temperature: float = 0.7) -> str:\n",
    "        \"\"\"Call OpenAI API with proper version handling and temperature bounds.\"\"\"\n",
    "        try:\n",
    "            # Ensure temperature is within valid bounds\n",
    "            temperature = min(max(temperature, 0.0), 1.0)\n",
    "            \n",
    "            if OPENAI_V1:\n",
    "                response = self.openai_client.chat.completions.create(\n",
    "                    model=\"gpt-4-turbo-preview\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_message},\n",
    "                        {\"role\": \"user\", \"content\": user_message}\n",
    "                    ],\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=temperature,\n",
    "                    presence_penalty=0.3,\n",
    "                    frequency_penalty=0.3\n",
    "                )\n",
    "                return response.choices[0].message.content.strip()\n",
    "            else:\n",
    "                response = openai.ChatCompletion.create(\n",
    "                    model=\"gpt-4-turbo-preview\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_message},\n",
    "                        {\"role\": \"user\", \"content\": user_message}\n",
    "                    ],\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=temperature,\n",
    "                    presence_penalty=0.3,\n",
    "                    frequency_penalty=0.3\n",
    "                )\n",
    "                return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"OpenAI API call failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_qa_pairs(self, \n",
    "                         context: str, \n",
    "                         triples: List[Tuple], \n",
    "                         question_type: QuestionType, \n",
    "                         num_questions: int,\n",
    "                         chunk_id: str) -> List[QAPair]:\n",
    "        \"\"\"Generate QA pairs using one-shot learning approach with basic filtering system.\"\"\"\n",
    "        qa_pairs = []\n",
    "        max_retries = 3\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Count API calls (separate from QA attempts)\n",
    "                self.generation_stats[\"total_api_calls\"] += 1\n",
    "                \n",
    "                prompt = self.generate_one_shot_prompt(context, triples, question_type, num_questions)\n",
    "                \n",
    "                system_message = \"\"\"You are an expert at generating domain-specific QA pairs using one-shot learning with exemplar guidance. \n",
    "\n",
    "You have been provided with a high-quality exemplar that demonstrates the expected format and quality for each question type. Use this exemplar as a guide to generate similar high-quality questions for the new context provided.\n",
    "\n",
    "Your output MUST be valid JSON in the exact format specified. Generate questions that demonstrate understanding of the domain relationships and entities present in the provided knowledge graph, following the pattern established by the exemplar.\n",
    "\n",
    "IMPORTANT: In the qa_metadata field, accurately list the specific entities and relationships that you mention in your question and answer.\"\"\"\n",
    "\n",
    "                logger.debug(f\"Generating {num_questions} {question_type.value} questions (attempt {attempt + 1})\")\n",
    "                \n",
    "                # Temperature progression with bounds\n",
    "                temperature = min(0.8 + (attempt * 0.1), 1.0)\n",
    "                \n",
    "                text_response = self.call_openai_api(\n",
    "                    system_message, \n",
    "                    prompt, \n",
    "                    temperature=temperature\n",
    "                )\n",
    "                \n",
    "                parsed_items = self.parse_json_response(text_response)\n",
    "                \n",
    "                if parsed_items:\n",
    "                    logger.info(f\"Successfully parsed {len(parsed_items)} QA pairs from one-shot response\")\n",
    "                    \n",
    "                    for item in parsed_items:\n",
    "                        q_text = item.get(\"question\", \"\").strip()\n",
    "                        a_text = item.get(\"answer\", \"\").strip()\n",
    "                        qa_metadata = item.get(\"qa_metadata\", {\n",
    "                            \"mentioned_entities\": [],\n",
    "                            \"mentioned_relationships\": []\n",
    "                        })\n",
    "\n",
    "                        if q_text and a_text:\n",
    "                            # Apply filtering (statistics are handled inside filter_qa_pair)\n",
    "                            accepted, filtering_result = self.filter_qa_pair(\n",
    "                                q_text, a_text, context, triples, qa_metadata\n",
    "                            )\n",
    "                            \n",
    "                            if accepted:\n",
    "                                unique_id = f\"{question_type.value}_oneshot_{chunk_id}_{int(time.time())}_{str(uuid.uuid4())[:8]}\"\n",
    "                                qa_pair = QAPair(\n",
    "                                    id=unique_id,\n",
    "                                    question=q_text,\n",
    "                                    answer=a_text,\n",
    "                                    question_type=question_type,\n",
    "                                    qa_metadata=qa_metadata,\n",
    "                                    filtering_result=filtering_result,\n",
    "                                    generation_method=\"one_shot\",\n",
    "                                    source_context=context,\n",
    "                                    source_triples=triples.copy(),\n",
    "                                    chunk_id=chunk_id\n",
    "                                )\n",
    "                                \n",
    "                                qa_pairs.append(qa_pair)\n",
    "                                \n",
    "                                logger.debug(f\"Accepted QA pair: {filtering_result.decision.value} \"\n",
    "                                           f\"(relevance={filtering_result.relevance_score:.3f})\")\n",
    "                                \n",
    "                                if len(qa_pairs) >= num_questions:\n",
    "                                    return qa_pairs[:num_questions]\n",
    "                            else:\n",
    "                                logger.debug(f\"Rejected QA pair: {filtering_result.reasoning}\")\n",
    "                        else:\n",
    "                            # Handle parsing failures - empty question or answer\n",
    "                            self.generation_stats[\"total_qa_attempts\"] += 1\n",
    "                            self.generation_stats[\"filtering_decisions\"][\"rejected_parsing\"] += 1\n",
    "                            \n",
    "                            # Store failed parsing attempt\n",
    "                            dummy_result = FilteringResult(\n",
    "                                accepted=False,\n",
    "                                decision=FilteringDecision.REJECTED_PARSING,\n",
    "                                relevance_score=0.0,\n",
    "                                context_similarity=0.0,\n",
    "                                triple_similarity=0.0,\n",
    "                                reasoning=\"Empty question or answer from parsing\",\n",
    "                                metadata={}\n",
    "                            )\n",
    "                            \n",
    "                            self.generation_stats[\"all_attempts\"].append({\n",
    "                                \"question\": q_text,\n",
    "                                \"answer\": a_text,\n",
    "                                \"filtering_result\": self._filtering_result_to_dict(dummy_result),\n",
    "                                \"relevance_score\": 0.0\n",
    "                            })\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in generation attempt {attempt + 1}: {str(e)}\")\n",
    "                time.sleep(min(2 ** attempt, 10))  # Exponential backoff with cap\n",
    "                continue\n",
    "\n",
    "        return qa_pairs\n",
    "\n",
    "    def create_one_shot_dataset(self, \n",
    "                               merged_grouped: Dict, \n",
    "                               output_file: str, \n",
    "                               limit: Optional[int] = None) -> None:\n",
    "        \"\"\"Create a complete one-shot QA dataset with basic filtering.\"\"\"\n",
    "        logger.info(\"Starting one-shot QA dataset creation with exemplar guidance\")\n",
    "        \n",
    "        all_qa_pairs = []\n",
    "        question_types = list(QuestionType)\n",
    "        \n",
    "        # Process chunks\n",
    "        chunks_to_process = list(merged_grouped.items())\n",
    "        if limit:\n",
    "            chunks_to_process = chunks_to_process[:limit]\n",
    "            \n",
    "        total_chunks = len(chunks_to_process)\n",
    "        logger.info(f\"Processing {total_chunks} chunks with one-shot learning\")\n",
    "        \n",
    "        for i, (chunk_id, chunk_data) in enumerate(chunks_to_process, 1):\n",
    "            logger.info(f\"Processing chunk {i}/{total_chunks}: {chunk_id}\")\n",
    "            \n",
    "            # Reset batch tracking for each chunk if specified\n",
    "            if self.reset_duplicates_per_chunk:\n",
    "                self.batch_questions.clear()\n",
    "                self.batch_embeddings.clear()\n",
    "            \n",
    "            context = chunk_data['text']\n",
    "            triples = chunk_data['triples']\n",
    "            \n",
    "            if not triples:\n",
    "                logger.warning(f\"No triples found for chunk {chunk_id}, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Generate QA pairs for each question type using one-shot learning\n",
    "            for question_type in question_types:\n",
    "                try:\n",
    "                    logger.debug(f\"Generating {question_type.value} questions for chunk {chunk_id} using one-shot learning\")\n",
    "                    \n",
    "                    qa_pairs = self.generate_qa_pairs(\n",
    "                        context=context,\n",
    "                        triples=triples,\n",
    "                        question_type=question_type,\n",
    "                        num_questions=2,\n",
    "                        chunk_id=chunk_id\n",
    "                    )\n",
    "                    \n",
    "                    all_qa_pairs.extend(qa_pairs)\n",
    "                    logger.info(f\"Generated {len(qa_pairs)} {question_type.value} QA pairs for chunk {chunk_id}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error generating {question_type.value} questions for chunk {chunk_id}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Add delay between chunks to avoid rate limiting\n",
    "            if i < total_chunks:\n",
    "                time.sleep(2)\n",
    "        \n",
    "        # Get embedding model name safely\n",
    "        try:\n",
    "            if hasattr(self.embedding_model, 'model_name'):\n",
    "                embedding_model_name = self.embedding_model.model_name\n",
    "            elif hasattr(self.embedding_model, '_model_name'):\n",
    "                embedding_model_name = self.embedding_model._model_name\n",
    "            elif hasattr(self.embedding_model, 'config') and hasattr(self.embedding_model.config, 'name_or_path'):\n",
    "                embedding_model_name = self.embedding_model.config.name_or_path\n",
    "            else:\n",
    "                try:\n",
    "                    embedding_model_name = self.embedding_model._modules['0'].auto_model.config.name_or_path\n",
    "                except:\n",
    "                    embedding_model_name = 'BAAI/bge-large-en-v1.5'\n",
    "        except:\n",
    "            embedding_model_name = 'BAAI/bge-large-en-v1.5'\n",
    "        \n",
    "        # Calculate question type statistics\n",
    "        question_type_stats = {}\n",
    "        question_type_quality = {}\n",
    "        \n",
    "        for qa_pair in all_qa_pairs:\n",
    "            q_type = qa_pair.question_type.value\n",
    "            \n",
    "            # Count by type\n",
    "            if q_type not in question_type_stats:\n",
    "                question_type_stats[q_type] = {\n",
    "                    \"count\": 0,\n",
    "                    \"avg_relevance_score\": 0.0,\n",
    "                    \"avg_question_length\": 0.0,\n",
    "                    \"avg_answer_length\": 0.0,\n",
    "                    \"avg_context_similarity\": 0.0,\n",
    "                    \"avg_triple_similarity\": 0.0\n",
    "                }\n",
    "                question_type_quality[q_type] = {\n",
    "                    \"relevance_scores\": [],\n",
    "                    \"question_lengths\": [],\n",
    "                    \"answer_lengths\": [],\n",
    "                    \"context_similarities\": [],\n",
    "                    \"triple_similarities\": []\n",
    "                }\n",
    "            \n",
    "            question_type_stats[q_type][\"count\"] += 1\n",
    "            \n",
    "            # Collect quality metrics\n",
    "            question_type_quality[q_type][\"relevance_scores\"].append(qa_pair.filtering_result.relevance_score)\n",
    "            question_type_quality[q_type][\"question_lengths\"].append(len(qa_pair.question.split()))\n",
    "            question_type_quality[q_type][\"answer_lengths\"].append(len(qa_pair.answer.split()))\n",
    "            question_type_quality[q_type][\"context_similarities\"].append(qa_pair.filtering_result.context_similarity)\n",
    "            question_type_quality[q_type][\"triple_similarities\"].append(qa_pair.filtering_result.triple_similarity)\n",
    "        \n",
    "        # Calculate averages for each question type\n",
    "        for q_type in question_type_stats:\n",
    "            metrics = question_type_quality[q_type]\n",
    "            question_type_stats[q_type][\"avg_relevance_score\"] = np.mean(metrics[\"relevance_scores\"]) if metrics[\"relevance_scores\"] else 0.0\n",
    "            question_type_stats[q_type][\"avg_question_length\"] = np.mean(metrics[\"question_lengths\"]) if metrics[\"question_lengths\"] else 0.0\n",
    "            question_type_stats[q_type][\"avg_answer_length\"] = np.mean(metrics[\"answer_lengths\"]) if metrics[\"answer_lengths\"] else 0.0\n",
    "            question_type_stats[q_type][\"avg_context_similarity\"] = np.mean(metrics[\"context_similarities\"]) if metrics[\"context_similarities\"] else 0.0\n",
    "            question_type_stats[q_type][\"avg_triple_similarity\"] = np.mean(metrics[\"triple_similarities\"]) if metrics[\"triple_similarities\"] else 0.0\n",
    "\n",
    "        # Create final dataset structure with enhanced metadata\n",
    "        dataset = {\n",
    "            \"metadata\": {\n",
    "                \"creation_date\": pd.Timestamp.now().isoformat(),\n",
    "                \"total_queries\": len(all_qa_pairs),\n",
    "                \"generation_method\": \"one_shot\",\n",
    "                \"model_version\": \"1.0\",\n",
    "                \"embedding_model\": embedding_model_name,\n",
    "                \"filtering_approach\": \"essential_quality_filters\",\n",
    "                \"exemplar_info\": {\n",
    "                    \"source_context_length\": len(ONE_SHOT_EXEMPLAR[\"context\"]),\n",
    "                    \"num_exemplar_triples\": len(ONE_SHOT_EXEMPLAR[\"knowledge_graph\"]),\n",
    "                    \"exemplar_question_types\": [q_type.value for q_type in ONE_SHOT_EXEMPLAR[\"exemplar_questions\"].keys()]\n",
    "                },\n",
    "                \"quality_thresholds\": {\n",
    "                    \"min_question_words\": self.thresholds.min_question_words,\n",
    "                    \"min_answer_words\": self.thresholds.min_answer_words,\n",
    "                    \"duplicate_similarity_threshold\": self.thresholds.duplicate_similarity_threshold,\n",
    "                    \"batch_similarity_threshold\": self.thresholds.batch_similarity_threshold\n",
    "                },\n",
    "                \"generation_statistics\": self.get_filtering_statistics(),\n",
    "                \"question_type_breakdown\": question_type_stats,\n",
    "                \"processing_summary\": {\n",
    "                    \"chunks_processed\": total_chunks,\n",
    "                    \"questions_per_chunk\": 2 * len(question_types),  # 2 per question type\n",
    "                    \"question_types_generated\": list(question_type_stats.keys()),\n",
    "                    \"overall_quality\": {\n",
    "                        \"avg_relevance_score\": np.mean([qa.filtering_result.relevance_score for qa in all_qa_pairs]) if all_qa_pairs else 0.0,\n",
    "                        \"avg_question_length\": np.mean([len(qa.question.split()) for qa in all_qa_pairs]) if all_qa_pairs else 0.0,\n",
    "                        \"avg_answer_length\": np.mean([len(qa.answer.split()) for qa in all_qa_pairs]) if all_qa_pairs else 0.0,\n",
    "                        \"avg_context_similarity\": np.mean([qa.filtering_result.context_similarity for qa in all_qa_pairs]) if all_qa_pairs else 0.0,\n",
    "                        \"avg_triple_similarity\": np.mean([qa.filtering_result.triple_similarity for qa in all_qa_pairs]) if all_qa_pairs else 0.0\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"queries\": []\n",
    "        }\n",
    "        \n",
    "        # Convert QA pairs to dictionary format\n",
    "        for qa_pair in all_qa_pairs:\n",
    "            query_dict = {\n",
    "                \"id\": qa_pair.id,\n",
    "                \"question\": qa_pair.question,\n",
    "                \"answer\": qa_pair.answer,\n",
    "                \"question_type\": qa_pair.question_type.value,\n",
    "                \"qa_metadata\": qa_pair.qa_metadata,\n",
    "                \"filtering_result\": {\n",
    "                    \"accepted\": qa_pair.filtering_result.accepted,\n",
    "                    \"decision\": qa_pair.filtering_result.decision.value,\n",
    "                    \"relevance_score\": qa_pair.filtering_result.relevance_score,\n",
    "                    \"context_similarity\": qa_pair.filtering_result.context_similarity,\n",
    "                    \"triple_similarity\": qa_pair.filtering_result.triple_similarity,\n",
    "                    \"reasoning\": qa_pair.filtering_result.reasoning,\n",
    "                    \"metadata\": qa_pair.filtering_result.metadata\n",
    "                },\n",
    "                \"generation_method\": qa_pair.generation_method,\n",
    "                # Add ground truth information for evaluation\n",
    "                \"ground_truth\": {\n",
    "                    \"source_context\": qa_pair.source_context,\n",
    "                    \"source_triples\": [\n",
    "                        {\n",
    "                            \"subject\": triple[0],\n",
    "                            \"predicate\": triple[1], \n",
    "                            \"object\": triple[2]\n",
    "                        } for triple in qa_pair.source_triples if len(triple) >= 3\n",
    "                    ],\n",
    "                    \"chunk_id\": qa_pair.chunk_id,\n",
    "                    \"context_length_chars\": len(qa_pair.source_context),\n",
    "                    \"num_source_triples\": len(qa_pair.source_triples)\n",
    "                }\n",
    "            }\n",
    "            dataset[\"queries\"].append(query_dict)\n",
    "        \n",
    "        # Save dataset\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(dataset, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        logger.info(f\"One-shot dataset creation completed!\")\n",
    "        logger.info(f\"Total QA pairs generated: {len(all_qa_pairs)}\")\n",
    "        logger.info(f\"Dataset saved to: {output_file}\")\n",
    "        \n",
    "        # Log detailed statistics\n",
    "        stats = self.get_filtering_statistics()\n",
    "        logger.info(\"\\nOne-Shot Basic Filtering Statistics:\")\n",
    "        logger.info(f\"Total API calls: {stats.get('total_api_calls', 0)}\")\n",
    "        logger.info(f\"Total QA attempts: {stats.get('total_qa_attempts', 0)}\")\n",
    "        logger.info(f\"Successful generations: {stats.get('successful_generations', 0)}\")\n",
    "        \n",
    "        logger.info(\"\\nFiltering Decision Breakdown:\")\n",
    "        for decision_type, count in stats[\"filtering_decisions\"].items():\n",
    "            if count > 0:\n",
    "                percentage = (count / stats['total_qa_attempts']) * 100 if stats['total_qa_attempts'] > 0 else 0\n",
    "                logger.info(f\"  {decision_type}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        if \"acceptance_rate\" in stats:\n",
    "            logger.info(f\"\\nOverall Acceptance Rate: {stats['acceptance_rate']:.1f}%\")\n",
    "        \n",
    "        # Log question type breakdown\n",
    "        logger.info(\"\\nQuestion Type Breakdown:\")\n",
    "        for q_type, type_stats in question_type_stats.items():\n",
    "            count = type_stats[\"count\"]\n",
    "            avg_relevance = type_stats[\"avg_relevance_score\"]\n",
    "            percentage = (count / len(all_qa_pairs)) * 100 if all_qa_pairs else 0\n",
    "            logger.info(f\"  {q_type}: {count} questions ({percentage:.1f}%) - avg relevance: {avg_relevance:.3f}\")\n",
    "        \n",
    "        # Validation check\n",
    "        total_decisions = sum(stats[\"filtering_decisions\"].values())\n",
    "        if total_decisions == stats[\"total_qa_attempts\"]:\n",
    "            logger.info(\"\\n Statistics validation: PASSED\")\n",
    "        else:\n",
    "            logger.warning(f\"\\n Statistics validation: FAILED ({total_decisions} decisions vs {stats['total_qa_attempts']} attempts)\")\n",
    "\n",
    "    def get_filtering_statistics(self) -> Dict:\n",
    "        \"\"\"Get comprehensive statistics about filtering decisions.\"\"\"\n",
    "        stats = self.generation_stats.copy()\n",
    "        \n",
    "        # Calculate acceptance rate based on QA pairs processed\n",
    "        total_qa_attempts = stats[\"total_qa_attempts\"]\n",
    "        if total_qa_attempts > 0:\n",
    "            acceptance_rate = (stats[\"successful_generations\"] / total_qa_attempts) * 100\n",
    "            stats[\"acceptance_rate\"] = acceptance_rate\n",
    "        else:\n",
    "            stats[\"acceptance_rate\"] = 0.0\n",
    "        \n",
    "        # Add validation check\n",
    "        total_decisions = sum(stats[\"filtering_decisions\"].values())\n",
    "        if total_decisions != total_qa_attempts:\n",
    "            logger.warning(f\"Statistics mismatch: {total_decisions} decisions vs {total_qa_attempts} attempts\")\n",
    "            stats[\"validation_status\"] = \"FAILED\"\n",
    "        else:\n",
    "            stats[\"validation_status\"] = \"PASSED\"\n",
    "        \n",
    "        return stats\n",
    "\n",
    "\n",
    "# Analysis and utility functions\n",
    "\n",
    "def analyze_one_shot_filtering_results(dataset_path: str, generator_stats: Optional[Dict] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze the results of one-shot filtering using both dataset and generation statistics.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to the generated dataset\n",
    "        generator_stats: Optional generator statistics for complete analysis\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "            dataset = json.load(f)\n",
    "        \n",
    "        queries = dataset.get(\"queries\", [])\n",
    "        metadata = dataset.get(\"metadata\", {})\n",
    "        \n",
    "        if not queries:\n",
    "            return {\"error\": \"No queries found in dataset\"}\n",
    "        \n",
    "        # Get actual generation statistics from metadata if available\n",
    "        generation_stats = metadata.get(\"generation_statistics\", {})\n",
    "        if generator_stats:\n",
    "            generation_stats = generator_stats\n",
    "        \n",
    "        # Analyze accepted queries (those in the final dataset)\n",
    "        accepted_count = len(queries)\n",
    "        \n",
    "        # Calculate quality metrics for accepted queries\n",
    "        quality_metrics = {\n",
    "            \"relevance_scores\": [],\n",
    "            \"question_lengths\": [],\n",
    "            \"answer_lengths\": [],\n",
    "            \"context_similarities\": [],\n",
    "            \"triple_similarities\": []\n",
    "        }\n",
    "        \n",
    "        question_type_distribution = {}\n",
    "        \n",
    "        for query in queries:\n",
    "            filtering_info = query.get(\"filtering_result\", {})\n",
    "            quality_metrics[\"relevance_scores\"].append(filtering_info.get(\"relevance_score\", 0.0))\n",
    "            quality_metrics[\"question_lengths\"].append(len(query[\"question\"].split()))\n",
    "            quality_metrics[\"answer_lengths\"].append(len(query[\"answer\"].split()))\n",
    "            quality_metrics[\"context_similarities\"].append(filtering_info.get(\"context_similarity\", 0.0))\n",
    "            quality_metrics[\"triple_similarities\"].append(filtering_info.get(\"triple_similarity\", 0.0))\n",
    "            \n",
    "            # Count question types\n",
    "            q_type = query.get(\"question_type\", \"unknown\")\n",
    "            question_type_distribution[q_type] = question_type_distribution.get(q_type, 0) + 1\n",
    "        \n",
    "        # Use generation statistics for complete picture\n",
    "        total_attempts = generation_stats.get(\"total_qa_attempts\", accepted_count)\n",
    "        rejected_count = total_attempts - accepted_count\n",
    "        \n",
    "        # Calculate comprehensive statistics\n",
    "        analysis_results = {\n",
    "            \"total_qa_attempts\": total_attempts,\n",
    "            \"final_dataset_queries\": accepted_count,\n",
    "            \"decision_distribution\": {\n",
    "                \"accepted_count\": accepted_count,\n",
    "                \"rejected_count\": rejected_count,\n",
    "                \"acceptance_rate\": (accepted_count / total_attempts) * 100 if total_attempts > 0 else 0\n",
    "            },\n",
    "            \"detailed_filtering_breakdown\": generation_stats.get(\"filtering_decisions\", {}),\n",
    "            \"question_type_distribution\": question_type_distribution,\n",
    "            \"quality_statistics\": {\n",
    "                \"avg_relevance_score\": np.mean(quality_metrics[\"relevance_scores\"]) if quality_metrics[\"relevance_scores\"] else 0,\n",
    "                \"min_relevance_score\": min(quality_metrics[\"relevance_scores\"]) if quality_metrics[\"relevance_scores\"] else 0,\n",
    "                \"max_relevance_score\": max(quality_metrics[\"relevance_scores\"]) if quality_metrics[\"relevance_scores\"] else 0,\n",
    "                \"std_relevance_score\": np.std(quality_metrics[\"relevance_scores\"]) if quality_metrics[\"relevance_scores\"] else 0,\n",
    "                \"avg_question_length\": np.mean(quality_metrics[\"question_lengths\"]) if quality_metrics[\"question_lengths\"] else 0,\n",
    "                \"avg_answer_length\": np.mean(quality_metrics[\"answer_lengths\"]) if quality_metrics[\"answer_lengths\"] else 0,\n",
    "                \"avg_context_similarity\": np.mean(quality_metrics[\"context_similarities\"]) if quality_metrics[\"context_similarities\"] else 0,\n",
    "                \"avg_triple_similarity\": np.mean(quality_metrics[\"triple_similarities\"]) if quality_metrics[\"triple_similarities\"] else 0\n",
    "            },\n",
    "            \"generation_efficiency\": {\n",
    "                \"api_calls\": generation_stats.get(\"total_api_calls\", 0),\n",
    "                \"qa_per_api_call\": accepted_count / generation_stats.get(\"total_api_calls\", 1),\n",
    "                \"acceptance_rate\": generation_stats.get(\"acceptance_rate\", 0)\n",
    "            },\n",
    "            \"exemplar_effectiveness\": metadata.get(\"exemplar_info\", {})\n",
    "        }\n",
    "        \n",
    "        return analysis_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error analyzing one-shot filtering results: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "def generate_one_shot_report(dataset_path: str, output_report_path: str, generator_stats: Optional[Dict] = None) -> None:\n",
    "    \"\"\"\n",
    "    Generate a comprehensive report on one-shot filtering performance.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to the one-shot filtering dataset\n",
    "        output_report_path: Path to save the filtering report\n",
    "        generator_stats: Optional generator statistics for complete analysis\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Analyze with complete statistics\n",
    "        analysis = analyze_one_shot_filtering_results(dataset_path, generator_stats)\n",
    "        \n",
    "        if \"error\" in analysis:\n",
    "            logger.error(f\"Analysis failed: {analysis['error']}\")\n",
    "            return\n",
    "        \n",
    "        # Generate comprehensive report\n",
    "        report = {\n",
    "            \"report_metadata\": {\n",
    "                \"generation_date\": pd.Timestamp.now().isoformat(),\n",
    "                \"dataset_analyzed\": dataset_path,\n",
    "                \"report_type\": \"One_Shot_QA_Analysis\",\n",
    "                \"approach\": \"Exemplar-guided generation with essential quality filtering\"\n",
    "            },\n",
    "            \"executive_summary\": {\n",
    "                \"total_qa_attempts\": analysis.get(\"total_qa_attempts\", 0),\n",
    "                \"final_questions_generated\": analysis.get(\"final_dataset_queries\", 0),\n",
    "                \"overall_acceptance_rate\": f\"{analysis.get('decision_distribution', {}).get('acceptance_rate', 0):.1f}%\",\n",
    "                \"avg_relevance_score\": f\"{analysis.get('quality_statistics', {}).get('avg_relevance_score', 0):.3f}\",\n",
    "                \"api_efficiency\": f\"{analysis.get('generation_efficiency', {}).get('qa_per_api_call', 0):.1f} QA pairs per API call\"\n",
    "            },\n",
    "            \"detailed_analysis\": analysis,\n",
    "            \"one_shot_approach\": {\n",
    "                \"method\": \"Exemplar-guided generation using high-quality template questions\",\n",
    "                \"exemplar_source\": \"Manually constructed regulatory text with KG triples\",\n",
    "                \"question_types_covered\": [\"factual\", \"relationship\", \"comparative\", \"inferential\"],\n",
    "                \"filtering_approach\": \"Essential quality filters: length requirements + duplicate detection\"\n",
    "            }\n",
    "            \n",
    "        }\n",
    "        \n",
    "        # Save report\n",
    "        with open(output_report_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        logger.info(f\"One-shot filtering report generated: {output_report_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating one-shot filtering report: {e}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate the one-shot learning QA generation system.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting One-Shot QA Generation with Exemplar Guidance\")\n",
    "    \n",
    "    try:\n",
    "        # Get API key\n",
    "        openai_api_key = input(\"Please enter your OpenAI API key: \").strip()\n",
    "        \n",
    "        if not openai_api_key or openai_api_key == \"your-openai-api-key-here\":\n",
    "            logger.error(\"Please provide a valid OpenAI API key\")\n",
    "            return\n",
    "        \n",
    "        # Initialize generator with one-shot learning\n",
    "        generator = OneShotQAGenerator(\n",
    "            openai_api_key=openai_api_key,\n",
    "            reset_duplicates_per_chunk=True,\n",
    "            embedding_model='BAAI/bge-large-en-v1.5',\n",
    "            max_embedding_cache=1000\n",
    "        )\n",
    "\n",
    "        # File paths\n",
    "        chunks_file = input(\"Enter path to chunks CSV file (or press Enter for 'chunks.csv'): \").strip() or \"chunks.csv\"\n",
    "        triples_file = input(\"Enter path to triples CSV file (or press Enter for 'Ontology_Guided_Triples.csv'): \").strip() or \"Ontology_Guided_Triples.csv\"\n",
    "        output_file = input(\"Enter output file name (or press Enter for 'One-Shot_qa_dataset.json'): \").strip() or \"One-Shot_qa_dataset.json\"\n",
    "\n",
    "        # Check if files exist\n",
    "        if not os.path.exists(chunks_file):\n",
    "            logger.error(f\"Chunks file not found: {chunks_file}\")\n",
    "            logger.info(\"Please ensure your chunks file exists and has the correct path\")\n",
    "            return\n",
    "            \n",
    "        if not os.path.exists(triples_file):\n",
    "            logger.error(f\"Triples file not found: {triples_file}\")\n",
    "            logger.info(\"Please ensure your triples file exists and has the correct path\")\n",
    "            return\n",
    "\n",
    "        # Load and merge data\n",
    "        logger.info(\"Loading and merging data files\")\n",
    "        merged_data = generator.load_and_merge_data(chunks_file, triples_file)\n",
    "\n",
    "        if not merged_data:\n",
    "            logger.error(\"No data loaded. Please check your input files.\")\n",
    "            return\n",
    "        # Get user input for number of chunks to process\n",
    "        try:\n",
    "            limit = input(\"Enter number of chunks to process (or press Enter for all): \").strip()\n",
    "            limit = int(limit) if limit else None\n",
    "        except ValueError:\n",
    "            limit = None\n",
    "            logger.info(\"Using default: processing all chunks\")\n",
    "        \n",
    "        if limit is None:\n",
    "            logger.info(\"Processing all chunks\")\n",
    "        else:\n",
    "            logger.info(f\"Processing {limit} chunks\")\n",
    "        \n",
    "        # Create one-shot learning dataset\n",
    "        logger.info(f\"Creating One-Shot QA dataset using exemplar guidance\")\n",
    "        generator.create_one_shot_dataset(\n",
    "            merged_grouped=merged_data,\n",
    "            output_file=output_file,\n",
    "            limit=limit\n",
    "        )\n",
    "\n",
    "        # Get final statistics from generator\n",
    "        final_stats = generator.get_filtering_statistics()\n",
    "\n",
    "        # Generate analysis report\n",
    "        logger.info(\"Analyzing one-shot filtering results\")\n",
    "        analysis_results = analyze_one_shot_filtering_results(output_file, final_stats)\n",
    "        \n",
    "        if \"error\" in analysis_results:\n",
    "            logger.error(f\"Analysis failed: {analysis_results['error']}\")\n",
    "        else:\n",
    "            logger.info(\"One-Shot Filtering Results:\")\n",
    "            logger.info(f\"  Total QA attempts: {analysis_results.get('total_qa_attempts', 0)}\")\n",
    "            logger.info(f\"  Final questions generated: {analysis_results.get('final_dataset_queries', 0)}\")\n",
    "            logger.info(f\"  Overall acceptance rate: {analysis_results.get('decision_distribution', {}).get('acceptance_rate', 0):.1f}%\")\n",
    "            logger.info(f\"  Average relevance score: {analysis_results.get('quality_statistics', {}).get('avg_relevance_score', 0):.3f}\")\n",
    "\n",
    "        # Generate comprehensive report\n",
    "        report_file = \"One_Shot_QA_analysis_report.json\"\n",
    "        generate_one_shot_report(output_file, report_file, final_stats)\n",
    "        \n",
    "        # Display final statistics\n",
    "        logger.info(\"\\nFINAL One-Shot Statistics:\")\n",
    "        logger.info(f\"Total API calls: {final_stats.get('total_api_calls', 0)}\")\n",
    "        logger.info(f\"Total QA attempts: {final_stats.get('total_qa_attempts', 0)}\")\n",
    "        logger.info(f\"Successful generations: {final_stats.get('successful_generations', 0)}\")\n",
    "\n",
    "        logger.info(\"\\nFiltering Decision Breakdown:\")\n",
    "        for decision_type, count in final_stats[\"filtering_decisions\"].items():\n",
    "            if count > 0:\n",
    "                percentage = (count / final_stats['total_qa_attempts']) * 100 if final_stats['total_qa_attempts'] > 0 else 0\n",
    "                logger.info(f\"  {decision_type}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        if \"acceptance_rate\" in final_stats:\n",
    "            logger.info(f\"\\nOverall Acceptance Rate: {final_stats['acceptance_rate']:.1f}%\")\n",
    "        \n",
    "        # Validation check\n",
    "        validation_status = final_stats.get(\"validation_status\", \"UNKNOWN\")\n",
    "        logger.info(f\"Statistics validation: {validation_status}\")\n",
    "        \n",
    "        logger.info(f\"\\nOutputs generated:\")\n",
    "        logger.info(f\"  Dataset: {output_file}\")\n",
    "        logger.info(f\"  Analysis Report: {report_file}\")\n",
    "        logger.info(\"\\nOne-Shot QA generation completed successfully!\")\n",
    "        logger.info(\"\\nOne-Shot Learning Features:\")\n",
    "        logger.info(\"   ✓ Exemplar-guided generation for consistent quality\")\n",
    "        logger.info(\"   ✓ High-quality template questions for each type\")\n",
    "        logger.info(\"   ✓ Essential filtering: length + duplicate detection\")\n",
    "        logger.info(\"   ✓ Balanced coverage across question types\")\n",
    "        logger.info(\"   ✓ Domain-specific pattern learning\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"\\nProcess interrupted by user\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main execution: {str(e)}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
