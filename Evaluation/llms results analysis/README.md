
## üìä LLM Results Analysis

This module provides a comprehensive statistical analysis of QA evaluation results generated by five LLMs acting as judges, based on the NeuReg QA datasets. It aggregates model scores, compares QA performance across shot types and question categories, and quantifies agreement among models using majority voting strategies.

---
### üîß Overall Performance Summary

This table shows the mean performance across all models, shots, and question types

| Metric            | Mean Score | Std Dev | Mean %     |
| ----------------- | ---------- | ------- | ---------- |
| Relevance         | 4.91       | 0.31    | 98.18%     |
| Accuracy          | 4.79       | 0.43    | 95.79%     |
| Completeness      | 4.44       | 0.67    | 88.82%     |
| Fluency           | 4.96       | 0.19    | 99.23%     |
| KG Alignment      | 4.74       | 0.51    | 94.86%     |
| **Overall Score** | **4.77**   | 0.34    | **95.38%** |

---
### üó≥Ô∏è Majority Vote Agreement

Across 3,303 unique QA IDs:

All Metrics Majority Agreement: 2,744

Some Metrics Majority: 559

No Majority: 0

Metric-wise full + majority agreement:

| Metric       | Full Agreement | Majority Exists | No Majority |
| ------------ | -------------- | --------------- | ----------- |
| Relevance    | 2118           | 1172            | 13          |
| Accuracy     | 1452           | 1824            | 27          |
| Completeness | 164            | 2664            | 475         |
| Fluency      | 2721           | 582             | 0           |
| KG Alignment | 1376           | 1793            | 134         |

---

### üîç Score Distribution

| Score Range | Count  | % of Total |
| ----------- | ------ | ---------- |
| 1.0‚Äì2.0     | 0      | 0.00%      |
| 2.0‚Äì3.0     | 15     | 0.09%      |
| 3.0‚Äì4.0     | 599    | 3.62%      |
| 4.0‚Äì5.0     | 15,945 | 96.29%     |

---

### üß† Performance by Shot Type

| Shot Type | Mean Score | % ‚â• 4.0 |
| --------- | ---------- | ------- |
| Zero-Shot | 4.77       | 95.42%  |
| One-Shot  | 4.77       | 95.43%  |
| Few-Shot  | 4.76       | 95.28%  |

---

### üß† Performance by Question Type

| Question Type | Mean Score | % ‚â• 4.0 |
| ------------- | ---------- | ------- |
| Factual       | 4.85       | 96.94%  |
| Relationship  | 4.77       | 95.30%  |
| Inferential   | 4.73       | 94.59%  |
| Comparative   | 4.73       | 94.58%  |

---

### Model-Wise Overall Scores

| Model             | Mean Score | Agreement Rate |
| ----------------- | ---------- | -------------- |
| **Mixtral-8x22B** | **4.94**   | **92.86%**     |
| DeepSeek-R1       | 4.89       | 90.79%         |
| Qwen3-32B         | 4.84       | 89.09%         |
| Gemma-2-27B-IT    | 4.68       | 79.14%         |
| Llama-3.3-70B     | 4.50       | 63.90%         |


