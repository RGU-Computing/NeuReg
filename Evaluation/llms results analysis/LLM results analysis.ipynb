{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9162dc-edec-4ca3-8dd9-9bd9fc40e2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "import json\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define files\n",
    "FILES = {\n",
    "    'Mixtral-8x22B': { # Corrected model name to match table\n",
    "        'Zero-Shot': 'mixtral_zeroshot_evaluation_results.csv',\n",
    "        'One-Shot': 'mixtral_oneshot_evaluation_results.csv',\n",
    "        'Few-Shot': 'mixtral_fewshot_evaluation_results.csv'\n",
    "    },\n",
    "    'Llama-3.3-70B': { # Corrected model name to match table\n",
    "        'Zero-Shot': 'Llama_zeroshot_evaluation_results.csv',\n",
    "        'One-Shot': 'Llama_oneshot_evaluation_results.csv',\n",
    "        'Few-Shot': 'Llama_fewshot_evaluation_results.csv'\n",
    "    },\n",
    "    'DeepSeek-R1': { # Corrected model name to match table\n",
    "        'Zero-Shot': 'DeepSeek_zeroshot_evaluation_results.csv',\n",
    "        'One-Shot': 'DeepSeek_oneshot_evaluation_results.csv',\n",
    "        'Few-Shot': 'DeepSeek_fewshot_evaluation_results.csv'\n",
    "    },\n",
    "    'Qwen3-32B': {\n",
    "        'Zero-Shot': 'Qwen_zeroshot_evaluation_results.csv',\n",
    "        'One-Shot': 'Qwen_oneshot_evaluation_results.csv',\n",
    "        'Few-Shot': 'Qwen_fewshot_evaluation_results.csv'\n",
    "    },\n",
    "    'Gemma-2-27B-IT': {\n",
    "        'Zero-Shot': 'gemma_zeroshot_evaluation_results.csv',\n",
    "        'One-Shot': 'gemma_oneshot_evaluation_results.csv',\n",
    "        'Few-Shot': 'gemma_fewshot_evaluation_results.csv'\n",
    "    }\n",
    "}\n",
    "\n",
    "METRICS = ['Relevance', 'Accuracy', 'Completeness', 'Fluency', 'KG_Alignment']\n",
    "NUM_JUDGES = 5 # Define the number of judges\n",
    "\n",
    "class MultiModelEvaluationAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.df = None\n",
    "        self.majority_vote_df = None\n",
    "        self.model_majority_agreement = {} # New attribute to store per-model agreement\n",
    "        self.analysis_report = {\n",
    "            'metadata': {},\n",
    "            'data_loading': {},\n",
    "            'majority_vote_analysis': {},\n",
    "            'overall_metrics': {},\n",
    "            'score_distribution': {},\n",
    "            'shot_type_analysis': {},\n",
    "            'question_type_analysis': {},\n",
    "            'interaction_matrix': {},\n",
    "            'individual_model_analysis': {}\n",
    "        }\n",
    "\n",
    "    def load_all_data(self):\n",
    "        \"\"\"Load all CSV files and combine them into a single DataFrame.\"\"\"\n",
    "        all_data = []\n",
    "        loading_info = {'files_loaded': [], 'files_failed': [], 'total_records': 0}\n",
    "        \n",
    "        for model, shot_types in FILES.items():\n",
    "            for shot_type, filename in shot_types.items():\n",
    "                try:\n",
    "                    df = pd.read_csv(filename)\n",
    "                    df['model'] = model\n",
    "                    df['shot_type'] = shot_type\n",
    "                    all_data.append(df)\n",
    "                    loading_info['files_loaded'].append({\n",
    "                        'filename': filename,\n",
    "                        'model': model,\n",
    "                        'shot_type': shot_type,\n",
    "                        'records': len(df)\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    loading_info['files_failed'].append({\n",
    "                        'filename': filename,\n",
    "                        'model': model,\n",
    "                        'shot_type': shot_type,\n",
    "                        'error': str(e)\n",
    "                    })\n",
    "        \n",
    "        if not all_data:\n",
    "            raise ValueError(\"No data files could be loaded!\")\n",
    "        \n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        loading_info['total_records'] = len(combined_df)\n",
    "        \n",
    "        # Ensure Overall_Score is calculated if not present\n",
    "        if 'Overall_Score' not in combined_df.columns or combined_df['Overall_Score'].isna().any():\n",
    "            combined_df['Overall_Score'] = combined_df[METRICS].mean(axis=1)\n",
    "        \n",
    "        self.df = combined_df\n",
    "        self.analysis_report['data_loading'] = loading_info\n",
    "        \n",
    "        # Compute majority vote dataframe (and now also per-model agreement stats)\n",
    "        self.compute_majority_vote()\n",
    "        \n",
    "        return combined_df\n",
    "    \n",
    "    def compute_majority_vote(self):\n",
    "        \"\"\"\n",
    "        Compute majority vote per QA pair per metric.\n",
    "        In this structure, each model IS a judge, so we need to compare scores across models for the same qa_id.\n",
    "        \"\"\"\n",
    "        majority_analysis = {}\n",
    "        \n",
    "        # In this data structure:\n",
    "        # - Each CSV file = one model's evaluations\n",
    "        # - Each model = one judge\n",
    "        # - For majority vote, we need to compare scores from all models for the same qa_id\n",
    "        \n",
    "        # Group by qa_id to see how many models evaluated each question\n",
    "        qa_id_counts = self.df.groupby('qa_id')['model'].nunique().reset_index(name='num_models')\n",
    "        \n",
    "        # Debug: Show distribution of model counts per qa_id\n",
    "        count_distribution = qa_id_counts['num_models'].value_counts().sort_index()\n",
    "        majority_analysis['model_distribution'] = {\n",
    "            int(model_count): int(instances) \n",
    "            for model_count, instances in count_distribution.items()\n",
    "        }\n",
    "        \n",
    "        # Filter to keep only qa_ids that were evaluated by all 5 models\n",
    "        valid_qa_ids = qa_id_counts[qa_id_counts['num_models'] == NUM_JUDGES]['qa_id'].tolist()\n",
    "        \n",
    "        majority_analysis['total_qa_ids'] = len(qa_id_counts)\n",
    "        majority_analysis['valid_qa_ids'] = len(valid_qa_ids)\n",
    "        \n",
    "        if len(valid_qa_ids) == 0:\n",
    "            majority_analysis['warning'] = f\"No qa_ids were evaluated by all {NUM_JUDGES} models!\"\n",
    "            self.majority_vote_df = pd.DataFrame()\n",
    "            self.model_majority_agreement = {}\n",
    "            self.analysis_report['majority_vote_analysis'] = majority_analysis\n",
    "            return self.majority_vote_df\n",
    "        \n",
    "        majority_vote_data = []\n",
    "        agreement_stats_by_metric = {metric: {'full_agreement': 0, 'majority_exists': 0, 'no_majority': 0} for metric in METRICS}\n",
    "        agreement_stats_by_qa = {'all_metrics_majority': 0, 'some_metrics_majority': 0, 'no_metrics_majority': 0}\n",
    "        \n",
    "        for qa_id in valid_qa_ids:\n",
    "            # Get all evaluations for this qa_id (one from each model/judge)\n",
    "            qa_data = self.df[self.df['qa_id'] == qa_id]\n",
    "            \n",
    "            if len(qa_data) != NUM_JUDGES:\n",
    "                continue  # Skip if we don't have exactly NUM_JUDGES evaluations\n",
    "            \n",
    "            # Get metadata from the first row\n",
    "            first_row = qa_data.iloc[0]\n",
    "            row_data = {\n",
    "                'qa_id': qa_id,\n",
    "                'question_type': first_row['question_type'],\n",
    "                'question': first_row['question'],\n",
    "                'answer': first_row['answer']\n",
    "            }\n",
    "            \n",
    "            metrics_with_majority = 0\n",
    "            \n",
    "            # Compute majority vote for each metric\n",
    "            for metric in METRICS:\n",
    "                # Get scores from all models/judges for this metric\n",
    "                scores = qa_data[metric].values\n",
    "                score_counts = Counter(scores)\n",
    "                \n",
    "                if not score_counts:\n",
    "                    row_data[f'{metric}_majority'] = np.nan\n",
    "                    continue\n",
    "                \n",
    "                # Get the most common score and its count\n",
    "                majority_score, majority_count = score_counts.most_common(1)[0]\n",
    "                row_data[f'{metric}_majority'] = majority_score\n",
    "                row_data[f'{metric}_majority_count'] = majority_count\n",
    "                \n",
    "                # Check for full agreement (all judges gave the same score)\n",
    "                if len(score_counts) == 1:\n",
    "                    agreement_stats_by_metric[metric]['full_agreement'] += 1\n",
    "                    metrics_with_majority += 1\n",
    "                # Check for simple majority (> NUM_JUDGES / 2)\n",
    "                elif majority_count > NUM_JUDGES / 2:\n",
    "                    agreement_stats_by_metric[metric]['majority_exists'] += 1\n",
    "                    metrics_with_majority += 1\n",
    "                else:\n",
    "                    # No clear majority (tie or no score exceeds half)\n",
    "                    agreement_stats_by_metric[metric]['no_majority'] += 1\n",
    "            \n",
    "            # Track QA-level agreement\n",
    "            if metrics_with_majority == len(METRICS):\n",
    "                agreement_stats_by_qa['all_metrics_majority'] += 1\n",
    "            elif metrics_with_majority > 0:\n",
    "                agreement_stats_by_qa['some_metrics_majority'] += 1\n",
    "            else:\n",
    "                agreement_stats_by_qa['no_metrics_majority'] += 1\n",
    "            \n",
    "            # Compute overall score from majority votes\n",
    "            majority_scores = [row_data[f'{metric}_majority'] for metric in METRICS if f'{metric}_majority' in row_data]\n",
    "            row_data['Overall_Score_majority'] = np.mean(majority_scores) if majority_scores else np.nan\n",
    "            \n",
    "            majority_vote_data.append(row_data)\n",
    "        \n",
    "        self.majority_vote_df = pd.DataFrame(majority_vote_data)\n",
    "        \n",
    "        # Store agreement statistics\n",
    "        majority_analysis['agreement_by_metric'] = agreement_stats_by_metric\n",
    "        majority_analysis['agreement_by_qa'] = agreement_stats_by_qa\n",
    "        majority_analysis['total_majority_vote_pairs'] = len(self.majority_vote_df)\n",
    "        \n",
    "        # Calculate per-model agreement statistics (how often each model agrees with majority)\n",
    "        model_agreement_stats = {}\n",
    "        \n",
    "        for model in self.df['model'].unique():\n",
    "            total_comparisons = 0\n",
    "            agreements = 0\n",
    "            \n",
    "            # For each qa_id in majority vote results\n",
    "            for _, maj_row in self.majority_vote_df.iterrows():\n",
    "                qa_id = maj_row['qa_id']\n",
    "                \n",
    "                # Get this model's evaluation for this qa_id\n",
    "                model_eval = self.df[(self.df['qa_id'] == qa_id) & (self.df['model'] == model)]\n",
    "                \n",
    "                if len(model_eval) == 0:\n",
    "                    continue\n",
    "                \n",
    "                model_row = model_eval.iloc[0]\n",
    "                \n",
    "                # Compare model's scores with majority votes\n",
    "                for metric in METRICS:\n",
    "                    if f'{metric}_majority' in maj_row and not pd.isna(maj_row[f'{metric}_majority']):\n",
    "                        majority_vote = maj_row[f'{metric}_majority']\n",
    "                        model_vote = model_row[metric]\n",
    "                        \n",
    "                        total_comparisons += 1\n",
    "                        if model_vote == majority_vote:\n",
    "                            agreements += 1\n",
    "            \n",
    "            if total_comparisons > 0:\n",
    "                agreement_pct = (agreements / total_comparisons) * 100\n",
    "                model_agreement_stats[model] = {\n",
    "                    'agreements': agreements,\n",
    "                    'total': total_comparisons,\n",
    "                    'percentage': agreement_pct\n",
    "                }\n",
    "        \n",
    "        self.model_majority_agreement = model_agreement_stats\n",
    "        majority_analysis['model_agreement_stats'] = model_agreement_stats\n",
    "        self.analysis_report['majority_vote_analysis'] = majority_analysis\n",
    "        \n",
    "        return self.majority_vote_df\n",
    "    \n",
    "    def normalize_to_percentage(self, score, max_score=5):\n",
    "        \"\"\"Convert score to percentage (0-100 scale).\"\"\"\n",
    "        return (score / max_score) * 100\n",
    "    \n",
    "    def compute_overall_metrics(self):\n",
    "        \"\"\"1. Compute mean and std dev per metric across all QA pairs.\"\"\"\n",
    "        results = {}\n",
    "        for metric in METRICS:\n",
    "            mean_score = self.df[metric].mean()\n",
    "            std_score = self.df[metric].std()\n",
    "            mean_pct = self.normalize_to_percentage(mean_score)\n",
    "            \n",
    "            results[metric] = {\n",
    "                'mean': float(mean_score),\n",
    "                'std': float(std_score),\n",
    "                'mean_pct': float(mean_pct)\n",
    "            }\n",
    "        \n",
    "        # Overall score (mean of all metrics)\n",
    "        overall_mean = self.df['Overall_Score'].mean()\n",
    "        overall_std = self.df['Overall_Score'].std()\n",
    "        overall_pct = self.normalize_to_percentage(overall_mean)\n",
    "        \n",
    "        results['Overall_Score'] = {\n",
    "            'mean': float(overall_mean),\n",
    "            'std': float(overall_std),\n",
    "            'mean_pct': float(overall_pct)\n",
    "        }\n",
    "        \n",
    "        self.analysis_report['overall_metrics'] = results\n",
    "        return results\n",
    "    \n",
    "    def compute_overall_score_distribution(self):\n",
    "        \"\"\"2. Compute overall score distribution.\"\"\"\n",
    "        overall_scores = self.df['Overall_Score']\n",
    "        \n",
    "        distribution = {\n",
    "            'statistics': {\n",
    "                'mean': float(overall_scores.mean()),\n",
    "                'std': float(overall_scores.std()),\n",
    "                'min': float(overall_scores.min()),\n",
    "                'max': float(overall_scores.max())\n",
    "            },\n",
    "            'ranges': {}\n",
    "        }\n",
    "        \n",
    "        # Distribution by ranges\n",
    "        ranges = [(1.0, 2.0), (2.0, 3.0), (3.0, 4.0), (4.0, 5.0)]\n",
    "        \n",
    "        for start, end in ranges:\n",
    "            if start == 4.0:  # Include 5.0 in the last range\n",
    "                count = ((overall_scores >= start) & (overall_scores <= end)).sum()\n",
    "            else:\n",
    "                count = ((overall_scores >= start) & (overall_scores < end)).sum()\n",
    "            pct = (count / len(overall_scores)) * 100\n",
    "            \n",
    "            distribution['ranges'][f'{start:.1f}-{end:.1f}'] = {\n",
    "                'count': int(count),\n",
    "                'percentage': float(pct)\n",
    "            }\n",
    "        \n",
    "        self.analysis_report['score_distribution'] = distribution\n",
    "        return distribution\n",
    "    \n",
    "    def compute_breakdown_by_shot_type(self):\n",
    "        \"\"\"3. Breakdown by shot type.\"\"\"\n",
    "        shot_type_results = {}\n",
    "        \n",
    "        for shot_type in sorted(self.df['shot_type'].unique()):\n",
    "            shot_df = self.df[self.df['shot_type'] == shot_type]\n",
    "            count = len(shot_df)\n",
    "            \n",
    "            results = {}\n",
    "            for metric in METRICS:\n",
    "                mean_score = shot_df[metric].mean()\n",
    "                std_score = shot_df[metric].std()\n",
    "                mean_pct = self.normalize_to_percentage(mean_score)\n",
    "                \n",
    "                results[metric] = {\n",
    "                    'mean': float(mean_score), \n",
    "                    'std': float(std_score), \n",
    "                    'mean_pct': float(mean_pct)\n",
    "                }\n",
    "            \n",
    "            # Overall score\n",
    "            overall_mean = shot_df['Overall_Score'].mean()\n",
    "            overall_std = shot_df['Overall_Score'].std()\n",
    "            overall_pct = self.normalize_to_percentage(overall_mean)\n",
    "            \n",
    "            shot_type_results[shot_type] = {\n",
    "                'count': count,\n",
    "                'metrics': results,\n",
    "                'overall': {\n",
    "                    'mean': float(overall_mean), \n",
    "                    'std': float(overall_std), \n",
    "                    'pct': float(overall_pct)\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        self.analysis_report['shot_type_analysis'] = shot_type_results\n",
    "        return shot_type_results\n",
    "    \n",
    "    def compute_breakdown_by_question_type(self):\n",
    "        \"\"\"4. Breakdown by question type.\"\"\"\n",
    "        question_type_results = {}\n",
    "        \n",
    "        for q_type in sorted(self.df['question_type'].unique()):\n",
    "            q_df = self.df[self.df['question_type'] == q_type]\n",
    "            count = len(q_df)\n",
    "            \n",
    "            results = {}\n",
    "            for metric in METRICS:\n",
    "                mean_score = q_df[metric].mean()\n",
    "                std_score = q_df[metric].std()\n",
    "                mean_pct = self.normalize_to_percentage(mean_score)\n",
    "                \n",
    "                results[metric] = {\n",
    "                    'mean': float(mean_score), \n",
    "                    'std': float(std_score), \n",
    "                    'mean_pct': float(mean_pct)\n",
    "                }\n",
    "            \n",
    "            # Overall score\n",
    "            overall_mean = q_df['Overall_Score'].mean()\n",
    "            overall_std = q_df['Overall_Score'].std()\n",
    "            overall_pct = self.normalize_to_percentage(overall_mean)\n",
    "            \n",
    "            question_type_results[q_type] = {\n",
    "                'count': count,\n",
    "                'metrics': results,\n",
    "                'overall': {\n",
    "                    'mean': float(overall_mean), \n",
    "                    'std': float(overall_std), \n",
    "                    'pct': float(overall_pct)\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        self.analysis_report['question_type_analysis'] = question_type_results\n",
    "        return question_type_results\n",
    "    \n",
    "    def compute_shot_type_x_question_type_matrix(self):\n",
    "        \"\"\"5. Create 2D matrix of shot type × question type.\"\"\"\n",
    "        # Create pivot table\n",
    "        pivot_df = self.df.pivot_table(\n",
    "            values='Overall_Score',\n",
    "            index='question_type',\n",
    "            columns='shot_type',\n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        \n",
    "        # Convert to dictionary format for JSON serialization\n",
    "        matrix_data = {}\n",
    "        shot_types = sorted(self.df['shot_type'].unique())\n",
    "        q_types = sorted(self.df['question_type'].unique())\n",
    "        \n",
    "        for q_type in q_types:\n",
    "            matrix_data[q_type] = {}\n",
    "            for shot_type in shot_types:\n",
    "                if q_type in pivot_df.index and shot_type in pivot_df.columns:\n",
    "                    score = pivot_df.loc[q_type, shot_type]\n",
    "                    if pd.notna(score):\n",
    "                        pct = self.normalize_to_percentage(score)\n",
    "                        matrix_data[q_type][shot_type] = {\n",
    "                            'score': float(score),\n",
    "                            'percentage': float(pct)\n",
    "                        }\n",
    "                    else:\n",
    "                        matrix_data[q_type][shot_type] = None\n",
    "                else:\n",
    "                    matrix_data[q_type][shot_type] = None\n",
    "        \n",
    "        self.analysis_report['interaction_matrix'] = matrix_data\n",
    "        return matrix_data\n",
    "    \n",
    "    def compute_individual_model_analysis(self):\n",
    "        \"\"\"6. Individual model analysis by question type and shot type.\"\"\"\n",
    "        models = self.df['model'].unique()\n",
    "        model_analysis = {}\n",
    "        \n",
    "        for model in sorted(models):\n",
    "            model_df = self.df[self.df['model'] == model]\n",
    "            \n",
    "            # 6.1 Overall performance for this model\n",
    "            overall_performance = {}\n",
    "            for metric in METRICS:\n",
    "                mean_score = model_df[metric].mean()\n",
    "                std_score = model_df[metric].std()\n",
    "                mean_pct = self.normalize_to_percentage(mean_score)\n",
    "                overall_performance[metric] = {\n",
    "                    'mean': float(mean_score),\n",
    "                    'std': float(std_score),\n",
    "                    'mean_pct': float(mean_pct)\n",
    "                }\n",
    "            \n",
    "            overall_mean = model_df['Overall_Score'].mean()\n",
    "            overall_std = model_df['Overall_Score'].std()\n",
    "            overall_pct = self.normalize_to_percentage(overall_mean)\n",
    "            overall_performance['Overall_Score'] = {\n",
    "                'mean': float(overall_mean),\n",
    "                'std': float(overall_std),\n",
    "                'mean_pct': float(overall_pct)\n",
    "            }\n",
    "            \n",
    "            # 6.2 By Question Type\n",
    "            by_question_type = {}\n",
    "            for q_type in sorted(model_df['question_type'].unique()):\n",
    "                q_model_df = model_df[model_df['question_type'] == q_type]\n",
    "                count = len(q_model_df)\n",
    "                \n",
    "                overall_mean = q_model_df['Overall_Score'].mean()\n",
    "                overall_pct = self.normalize_to_percentage(overall_mean)\n",
    "                \n",
    "                by_question_type[q_type] = {\n",
    "                    'mean': float(overall_mean),\n",
    "                    'percentage': float(overall_pct),\n",
    "                    'count': count\n",
    "                }\n",
    "            \n",
    "            # 6.3 By Shot Type\n",
    "            by_shot_type = {}\n",
    "            for shot_type in sorted(model_df['shot_type'].unique()):\n",
    "                shot_model_df = model_df[model_df['shot_type'] == shot_type]\n",
    "                count = len(shot_model_df)\n",
    "                \n",
    "                overall_mean = shot_model_df['Overall_Score'].mean()\n",
    "                overall_pct = self.normalize_to_percentage(overall_mean)\n",
    "                \n",
    "                by_shot_type[shot_type] = {\n",
    "                    'mean': float(overall_mean),\n",
    "                    'percentage': float(overall_pct),\n",
    "                    'count': count\n",
    "                }\n",
    "            \n",
    "            # 6.4 Shot Type × Question Type Matrix for this model\n",
    "            pivot_df = model_df.pivot_table(\n",
    "                values='Overall_Score',\n",
    "                index='question_type',\n",
    "                columns='shot_type',\n",
    "                aggfunc='mean'\n",
    "            )\n",
    "            \n",
    "            matrix = {}\n",
    "            if not pivot_df.empty:\n",
    "                shot_types = sorted(model_df['shot_type'].unique())\n",
    "                q_types = sorted(model_df['question_type'].unique())\n",
    "                \n",
    "                for q_type in q_types:\n",
    "                    matrix[q_type] = {}\n",
    "                    for shot_type in shot_types:\n",
    "                        if q_type in pivot_df.index and shot_type in pivot_df.columns:\n",
    "                            score = pivot_df.loc[q_type, shot_type]\n",
    "                            if pd.notna(score):\n",
    "                                pct = self.normalize_to_percentage(score)\n",
    "                                matrix[q_type][shot_type] = {\n",
    "                                    'score': float(score),\n",
    "                                    'percentage': float(pct)\n",
    "                                }\n",
    "                            else:\n",
    "                                matrix[q_type][shot_type] = None\n",
    "                        else:\n",
    "                            matrix[q_type][shot_type] = None\n",
    "            \n",
    "            model_analysis[model] = {\n",
    "                'overall_performance': overall_performance,\n",
    "                'by_question_type': by_question_type,\n",
    "                'by_shot_type': by_shot_type,\n",
    "                'interaction_matrix': matrix\n",
    "            }\n",
    "        \n",
    "        self.analysis_report['individual_model_analysis'] = model_analysis\n",
    "        return model_analysis\n",
    "    \n",
    "    def generate_analysis_report(self):\n",
    "        \"\"\"Generate and save comprehensive analysis report.\"\"\"\n",
    "        # Add metadata\n",
    "        self.analysis_report['metadata'] = {\n",
    "            'analysis_timestamp': datetime.now().isoformat(),\n",
    "            'total_models': len(self.df['model'].unique()) if self.df is not None else 0,\n",
    "            'total_shot_types': len(self.df['shot_type'].unique()) if self.df is not None else 0,\n",
    "            'total_question_types': len(self.df['question_type'].unique()) if self.df is not None else 0,\n",
    "            'total_records': len(self.df) if self.df is not None else 0,\n",
    "            'metrics_evaluated': METRICS,\n",
    "            'num_judges': NUM_JUDGES\n",
    "        }\n",
    "        \n",
    "        # Save as JSON\n",
    "        with open('comprehensive_analysis_report.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.analysis_report, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        return self.analysis_report\n",
    "    \n",
    "    def print_summary_report(self):\n",
    "        \"\"\"Print a formatted summary of the analysis.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"COMPREHENSIVE LLM-BASED QA EVALUATION ANALYSIS REPORT\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Metadata\n",
    "        metadata = self.analysis_report['metadata']\n",
    "        print(f\"\\nANALYSIS METADATA:\")\n",
    "        print(f\"  Timestamp: {metadata['analysis_timestamp']}\")\n",
    "        print(f\"  Total Models: {metadata['total_models']}\")\n",
    "        print(f\"  Total Records: {metadata['total_records']}\")\n",
    "        print(f\"  Metrics: {', '.join(metadata['metrics_evaluated'])}\")\n",
    "        \n",
    "        # Data Loading Summary\n",
    "        loading = self.analysis_report['data_loading']\n",
    "        print(f\"\\nDATA LOADING SUMMARY:\")\n",
    "        print(f\"  Files Successfully Loaded: {len(loading['files_loaded'])}\")\n",
    "        print(f\"  Files Failed: {len(loading['files_failed'])}\")\n",
    "        print(f\"  Total Records: {loading['total_records']}\")\n",
    "        \n",
    "        # Overall Metrics\n",
    "        overall = self.analysis_report['overall_metrics']\n",
    "        print(f\"\\nOVERALL PERFORMANCE:\")\n",
    "        for metric, stats in overall.items():\n",
    "            print(f\"  {metric:15s}: {stats['mean']:.3f} ± {stats['std']:.3f} ({stats['mean_pct']:.1f}%)\")\n",
    "        \n",
    "        # Best Performing Models (by overall score)\n",
    "        model_scores = []\n",
    "        for model, data in self.analysis_report['individual_model_analysis'].items():\n",
    "            overall_score = data['overall_performance']['Overall_Score']['mean']\n",
    "            model_scores.append((model, overall_score))\n",
    "        \n",
    "        model_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        print(f\"\\nMODEL RANKING (by Overall Score):\")\n",
    "        for i, (model, score) in enumerate(model_scores, 1):\n",
    "            pct = self.normalize_to_percentage(score)\n",
    "            print(f\"  {i}. {model:25s}: {score:.3f} ({pct:.1f}%)\")\n",
    "        \n",
    "        # Majority Vote Summary\n",
    "        if 'majority_vote_analysis' in self.analysis_report:\n",
    "            majority = self.analysis_report['majority_vote_analysis']\n",
    "            if 'model_agreement_stats' in majority:\n",
    "                print(f\"\\nMODEL AGREEMENT WITH MAJORITY VOTE:\")\n",
    "                for model, stats in majority['model_agreement_stats'].items():\n",
    "                    print(f\"  {model:25s}: {stats['percentage']:.1f}% ({stats['agreements']}/{stats['total']})\")\n",
    "        \n",
    "        print(f\"\\nDETAILED REPORT SAVED TO: comprehensive_analysis_report.json\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    def run_all_analyses(self):\n",
    "        \"\"\"Main function to run all analyses and generate single report.\"\"\"\n",
    "        # Load all data (this also computes majority vote)\n",
    "        self.load_all_data()\n",
    "        \n",
    "        # Run all analyses\n",
    "        self.compute_overall_metrics()\n",
    "        self.compute_overall_score_distribution()\n",
    "        self.compute_breakdown_by_shot_type()\n",
    "        self.compute_breakdown_by_question_type()\n",
    "        self.compute_shot_type_x_question_type_matrix()\n",
    "        self.compute_individual_model_analysis()\n",
    "        \n",
    "        # Generate comprehensive report\n",
    "        self.generate_analysis_report()\n",
    "        \n",
    "        # Print summary\n",
    "        self.print_summary_report()\n",
    "        \n",
    "        return self.analysis_report\n",
    "\n",
    "def main():\n",
    "    analyzer = MultiModelEvaluationAnalyzer()\n",
    "    report = analyzer.run_all_analyses()\n",
    "    return report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
