## ðŸ§  LLM vs Human Evaluation

This module presents a comparative analysis between LLM-based evaluations and human expert judgments of **160** QA pairs generated by the NeuReg framework. The goal is to quantify alignment between automated scoring and human assessment across five core evaluation metrics.

### ðŸ“‹ Evaluation Setup
Human Ratings: Served as the ground truth benchmark.

LLM Ratings: Derived from five leading models using the standard NeuReg QA evaluation prompt.

Metrics Compared:

Relevance

Accuracy

Completeness

Fluency

KG Alignment

Comparison Metrics:

Exact Match (%): Percentage of scores that exactly matched human ratings.

F1 Score: Harmonic mean of precision and recall, reflecting partial agreement.

### ðŸ“Š LLMâ€“Human Agreement Results

| Model          | Metric        | Exact Match (%) | F1 Score |
| -------------- | ------------- | --------------- | -------- |
|                | Relevance     | 81.88           | 0.227    |
|                | Accuracy      | 75.63           | 0.218    |
| Mixtral-8x22B  | Completeness  | 58.75           | 0.167    |
|                | Fluency       | 80.00           | 0.444    |
|                | KG\_Alignment | 70.00           | 0.241    |
            ---------------------------
|                | Relevance     | 58.75           | 0.257    |
|                | Accuracy      | 50.63           | 0.246    |
| LLaMA-3.3-70B  | Completeness  | 10.00           | 0.066    |
|                | Fluency       | 66.88           | 0.274    |
|                | KG\_Alignment | 52.50           | 0.191    |
            ---------------------------
|                | Relevance     | 83.13           | 0.304    |
|                | Accuracy      | 75.63           | 0.287    |
| DeepSeek-R1    | Completeness  | 45.63           | 0.185    |
|                | Fluency       | 80.00           | 0.444    |
|                | KG\_Alignment | 70.63           | 0.208    |
            ---------------------------
|                | Relevance     | 82.50           | 0.301    |
|                | Accuracy      | 68.75           | 0.165    |
| Qwen3-32B      | Completeness  | 48.13           | 0.159    |
|                | Fluency       | 78.75           | 0.296    |
|                | KG\_Alignment | 59.38           | 0.151    |
            ---------------------------
|                | Relevance     | 78.75           | 0.177    |
|                | Accuracy      | 47.50           | 0.177    |
| Gemma-2-27B-IT | Completeness  | 25.63           | 0.134    |
|                | Fluency       | 78.13           | 0.294    |
|                | KG\_Alignment | 47.50           | 0.143    |
            ---------------------------
|     **Overall**                | **63.00**       | **0.23**|

----

ðŸ“Ž Related Modules
ðŸ”— LLM-as-a-Judge â€” Raw LLM scoring output

ðŸ”— Human Evaluation â€” Source human ratings

ðŸ”— LLM Results Analysis â€” Aggregated model performance
