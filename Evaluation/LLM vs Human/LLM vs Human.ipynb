{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e03d43-8820-40fb-aa46-4de981da9903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "import json\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define file mappings for human evaluators\n",
    "HUMAN_FILES = {\n",
    "    'Evaluator_01': 'Ev_Ramita_01.csv',\n",
    "    'Evaluator_02': 'Ev_Sibgha_02.csv',\n",
    "    'Evaluator_03': 'Ev_Saim_03.csv',\n",
    "    'Evaluator_04': 'Ev_Shahzad_04.csv',\n",
    "    'Evaluator_05': 'Ev_Ammar_05.csv',\n",
    "    \n",
    "}\n",
    "\n",
    "# Define file mappings for LLM evaluations\n",
    "LLM_FILES = {\n",
    "    'Mixtral-8x22B': {\n",
    "        'Zero-Shot': 'mixtral_zeroshot_evaluation_results.csv',\n",
    "        'One-Shot': 'mixtral_oneshot_evaluation_results.csv',\n",
    "        'Few-Shot': 'mixtral_fewshot_evaluation_results.csv'\n",
    "    },\n",
    "    'Llama-3.3-70B': {\n",
    "        'Zero-Shot': 'Llama_zeroshot_evaluation_results.csv',\n",
    "        'One-Shot': 'Llama_oneshot_evaluation_results.csv',\n",
    "        'Few-Shot': 'Llama_fewshot_evaluation_results.csv'\n",
    "    },\n",
    "    'DeepSeek-R1': {\n",
    "        'Zero-Shot': 'DeepSeek_zeroshot_evaluation_results.csv',\n",
    "        'One-Shot': 'DeepSeek_oneshot_evaluation_results.csv',\n",
    "        'Few-Shot': 'DeepSeek_fewshot_evaluation_results.csv'\n",
    "    },\n",
    "    'Qwen3-32B': {\n",
    "        'Zero-Shot': 'Qwen_zeroshot_evaluation_results.csv',\n",
    "        'One-Shot': 'Qwen_oneshot_evaluation_results.csv',\n",
    "        'Few-Shot': 'Qwen_fewshot_evaluation_results.csv'\n",
    "    },\n",
    "    'Gemma-2-27B-IT': {\n",
    "        'Zero-Shot': 'gemma_zeroshot_evaluation_results.csv',\n",
    "        'One-Shot': 'gemma_oneshot_evaluation_results.csv',\n",
    "        'Few-Shot': 'gemma_fewshot_evaluation_results.csv'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Column name mappings for human evaluation files\n",
    "HUMAN_COLUMN_MAPPINGS = {\n",
    "    'QA_ID': 'qa_id',\n",
    "    'Question_Type': 'question_type',\n",
    "    'Shot_Type': 'shot_type',\n",
    "    'Question': 'question',\n",
    "    'Answer': 'answer',\n",
    "    'Relevance (1–5)': 'Relevance',\n",
    "    'Accuracy  (1–5)': 'Accuracy',\n",
    "    'Completeness  (1–5)': 'Completeness',\n",
    "    'Fluency  (1–5)': 'Fluency',\n",
    "    'KG Alignment  (1–5)': 'KG_Alignment'\n",
    "}\n",
    "\n",
    "METRICS = ['Relevance', 'Accuracy', 'Completeness', 'Fluency', 'KG_Alignment']\n",
    "\n",
    "class HumanLLMComparisonAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.human_df = None\n",
    "        self.llm_data = {}\n",
    "        self.comparison_results = None\n",
    "        self.analysis_report = {\n",
    "            'metadata': {},\n",
    "            'data_loading': {},\n",
    "            'comparison_results': {},\n",
    "            'model_performance': {},\n",
    "            'metric_performance': {},\n",
    "            'overall_statistics': {},\n",
    "            'detailed_comparisons': {}\n",
    "        }\n",
    "    \n",
    "    def round_to_half(self, x):\n",
    "        \"\"\"Round to nearest 0.5\"\"\"\n",
    "        return round(x * 2) / 2\n",
    "\n",
    "    def load_human_evaluations(self):\n",
    "        \"\"\"Load all human evaluation files and compute mean scores per QA_ID\"\"\"\n",
    "        loading_info = {'files_loaded': [], 'files_failed': [], 'total_evaluations': 0, 'unique_qa_pairs': 0}\n",
    "        \n",
    "        all_human_data = []\n",
    "\n",
    "        for evaluator, filename in HUMAN_FILES.items():\n",
    "            try:\n",
    "                df = pd.read_csv(filename)\n",
    "                # Rename columns to match expected format\n",
    "                df = df.rename(columns=HUMAN_COLUMN_MAPPINGS)\n",
    "                # Keep only relevant columns\n",
    "                relevant_cols = ['qa_id', 'question_type', 'shot_type', 'question', 'answer'] + METRICS\n",
    "                df = df[relevant_cols]\n",
    "                all_human_data.append(df)\n",
    "                loading_info['files_loaded'].append({\n",
    "                    'evaluator': evaluator,\n",
    "                    'filename': filename,\n",
    "                    'evaluations': len(df)\n",
    "                })\n",
    "            except Exception as e:\n",
    "                loading_info['files_failed'].append({\n",
    "                    'evaluator': evaluator,\n",
    "                    'filename': filename,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "\n",
    "        if not all_human_data:\n",
    "            raise ValueError(\"No human evaluation files could be loaded!\")\n",
    "\n",
    "        # Combine all human evaluations\n",
    "        human_df_all = pd.concat(all_human_data, ignore_index=True)\n",
    "        loading_info['total_evaluations'] = len(human_df_all)\n",
    "\n",
    "        # Compute mean scores per QA_ID for each metric\n",
    "        metric_means = human_df_all.groupby('qa_id')[METRICS].mean().reset_index()\n",
    "        metadata = human_df_all.groupby('qa_id').first()[['question_type', 'shot_type', 'question', 'answer']].reset_index()\n",
    "        \n",
    "        # Merge metadata with mean scores\n",
    "        self.human_df = pd.merge(metadata, metric_means, on='qa_id')\n",
    "        loading_info['unique_qa_pairs'] = len(self.human_df)\n",
    "        \n",
    "        self.analysis_report['data_loading']['human_evaluations'] = loading_info\n",
    "        return self.human_df\n",
    "\n",
    "    def load_llm_evaluations(self, human_qa_ids):\n",
    "        \"\"\"Load all LLM evaluation files and filter to match human QA_IDs\"\"\"\n",
    "        llm_loading_info = {'models_processed': [], 'models_failed': []}\n",
    "        \n",
    "        for model_name, shot_files in LLM_FILES.items():\n",
    "            model_info = {'model_name': model_name, 'shot_types': [], 'total_qa_pairs': 0}\n",
    "            model_data = []\n",
    "\n",
    "            for shot_type, filename in shot_files.items():\n",
    "                try:\n",
    "                    df = pd.read_csv(filename)\n",
    "                    # Filter to keep only QA_IDs that exist in human evaluations\n",
    "                    df_filtered = df[df['qa_id'].isin(human_qa_ids)]\n",
    "\n",
    "                    if len(df_filtered) > 0:\n",
    "                        df_filtered['model'] = model_name\n",
    "                        df_filtered['shot_type'] = shot_type\n",
    "                        model_data.append(df_filtered)\n",
    "                        model_info['shot_types'].append({\n",
    "                            'shot_type': shot_type,\n",
    "                            'filename': filename,\n",
    "                            'matching_pairs': len(df_filtered),\n",
    "                            'total_pairs': len(df)\n",
    "                        })\n",
    "                    else:\n",
    "                        model_info['shot_types'].append({\n",
    "                            'shot_type': shot_type,\n",
    "                            'filename': filename,\n",
    "                            'matching_pairs': 0,\n",
    "                            'total_pairs': len(df)\n",
    "                        })\n",
    "\n",
    "                except Exception as e:\n",
    "                    model_info['shot_types'].append({\n",
    "                        'shot_type': shot_type,\n",
    "                        'filename': filename,\n",
    "                        'error': str(e)\n",
    "                    })\n",
    "\n",
    "            if model_data:\n",
    "                # Combine all shot types for this model\n",
    "                model_df = pd.concat(model_data, ignore_index=True)\n",
    "                # Average scores across shot types for each qa_id\n",
    "                model_avg = model_df.groupby('qa_id')[METRICS].mean().reset_index()\n",
    "                self.llm_data[model_name] = model_avg\n",
    "                model_info['total_qa_pairs'] = len(model_avg)\n",
    "                llm_loading_info['models_processed'].append(model_info)\n",
    "            else:\n",
    "                llm_loading_info['models_failed'].append(model_info)\n",
    "\n",
    "        self.analysis_report['data_loading']['llm_evaluations'] = llm_loading_info\n",
    "        return self.llm_data\n",
    "\n",
    "    def compute_exact_match_percentage(self, human_scores, llm_scores):\n",
    "        \"\"\"Compute exact match percentage after rounding to nearest 0.5\"\"\"\n",
    "        human_rounded = np.array([self.round_to_half(x) for x in human_scores])\n",
    "        llm_rounded = np.array([self.round_to_half(x) for x in llm_scores])\n",
    "\n",
    "        matches = (human_rounded == llm_rounded).sum()\n",
    "        total = len(human_scores)\n",
    "\n",
    "        return (matches / total) * 100 if total > 0 else 0\n",
    "\n",
    "    def compute_f1_macro(self, human_scores, llm_scores):\n",
    "        \"\"\"Compute macro F1 score between rounded scores\"\"\"\n",
    "        human_rounded = np.array([self.round_to_half(x) for x in human_scores])\n",
    "        llm_rounded = np.array([self.round_to_half(x) for x in llm_scores])\n",
    "\n",
    "        # Convert to integer labels for sklearn (multiply by 2 to avoid decimals)\n",
    "        human_labels = (human_rounded * 2).astype(int)\n",
    "        llm_labels = (llm_rounded * 2).astype(int)\n",
    "\n",
    "        try:\n",
    "            return f1_score(human_labels, llm_labels, average='macro', zero_division=0)\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    def compare_evaluations(self):\n",
    "        \"\"\"Compare human and LLM evaluations for all models and metrics\"\"\"\n",
    "        results = []\n",
    "        detailed_comparisons = {}\n",
    "\n",
    "        for model_name, llm_df in self.llm_data.items():\n",
    "            # Merge human and LLM data on qa_id\n",
    "            merged_df = pd.merge(self.human_df[['qa_id'] + METRICS],\n",
    "                                llm_df[['qa_id'] + METRICS],\n",
    "                                on='qa_id',\n",
    "                                suffixes=('_human', '_llm'))\n",
    "\n",
    "            if len(merged_df) == 0:\n",
    "                continue\n",
    "\n",
    "            model_comparisons = {'matched_qa_pairs': len(merged_df), 'metrics': {}}\n",
    "\n",
    "            for metric in METRICS:\n",
    "                human_col = f'{metric}_human'\n",
    "                llm_col = f'{metric}_llm'\n",
    "\n",
    "                # Get scores\n",
    "                human_scores = merged_df[human_col].values\n",
    "                llm_scores = merged_df[llm_col].values\n",
    "\n",
    "                # Compute metrics\n",
    "                exact_match = self.compute_exact_match_percentage(human_scores, llm_scores)\n",
    "                f1 = self.compute_f1_macro(human_scores, llm_scores)\n",
    "\n",
    "                results.append({\n",
    "                    'Model': model_name,\n",
    "                    'Metric': metric,\n",
    "                    'Exact Match (%)': exact_match,\n",
    "                    'F1 Score': f1\n",
    "                })\n",
    "\n",
    "                model_comparisons['metrics'][metric] = {\n",
    "                    'exact_match_percentage': float(exact_match),\n",
    "                    'f1_score': float(f1),\n",
    "                    'human_mean': float(human_scores.mean()),\n",
    "                    'llm_mean': float(llm_scores.mean()),\n",
    "                    'human_std': float(human_scores.std()),\n",
    "                    'llm_std': float(llm_scores.std())\n",
    "                }\n",
    "\n",
    "            detailed_comparisons[model_name] = model_comparisons\n",
    "\n",
    "        self.comparison_results = pd.DataFrame(results)\n",
    "        self.analysis_report['detailed_comparisons'] = detailed_comparisons\n",
    "        return self.comparison_results\n",
    "\n",
    "    def analyze_model_performance(self):\n",
    "        \"\"\"Analyze performance by model\"\"\"\n",
    "        if self.comparison_results is None or self.comparison_results.empty:\n",
    "            return {}\n",
    "\n",
    "        numeric_cols = ['Exact Match (%)', 'F1 Score']\n",
    "        model_averages = self.comparison_results.groupby('Model')[numeric_cols].mean().reset_index()\n",
    "        \n",
    "        model_performance = {}\n",
    "        for _, row in model_averages.iterrows():\n",
    "            model_performance[row['Model']] = {\n",
    "                'avg_exact_match': float(row['Exact Match (%)']),\n",
    "                'avg_f1_score': float(row['F1 Score'])\n",
    "            }\n",
    "\n",
    "        # Sort models by performance\n",
    "        sorted_models = sorted(model_performance.items(), \n",
    "                              key=lambda x: x[1]['avg_f1_score'], reverse=True)\n",
    "        \n",
    "        model_performance_analysis = {\n",
    "            'rankings': {\n",
    "                'by_exact_match': sorted(model_performance.items(), \n",
    "                                       key=lambda x: x[1]['avg_exact_match'], reverse=True),\n",
    "                'by_f1_score': sorted_models\n",
    "            },\n",
    "            'performance_data': model_performance\n",
    "        }\n",
    "\n",
    "        self.analysis_report['model_performance'] = model_performance_analysis\n",
    "        return model_performance_analysis\n",
    "\n",
    "    def analyze_metric_performance(self):\n",
    "        \"\"\"Analyze performance by metric\"\"\"\n",
    "        if self.comparison_results is None or self.comparison_results.empty:\n",
    "            return {}\n",
    "\n",
    "        numeric_cols = ['Exact Match (%)', 'F1 Score']\n",
    "        metric_averages = self.comparison_results.groupby('Metric')[numeric_cols].mean().reset_index()\n",
    "        \n",
    "        metric_performance = {}\n",
    "        for _, row in metric_averages.iterrows():\n",
    "            metric_performance[row['Metric']] = {\n",
    "                'avg_exact_match': float(row['Exact Match (%)']),\n",
    "                'avg_f1_score': float(row['F1 Score'])\n",
    "            }\n",
    "\n",
    "        # Sort metrics by difficulty (lower agreement = more difficult)\n",
    "        sorted_metrics = sorted(metric_performance.items(), \n",
    "                               key=lambda x: x[1]['avg_f1_score'])\n",
    "        \n",
    "        metric_performance_analysis = {\n",
    "            'rankings': {\n",
    "                'easiest_to_hardest_f1': sorted_metrics,\n",
    "                'easiest_to_hardest_exact_match': sorted(metric_performance.items(), \n",
    "                                                        key=lambda x: x[1]['avg_exact_match'])\n",
    "            },\n",
    "            'performance_data': metric_performance\n",
    "        }\n",
    "\n",
    "        self.analysis_report['metric_performance'] = metric_performance_analysis\n",
    "        return metric_performance_analysis\n",
    "\n",
    "    def compute_overall_statistics(self):\n",
    "        \"\"\"Compute overall statistics\"\"\"\n",
    "        if self.comparison_results is None or self.comparison_results.empty:\n",
    "            return {}\n",
    "\n",
    "        overall_stats = {\n",
    "            'overall_exact_match': float(self.comparison_results['Exact Match (%)'].mean()),\n",
    "            'overall_f1_score': float(self.comparison_results['F1 Score'].mean()),\n",
    "            'total_comparisons': len(self.comparison_results),\n",
    "            'total_models': len(self.llm_data),\n",
    "            'total_metrics': len(METRICS),\n",
    "            'best_performing_model': {\n",
    "                'by_exact_match': self.comparison_results.groupby('Model')['Exact Match (%)'].mean().idxmax(),\n",
    "                'by_f1_score': self.comparison_results.groupby('Model')['F1 Score'].mean().idxmax()\n",
    "            },\n",
    "            'most_agreeable_metric': {\n",
    "                'by_exact_match': self.comparison_results.groupby('Metric')['Exact Match (%)'].mean().idxmax(),\n",
    "                'by_f1_score': self.comparison_results.groupby('Metric')['F1 Score'].mean().idxmax()\n",
    "            }\n",
    "        }\n",
    "\n",
    "        self.analysis_report['overall_statistics'] = overall_stats\n",
    "        return overall_stats\n",
    "\n",
    "    def generate_analysis_report(self):\n",
    "        \"\"\"Generate and save comprehensive analysis report\"\"\"\n",
    "        # Add metadata\n",
    "        self.analysis_report['metadata'] = {\n",
    "            'analysis_timestamp': datetime.now().isoformat(),\n",
    "            'analysis_type': 'Human vs LLM Evaluation Comparison',\n",
    "            'total_human_evaluators': len(HUMAN_FILES),\n",
    "            'total_llm_models': len(LLM_FILES),\n",
    "            'metrics_compared': METRICS,\n",
    "            'comparison_method': 'Exact Match and F1 Score (rounded to nearest 0.5)'\n",
    "        }\n",
    "\n",
    "        # Perform all analyses\n",
    "        self.analyze_model_performance()\n",
    "        self.analyze_metric_performance()\n",
    "        self.compute_overall_statistics()\n",
    "\n",
    "        # Save as JSON\n",
    "        with open('human_llm_comparison_analysis_report.json', 'w', encoding='utf-8') as f:\n",
    "            # Convert DataFrame to dict for JSON serialization\n",
    "            if self.comparison_results is not None:\n",
    "                self.analysis_report['comparison_results'] = self.comparison_results.to_dict('records')\n",
    "            json.dump(self.analysis_report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        return self.analysis_report\n",
    "\n",
    "    def print_summary_report(self):\n",
    "        \"\"\"Print a formatted summary of the comparison analysis\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"COMPREHENSIVE HUMAN vs LLM EVALUATION COMPARISON REPORT\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Metadata\n",
    "        metadata = self.analysis_report['metadata']\n",
    "        print(f\"\\nANALYSIS METADATA:\")\n",
    "        print(f\"  Timestamp: {metadata['analysis_timestamp']}\")\n",
    "        print(f\"  Human Evaluators: {metadata['total_human_evaluators']}\")\n",
    "        print(f\"  LLM Models: {metadata['total_llm_models']}\")\n",
    "        print(f\"  Metrics Compared: {', '.join(metadata['metrics_compared'])}\")\n",
    "\n",
    "        # Data Loading Summary\n",
    "        loading = self.analysis_report['data_loading']\n",
    "        print(f\"\\nDATA LOADING SUMMARY:\")\n",
    "        human_loading = loading['human_evaluations']\n",
    "        print(f\"  Human Files Loaded: {len(human_loading['files_loaded'])}\")\n",
    "        print(f\"  Total Human Evaluations: {human_loading['total_evaluations']}\")\n",
    "        print(f\"  Unique QA Pairs: {human_loading['unique_qa_pairs']}\")\n",
    "        \n",
    "        llm_loading = loading['llm_evaluations']\n",
    "        print(f\"  LLM Models Processed: {len(llm_loading['models_processed'])}\")\n",
    "\n",
    "        # Overall Statistics\n",
    "        overall = self.analysis_report['overall_statistics']\n",
    "        print(f\"\\nOVERALL AGREEMENT STATISTICS:\")\n",
    "        print(f\"  Overall Exact Match: {overall['overall_exact_match']:.1f}%\")\n",
    "        print(f\"  Overall F1 Score: {overall['overall_f1_score']:.3f}\")\n",
    "        print(f\"  Total Comparisons: {overall['total_comparisons']}\")\n",
    "\n",
    "        # Best Performing Models\n",
    "        print(f\"\\nBEST PERFORMING MODELS:\")\n",
    "        print(f\"  By Exact Match: {overall['best_performing_model']['by_exact_match']}\")\n",
    "        print(f\"  By F1 Score: {overall['best_performing_model']['by_f1_score']}\")\n",
    "\n",
    "        # Model Rankings\n",
    "        model_perf = self.analysis_report['model_performance']\n",
    "        print(f\"\\nMODEL RANKINGS (by F1 Score):\")\n",
    "        for i, (model, score) in enumerate(model_perf['rankings']['by_f1_score'], 1):\n",
    "            print(f\"  {i}. {model:25s}: F1={score['avg_f1_score']:.3f}, EM={score['avg_exact_match']:.1f}%\")\n",
    "\n",
    "        # Metric Difficulty Analysis\n",
    "        metric_perf = self.analysis_report['metric_performance']\n",
    "        print(f\"\\nMETRIC DIFFICULTY (easiest to hardest by F1 Score):\")\n",
    "        for i, (metric, score) in enumerate(reversed(metric_perf['rankings']['easiest_to_hardest_f1']), 1):\n",
    "            print(f\"  {i}. {metric:15s}: F1={score['avg_f1_score']:.3f}, EM={score['avg_exact_match']:.1f}%\")\n",
    "\n",
    "        print(f\"\\nDETAILED REPORT SAVED TO: human_llm_comparison_analysis_report.json\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "    def run_comparison_analysis(self):\n",
    "        \"\"\"Main function to run all analyses and generate single report\"\"\"\n",
    "        # Load human evaluations\n",
    "        self.load_human_evaluations()\n",
    "        human_qa_ids = self.human_df['qa_id'].unique()\n",
    "\n",
    "        # Load LLM evaluations\n",
    "        self.load_llm_evaluations(human_qa_ids)\n",
    "\n",
    "        if not self.llm_data:\n",
    "            raise ValueError(\"No LLM data could be loaded!\")\n",
    "\n",
    "        # Compare evaluations\n",
    "        self.compare_evaluations()\n",
    "\n",
    "        if self.comparison_results is None or self.comparison_results.empty:\n",
    "            raise ValueError(\"No comparison results generated!\")\n",
    "\n",
    "        # Generate comprehensive report\n",
    "        self.generate_analysis_report()\n",
    "\n",
    "        # Print summary\n",
    "        self.print_summary_report()\n",
    "\n",
    "        # Save detailed CSV\n",
    "        self.comparison_results.to_csv('human_llm_comparison_results.csv', index=False)\n",
    "\n",
    "        return self.analysis_report\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the comparison analysis\"\"\"\n",
    "    analyzer = HumanLLMComparisonAnalyzer()\n",
    "    \n",
    "    try:\n",
    "        report = analyzer.run_comparison_analysis()\n",
    "        return report\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
