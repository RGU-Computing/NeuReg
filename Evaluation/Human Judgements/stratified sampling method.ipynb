{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115b0f9a-5596-4d10-9418-6d3292500679",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration\n",
    "# ----------------------------\n",
    "json_files = {\n",
    "    \"zero_shot\": \"Zero-Shot_qa_dataset.json\",\n",
    "    \"one_shot\": \"One-Shot_qa_dataset.json\", \n",
    "    \"few_shot\": \"Few-Shot_qa_dataset.json\"\n",
    "}\n",
    "\n",
    "QUESTION_TYPES = [\"factual\", \"relationship\", \"comparative\", \"inferential\"]  \n",
    "SHOT_TYPES = [\"zero_shot\", \"one_shot\", \"few_shot\"]\n",
    "\n",
    "# Calculate 5% from each dataset\n",
    "SAMPLE_PERCENTAGE = 0.05\n",
    "DATASET_SIZES = {\n",
    "    \"zero_shot\": 1127,\n",
    "    \"one_shot\": 1080, \n",
    "    \"few_shot\": 1107\n",
    "}\n",
    "\n",
    "# Calculate target samples per shot type (5% of each)\n",
    "SAMPLES_PER_SHOT_TYPE = {\n",
    "    shot_type: int(size * SAMPLE_PERCENTAGE) \n",
    "    for shot_type, size in DATASET_SIZES.items()\n",
    "}\n",
    "\n",
    "# Calculate samples per combination (divided equally across 4 question types)\n",
    "SAMPLES_PER_COMBINATION = {\n",
    "    shot_type: max(1, samples // len(QUESTION_TYPES))\n",
    "    for shot_type, samples in SAMPLES_PER_SHOT_TYPE.items()\n",
    "}\n",
    "\n",
    "TOTAL_TARGET_SAMPLES = sum(SAMPLES_PER_SHOT_TYPE.values())\n",
    "\n",
    "print(f\" Target: 5% from each dataset\")\n",
    "print(f\" Samples per shot type:\")\n",
    "for shot_type, total in SAMPLES_PER_SHOT_TYPE.items():\n",
    "    per_qtype = SAMPLES_PER_COMBINATION[shot_type]\n",
    "    print(f\" {shot_type}: {total} total ({per_qtype} per question type)\")\n",
    "print(f\" Total target samples: {TOTAL_TARGET_SAMPLES}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Extract and Format Triples for Human Readability\n",
    "# ----------------------------\n",
    "def extract_all_triples(obj):\n",
    "    \"\"\"Extract all subject-predicate-object triples from nested structures and format for readability.\"\"\"\n",
    "    triples = []\n",
    "    if isinstance(obj, dict):\n",
    "        if set(obj.keys()) >= {\"subject\", \"predicate\", \"object\"}:\n",
    "            # Format as: Subject → Predicate → Object\n",
    "            formatted_triple = f\"{obj['subject']} → {obj['predicate']} → {obj['object']}\"\n",
    "            triples.append(formatted_triple)\n",
    "        for value in obj.values():\n",
    "            triples.extend(extract_all_triples(value))\n",
    "    elif isinstance(obj, list):\n",
    "        for item in obj:\n",
    "            triples.extend(extract_all_triples(item))\n",
    "    return triples\n",
    "\n",
    "def format_kg_for_humans(triples_string):\n",
    "    \"\"\"Convert semicolon-separated triples to numbered, readable format.\"\"\"\n",
    "    if not triples_string or triples_string.strip() == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    # Split by semicolon and clean up\n",
    "    triples = [triple.strip() for triple in triples_string.split(\";\") if triple.strip()]\n",
    "    \n",
    "    if not triples:\n",
    "        return \"\"\n",
    "    \n",
    "    # Format as numbered list with better spacing\n",
    "    formatted_triples = []\n",
    "    for i, triple in enumerate(triples, 1):\n",
    "        # Replace dashes with arrows for better readability\n",
    "        if \" - \" in triple:\n",
    "            parts = triple.split(\" - \")\n",
    "            if len(parts) == 3:\n",
    "                subject, predicate, object_part = parts\n",
    "                formatted_triple = f\"{i}. {subject.strip()} → {predicate.strip()} → {object_part.strip()}\"\n",
    "                formatted_triples.append(formatted_triple)\n",
    "            else:\n",
    "                # Fallback for malformed triples\n",
    "                formatted_triples.append(f\"{i}. {triple}\")\n",
    "        else:\n",
    "            # Already formatted or different format\n",
    "            formatted_triples.append(f\"{i}. {triple}\")\n",
    "    \n",
    "    return \"\\n\".join(formatted_triples)\n",
    "\n",
    "# ----------------------------\n",
    "# Load Data from JSON Files\n",
    "# ----------------------------\n",
    "def load_data(file_path, shot_type):\n",
    "    \"\"\"Load and process data from JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\" File not found: {file_path}\")\n",
    "        return pd.DataFrame()\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\" Invalid JSON in file: {file_path}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    queries = data.get(\"queries\", [])\n",
    "    rows = []\n",
    "    skipped_types = Counter()\n",
    "    total_items = len(queries)\n",
    "    \n",
    "    for item in queries:\n",
    "        qtype = item.get(\"question_type\", \"\").lower().strip()\n",
    "        \n",
    "        # Track what question types we're seeing\n",
    "        if qtype not in QUESTION_TYPES:\n",
    "            skipped_types[qtype] += 1\n",
    "            continue\n",
    "        \n",
    "        # Extract triples from the knowledge graph\n",
    "        triples = extract_all_triples(item)\n",
    "        raw_source_kg = \"; \".join(triples)\n",
    "        \n",
    "        # Format KG for human readability\n",
    "        source_kg = format_kg_for_humans(raw_source_kg)\n",
    "        \n",
    "        # Extract source_context from the correct location\n",
    "        source_context = \"\"\n",
    "        \n",
    "        # The source_context is nested under ground_truth\n",
    "        if \"ground_truth\" in item and isinstance(item[\"ground_truth\"], dict):\n",
    "            source_context = item[\"ground_truth\"].get(\"source_context\", \"\")\n",
    "        \n",
    "        # Fallback: check if it's at root level (for backward compatibility)\n",
    "        if not source_context:\n",
    "            source_context = item.get(\"source_context\", \"\")\n",
    "        \n",
    "        rows.append({\n",
    "            \"qa_id\": item.get(\"id\", \"\"),\n",
    "            \"question_type\": qtype,\n",
    "            \"shot_type\": shot_type,\n",
    "            \"question\": item.get(\"question\", \"\"),\n",
    "            \"answer\": item.get(\"answer\", \"\"),\n",
    "            \"source_context\": source_context,\n",
    "            \"source_kg\": source_kg\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    print(f\" {shot_type}: Loaded {len(df)} valid QA pairs out of {total_items} total\")\n",
    "    \n",
    "    # Debug: Check how many have empty source_context\n",
    "    empty_context_count = sum(1 for row in rows if not row['source_context'].strip())\n",
    "    if empty_context_count > 0:\n",
    "        print(f\"   {empty_context_count} entries have empty source_context\")\n",
    "        # Show a sample entry to debug structure\n",
    "        if rows:\n",
    "            print(f\" Sample entry keys: {list(rows[0].keys())}\")\n",
    "            print(f\" Sample source_context length: {len(rows[0]['source_context'])}\")\n",
    "    \n",
    "    # Show what was skipped\n",
    "    if skipped_types:\n",
    "        print(f\"   Skipped question types: {dict(skipped_types)}\")\n",
    "    \n",
    "    # Show distribution of question types\n",
    "    if len(df) > 0:\n",
    "        type_counts = df[\"question_type\"].value_counts()\n",
    "        print(f\"  Question type distribution: {dict(type_counts)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ----------------------------\n",
    "# Stratified Sampling Function\n",
    "# ----------------------------\n",
    "def stratified_sample_per_type(df, shot_type, samples_per_type):\n",
    "    \"\"\"Sample equal numbers from each question type within a shot type.\"\"\"\n",
    "    sampled_rows = []\n",
    "    \n",
    "    for qtype in QUESTION_TYPES:\n",
    "        type_df = df[df[\"question_type\"] == qtype]\n",
    "        \n",
    "        if len(type_df) == 0:\n",
    "            print(f\"  {shot_type} - {qtype}: No samples available\")\n",
    "            continue\n",
    "        \n",
    "        if len(type_df) < samples_per_type:\n",
    "            print(f\"  {shot_type} - {qtype}: Only {len(type_df)} samples available (need {samples_per_type})\")\n",
    "            # Take all available samples\n",
    "            sampled = type_df.copy()\n",
    "        else:\n",
    "            # Random sample without replacement\n",
    "            sampled = type_df.sample(n=samples_per_type, random_state=42)\n",
    "        \n",
    "        sampled_rows.append(sampled)\n",
    "        print(f\" {shot_type} - {qtype}: Sampled {len(sampled)} samples\")\n",
    "    \n",
    "    if sampled_rows:\n",
    "        return pd.concat(sampled_rows, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# ----------------------------\n",
    "# Load All Data\n",
    "# ----------------------------\n",
    "print(\"\\n Loading data from files...\")\n",
    "all_dfs = []\n",
    "\n",
    "for shot_type, file_path in json_files.items():\n",
    "    df = load_data(file_path, shot_type)\n",
    "    if len(df) > 0:\n",
    "        all_dfs.append(df)\n",
    "\n",
    "if not all_dfs:\n",
    "    print(\" No valid data loaded from any file!\")\n",
    "    exit(1)\n",
    "\n",
    "# ----------------------------\n",
    "# Perform Stratified Sampling\n",
    "# ----------------------------\n",
    "print(f\"\\n Performing stratified sampling (5% from each dataset)...\")\n",
    "sampled_dfs = []\n",
    "\n",
    "for shot_type, file_path in json_files.items():\n",
    "    df = load_data(file_path, shot_type)\n",
    "    if len(df) > 0:\n",
    "        samples_per_qtype = SAMPLES_PER_COMBINATION[shot_type]\n",
    "        sampled = stratified_sample_per_type(df, shot_type, samples_per_qtype)\n",
    "        if len(sampled) > 0:\n",
    "            sampled_dfs.append(sampled)\n",
    "\n",
    "# ----------------------------\n",
    "# Combine and Shuffle\n",
    "# ----------------------------\n",
    "if sampled_dfs:\n",
    "    final_df = pd.concat(sampled_dfs, ignore_index=True)\n",
    "    \n",
    "    # Shuffle the final dataset\n",
    "    final_df = final_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Reorder columns for better readability\n",
    "    column_order = [\"qa_id\", \"question_type\", \"shot_type\", \"question\", \"answer\", \"source_context\", \"source_kg\"]\n",
    "    final_df = final_df[column_order]\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Save to CSV\n",
    "    # ----------------------------\n",
    "    output_file = f\"QA_Human_Eval_Stratified_5percent.csv\"\n",
    "    final_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Analysis and Summary\n",
    "    # ----------------------------\n",
    "    print(\"\\n STRATIFIED SAMPLING RESULTS:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Cross-tabulation showing distribution\n",
    "    cross_tab = pd.crosstab(final_df['shot_type'], final_df['question_type'], margins=True)\n",
    "    print(\"\\n Distribution Matrix:\")\n",
    "    print(cross_tab)\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\n Summary:\")\n",
    "    print(f\"   • Total samples: {len(final_df)}\")\n",
    "    print(f\"   • Target samples: {TOTAL_TARGET_SAMPLES}\")\n",
    "    print(f\"   • Shot types: {final_df['shot_type'].nunique()}\")\n",
    "    print(f\"   • Question types: {final_df['question_type'].nunique()}\")\n",
    "    print(f\"   • Combinations covered: {len(final_df.groupby(['shot_type', 'question_type']))}\")\n",
    "    \n",
    "    # Check balance\n",
    "    combination_counts = final_df.groupby(['shot_type', 'question_type']).size()\n",
    "    print(f\"\\n⚖️  Balance check:\")\n",
    "    print(f\"   • Min samples per combination: {combination_counts.min()}\")\n",
    "    print(f\"   • Max samples per combination: {combination_counts.max()}\")\n",
    "    print(f\"   • Mean samples per combination: {combination_counts.mean():.1f}\")\n",
    "    \n",
    "    # Check if balance is within expected range for proportional sampling\n",
    "    max_diff = combination_counts.max() - combination_counts.min()\n",
    "    if max_diff <= 1:  # Allow for 1 sample difference due to proportional sampling\n",
    "        print(\" Excellent stratified balance achieved!\")\n",
    "    elif max_diff <= 2:\n",
    "        print(\" Good stratified balance achieved!\")\n",
    "    else:\n",
    "        print(\"  Significant imbalance detected - check data distribution\")\n",
    "    \n",
    "    print(f\"\\n Saved to: {output_file}\")\n",
    "    print(f\" Stratified sampling completed successfully!\")\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Create Visualizations\n",
    "    # ----------------------------\n",
    "    print(f\"\\n Creating visualizations...\")\n",
    "    \n",
    "    # Create a figure with multiple subplots\n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # 1. Distribution Matrix Heatmap\n",
    "    plt.subplot(2, 3, 1)\n",
    "    cross_tab_no_margins = pd.crosstab(final_df['shot_type'], final_df['question_type'])\n",
    "    sns.heatmap(cross_tab_no_margins, annot=True, fmt='d', cmap='Blues', \n",
    "                cbar_kws={'label': 'Number of Samples'})\n",
    "    plt.title('Sample Distribution Matrix\\n(Shot Type × Question Type)', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Question Type', fontweight='bold')\n",
    "    plt.ylabel('Shot Type', fontweight='bold')\n",
    "    \n",
    "    # 2. Samples per Shot Type\n",
    "    plt.subplot(2, 3, 2)\n",
    "    shot_counts = final_df['shot_type'].value_counts()\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "    bars = plt.bar(shot_counts.index, shot_counts.values, color=colors, alpha=0.8, edgecolor='black')\n",
    "    plt.title('Samples per Shot Type\\n(5% from each dataset)', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Shot Type', fontweight='bold')\n",
    "    plt.ylabel('Number of Samples', fontweight='bold')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 3. Samples per Question Type\n",
    "    plt.subplot(2, 3, 3)\n",
    "    question_counts = final_df['question_type'].value_counts()\n",
    "    colors = ['#96CEB4', '#FECA57', '#FF9FF3', '#54A0FF']\n",
    "    bars = plt.bar(question_counts.index, question_counts.values, color=colors, alpha=0.8, edgecolor='black')\n",
    "    plt.title('Samples per Question Type\\n(Equal distribution)', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Question Type', fontweight='bold')\n",
    "    plt.ylabel('Number of Samples', fontweight='bold')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. Stacked Bar Chart\n",
    "    plt.subplot(2, 3, 4)\n",
    "    pivot_data = final_df.pivot_table(index='shot_type', columns='question_type', \n",
    "                                     values='qa_id', aggfunc='count', fill_value=0)\n",
    "    pivot_data.plot(kind='bar', stacked=True, ax=plt.gca(), \n",
    "                   color=['#96CEB4', '#FECA57', '#FF9FF3', '#54A0FF'], alpha=0.8)\n",
    "    plt.title('Stacked Distribution\\n(Question Types within Shot Types)', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Shot Type', fontweight='bold')\n",
    "    plt.ylabel('Number of Samples', fontweight='bold')\n",
    "    plt.legend(title='Question Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # 5. Comparison: Original vs Sampled\n",
    "    plt.subplot(2, 3, 5)\n",
    "    original_sizes = [DATASET_SIZES['zero_shot'], DATASET_SIZES['one_shot'], DATASET_SIZES['few_shot']]\n",
    "    sampled_sizes = [SAMPLES_PER_SHOT_TYPE['zero_shot'], SAMPLES_PER_SHOT_TYPE['one_shot'], SAMPLES_PER_SHOT_TYPE['few_shot']]\n",
    "    shot_types_labels = ['Zero-Shot', 'One-Shot', 'Few-Shot']\n",
    "    \n",
    "    x = np.arange(len(shot_types_labels))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = plt.bar(x - width/2, original_sizes, width, label='Original Dataset', color='lightcoral', alpha=0.8)\n",
    "    bars2 = plt.bar(x + width/2, sampled_sizes, width, label='5% Sample', color='lightblue', alpha=0.8)\n",
    "    \n",
    "    plt.title('Dataset Size Comparison\\n(Original vs 5% Sample)', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Shot Type', fontweight='bold')\n",
    "    plt.ylabel('Number of Samples', fontweight='bold')\n",
    "    plt.xticks(x, shot_types_labels)\n",
    "    plt.legend()\n",
    "    plt.yscale('log')  # Log scale to show both large and small numbers\n",
    "    \n",
    "    # Add value labels\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height * 1.1,\n",
    "                    f'{int(height)}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # 6. Pie Chart of Question Type Distribution\n",
    "    plt.subplot(2, 3, 6)\n",
    "    question_counts = final_df['question_type'].value_counts()\n",
    "    colors = ['#96CEB4', '#FECA57', '#FF9FF3', '#54A0FF']\n",
    "    plt.pie(question_counts.values, labels=question_counts.index, autopct='%1.1f%%', \n",
    "            colors=colors, startangle=90)\n",
    "    plt.title('Question Type Distribution\\n(Percentage of Total Sample)', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the visualization\n",
    "    viz_filename = \"QA_Stratified_Sampling_Visualization.png\"\n",
    "    plt.savefig(viz_filename, dpi=300, bbox_inches='tight')\n",
    "    print(f\" Visualizations saved to: {viz_filename}\")\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Create Summary Statistics Table\n",
    "    # ----------------------------\n",
    "    print(f\"\\n Creating summary statistics table...\")\n",
    "    \n",
    "    # Create detailed statistics\n",
    "    summary_stats = []\n",
    "    for shot_type in SHOT_TYPES:\n",
    "        for qtype in QUESTION_TYPES:\n",
    "            count = len(final_df[(final_df['shot_type'] == shot_type) & (final_df['question_type'] == qtype)])\n",
    "            original_total = DATASET_SIZES[shot_type]\n",
    "            percentage = (count / len(final_df)) * 100\n",
    "            \n",
    "            summary_stats.append({\n",
    "                'Shot Type': shot_type,\n",
    "                'Question Type': qtype,\n",
    "                'Samples': count,\n",
    "                'Original Dataset Size': original_total,\n",
    "                'Percentage of Total Sample': f\"{percentage:.1f}%\"\n",
    "            })\n",
    "    \n",
    "    stats_df = pd.DataFrame(summary_stats)\n",
    "    \n",
    "    # Save summary statistics\n",
    "    stats_filename = \"QA_Sampling_Summary_Statistics.csv\"\n",
    "    stats_df.to_csv(stats_filename, index=False)\n",
    "    print(f\" Summary statistics saved to: {stats_filename}\")\n",
    "    \n",
    "    # Display the table\n",
    "    print(f\"\\n Detailed Sampling Statistics:\")\n",
    "    print(stats_df.to_string(index=False))\n",
    "    \n",
    "    # Show first few rows as preview\n",
    "    print(f\"\\n👀 Preview of first 5 rows:\")\n",
    "    print(final_df.head()[['qa_id', 'question_type', 'shot_type', 'question']].to_string())\n",
    "\n",
    "else:\n",
    "    print(\" No samples could be extracted from any file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f779841-354a-4f4e-90a4-a7aca93cd078",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
