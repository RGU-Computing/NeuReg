{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a0ea7a-f06b-4791-8d1c-612aabd8e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "import json\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define file mappings for human evaluators\n",
    "FILES = {\n",
    "    'Evaluator_01': 'Ev_Ramita_01.csv',\n",
    "    'Evaluator_02': 'Ev_Sibgha_02.csv',\n",
    "    'Evaluator_03': 'Ev_Saim_03.csv',\n",
    "    'Evaluator_04': 'Ev_Shahzad_04.csv',\n",
    "    'Evaluator_05': 'Ev_Ammar_05.csv'\n",
    "}\n",
    "\n",
    "# Column name mappings (human evaluation uses different column names)\n",
    "COLUMN_MAPPINGS = {\n",
    "    'QA_ID': 'qa_id',\n",
    "    'Question_Type': 'question_type',\n",
    "    'Shot_Type': 'shot_type',\n",
    "    'Question': 'question',\n",
    "    'Answer': 'answer',\n",
    "    'Relevance (1–5)': 'Relevance',\n",
    "    'Accuracy  (1–5)': 'Accuracy',\n",
    "    'Completeness  (1–5)': 'Completeness',\n",
    "    'Fluency  (1–5)': 'Fluency',\n",
    "    'KG Alignment  (1–5)': 'KG_Alignment',\n",
    "    'Evaluator ID': 'evaluator_id',\n",
    "    'Comments': 'comments'\n",
    "}\n",
    "\n",
    "METRICS = ['Relevance', 'Accuracy', 'Completeness', 'Fluency', 'KG_Alignment']\n",
    "NUM_EVALUATORS = 5  # 5 human evaluators\n",
    "\n",
    "class HumanEvaluationAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.df = None\n",
    "        self.majority_vote_df = None\n",
    "        self.evaluator_majority_agreement = {}\n",
    "        self.total_qa_pairs = 160  # Total QA pairs evaluated by humans\n",
    "        self.analysis_report = {\n",
    "            'metadata': {},\n",
    "            'data_loading': {},\n",
    "            'majority_vote_analysis': {},\n",
    "            'overall_metrics': {},\n",
    "            'score_distribution': {},\n",
    "            'shot_type_analysis': {},\n",
    "            'question_type_analysis': {},\n",
    "            'interaction_matrix': {},\n",
    "            'individual_evaluator_analysis': {},\n",
    "            'inter_evaluator_variance': {}\n",
    "        }\n",
    "        \n",
    "    def load_all_data(self):\n",
    "        \"\"\"Load all CSV files and combine them into a single DataFrame.\"\"\"\n",
    "        all_data = []\n",
    "        loading_info = {'files_loaded': [], 'files_failed': [], 'total_records': 0, 'expected_records': 0}\n",
    "        \n",
    "        for evaluator, filename in FILES.items():\n",
    "            try:\n",
    "                df = pd.read_csv(filename)\n",
    "                # Rename columns to match LLM evaluation format\n",
    "                df = df.rename(columns=COLUMN_MAPPINGS)\n",
    "                df['evaluator'] = evaluator\n",
    "                all_data.append(df)\n",
    "                loading_info['files_loaded'].append({\n",
    "                    'filename': filename,\n",
    "                    'evaluator': evaluator,\n",
    "                    'records': len(df)\n",
    "                })\n",
    "            except Exception as e:\n",
    "                loading_info['files_failed'].append({\n",
    "                    'filename': filename,\n",
    "                    'evaluator': evaluator,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "        \n",
    "        if not all_data:\n",
    "            raise ValueError(\"No data files could be loaded!\")\n",
    "        \n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        loading_info['total_records'] = len(combined_df)\n",
    "        loading_info['expected_records'] = self.total_qa_pairs * NUM_EVALUATORS\n",
    "        \n",
    "        # Ensure Overall_Score is calculated if not present\n",
    "        if 'Overall_Score' not in combined_df.columns or combined_df['Overall_Score'].isna().any():\n",
    "            combined_df['Overall_Score'] = combined_df[METRICS].mean(axis=1)\n",
    "        \n",
    "        self.df = combined_df\n",
    "        self.analysis_report['data_loading'] = loading_info\n",
    "        \n",
    "        # Compute majority vote dataframe\n",
    "        self.compute_majority_vote()\n",
    "        \n",
    "        return combined_df\n",
    "    \n",
    "    def compute_majority_vote(self):\n",
    "        \"\"\"\n",
    "        Compute majority vote per QA pair per metric.\n",
    "        Each human evaluator is a judge, so we compare scores across evaluators for the same qa_id.\n",
    "        \"\"\"\n",
    "        majority_analysis = {}\n",
    "        \n",
    "        # Group by qa_id to see how many evaluators evaluated each question\n",
    "        qa_id_counts = self.df.groupby('qa_id')['evaluator'].nunique().reset_index(name='num_evaluators')\n",
    "        \n",
    "        # Debug: Show distribution of evaluator counts per qa_id\n",
    "        count_distribution = qa_id_counts['num_evaluators'].value_counts().sort_index()\n",
    "        majority_analysis['evaluator_distribution'] = {\n",
    "            int(evaluator_count): int(instances) \n",
    "            for evaluator_count, instances in count_distribution.items()\n",
    "        }\n",
    "        \n",
    "        # Filter to keep only qa_ids that were evaluated by all 5 evaluators\n",
    "        valid_qa_ids = qa_id_counts[qa_id_counts['num_evaluators'] == NUM_EVALUATORS]['qa_id'].tolist()\n",
    "        \n",
    "        majority_analysis['total_qa_ids'] = len(qa_id_counts)\n",
    "        majority_analysis['valid_qa_ids'] = len(valid_qa_ids)\n",
    "        \n",
    "        if len(valid_qa_ids) == 0:\n",
    "            majority_analysis['warning'] = f\"No qa_ids were evaluated by all {NUM_EVALUATORS} evaluators!\"\n",
    "            self.majority_vote_df = pd.DataFrame()\n",
    "            self.evaluator_majority_agreement = {}\n",
    "            self.analysis_report['majority_vote_analysis'] = majority_analysis\n",
    "            return self.majority_vote_df\n",
    "        \n",
    "        majority_vote_data = []\n",
    "        agreement_stats_by_metric = {metric: {'full_agreement': 0, 'majority_exists': 0, 'no_majority': 0} for metric in METRICS}\n",
    "        agreement_stats_by_qa = {'all_metrics_majority': 0, 'some_metrics_majority': 0, 'no_metrics_majority': 0}\n",
    "        \n",
    "        for qa_id in valid_qa_ids:\n",
    "            # Get all evaluations for this qa_id (one from each evaluator)\n",
    "            qa_data = self.df[self.df['qa_id'] == qa_id]\n",
    "            \n",
    "            if len(qa_data) != NUM_EVALUATORS:\n",
    "                continue  # Skip if we don't have exactly NUM_EVALUATORS evaluations\n",
    "            \n",
    "            # Get metadata from the first row\n",
    "            first_row = qa_data.iloc[0]\n",
    "            row_data = {\n",
    "                'qa_id': qa_id,\n",
    "                'question_type': first_row['question_type'],\n",
    "                'shot_type': first_row['shot_type'],\n",
    "                'question': first_row['question'],\n",
    "                'answer': first_row['answer']\n",
    "            }\n",
    "            \n",
    "            metrics_with_majority = 0\n",
    "            \n",
    "            # Compute majority vote for each metric\n",
    "            for metric in METRICS:\n",
    "                # Get scores from all evaluators for this metric\n",
    "                scores = qa_data[metric].values\n",
    "                score_counts = Counter(scores)\n",
    "                \n",
    "                if not score_counts:\n",
    "                    row_data[f'{metric}_majority'] = np.nan\n",
    "                    continue\n",
    "                \n",
    "                # Get the most common score and its count\n",
    "                majority_score, majority_count = score_counts.most_common(1)[0]\n",
    "                row_data[f'{metric}_majority'] = majority_score\n",
    "                row_data[f'{metric}_majority_count'] = majority_count\n",
    "                \n",
    "                # Check for full agreement (all evaluators gave the same score)\n",
    "                if len(score_counts) == 1:\n",
    "                    agreement_stats_by_metric[metric]['full_agreement'] += 1\n",
    "                    metrics_with_majority += 1\n",
    "                # Check for simple majority (> NUM_EVALUATORS / 2)\n",
    "                elif majority_count > NUM_EVALUATORS / 2:\n",
    "                    agreement_stats_by_metric[metric]['majority_exists'] += 1\n",
    "                    metrics_with_majority += 1\n",
    "                else:\n",
    "                    # No clear majority (tie or no score exceeds half)\n",
    "                    agreement_stats_by_metric[metric]['no_majority'] += 1\n",
    "            \n",
    "            # Track QA-level agreement\n",
    "            if metrics_with_majority == len(METRICS):\n",
    "                agreement_stats_by_qa['all_metrics_majority'] += 1\n",
    "            elif metrics_with_majority > 0:\n",
    "                agreement_stats_by_qa['some_metrics_majority'] += 1\n",
    "            else:\n",
    "                agreement_stats_by_qa['no_metrics_majority'] += 1\n",
    "            \n",
    "            # Compute overall score from majority votes\n",
    "            majority_scores = [row_data[f'{metric}_majority'] for metric in METRICS if f'{metric}_majority' in row_data]\n",
    "            row_data['Overall_Score_majority'] = np.mean(majority_scores) if majority_scores else np.nan\n",
    "            \n",
    "            majority_vote_data.append(row_data)\n",
    "        \n",
    "        self.majority_vote_df = pd.DataFrame(majority_vote_data)\n",
    "        \n",
    "        # Store agreement statistics\n",
    "        majority_analysis['agreement_by_metric'] = agreement_stats_by_metric\n",
    "        majority_analysis['agreement_by_qa'] = agreement_stats_by_qa\n",
    "        majority_analysis['total_majority_vote_pairs'] = len(self.majority_vote_df)\n",
    "        \n",
    "        # Calculate per-evaluator agreement statistics\n",
    "        evaluator_agreement_stats = {}\n",
    "        \n",
    "        for evaluator in self.df['evaluator'].unique():\n",
    "            total_comparisons = 0\n",
    "            agreements = 0\n",
    "            \n",
    "            # For each qa_id in majority vote results\n",
    "            for _, maj_row in self.majority_vote_df.iterrows():\n",
    "                qa_id = maj_row['qa_id']\n",
    "                \n",
    "                # Get this evaluator's evaluation for this qa_id\n",
    "                evaluator_eval = self.df[(self.df['qa_id'] == qa_id) & (self.df['evaluator'] == evaluator)]\n",
    "                \n",
    "                if len(evaluator_eval) == 0:\n",
    "                    continue\n",
    "                \n",
    "                evaluator_row = evaluator_eval.iloc[0]\n",
    "                \n",
    "                # Compare evaluator's scores with majority votes\n",
    "                for metric in METRICS:\n",
    "                    if f'{metric}_majority' in maj_row and not pd.isna(maj_row[f'{metric}_majority']):\n",
    "                        majority_vote = maj_row[f'{metric}_majority']\n",
    "                        evaluator_vote = evaluator_row[metric]\n",
    "                        \n",
    "                        total_comparisons += 1\n",
    "                        if evaluator_vote == majority_vote:\n",
    "                            agreements += 1\n",
    "            \n",
    "            if total_comparisons > 0:\n",
    "                agreement_pct = (agreements / total_comparisons) * 100\n",
    "                evaluator_agreement_stats[evaluator] = {\n",
    "                    'agreements': agreements,\n",
    "                    'total': total_comparisons,\n",
    "                    'percentage': agreement_pct\n",
    "                }\n",
    "        \n",
    "        self.evaluator_majority_agreement = evaluator_agreement_stats\n",
    "        majority_analysis['evaluator_agreement_stats'] = evaluator_agreement_stats\n",
    "        self.analysis_report['majority_vote_analysis'] = majority_analysis\n",
    "        \n",
    "        return self.majority_vote_df\n",
    "    \n",
    "    def normalize_to_percentage(self, score, max_score=5):\n",
    "        \"\"\"Convert score to percentage (0-100 scale).\"\"\"\n",
    "        return (score / max_score) * 100\n",
    "    \n",
    "    def compute_overall_metrics(self):\n",
    "        \"\"\"1. Compute mean and std dev per metric across all QA pairs.\"\"\"\n",
    "        results = {}\n",
    "        for metric in METRICS:\n",
    "            mean_score = self.df[metric].mean()\n",
    "            std_score = self.df[metric].std()\n",
    "            mean_pct = self.normalize_to_percentage(mean_score)\n",
    "            \n",
    "            results[metric] = {\n",
    "                'mean': float(mean_score),\n",
    "                'std': float(std_score),\n",
    "                'mean_pct': float(mean_pct)\n",
    "            }\n",
    "        \n",
    "        # Overall score (mean of all metrics)\n",
    "        overall_mean = self.df['Overall_Score'].mean()\n",
    "        overall_std = self.df['Overall_Score'].std()\n",
    "        overall_pct = self.normalize_to_percentage(overall_mean)\n",
    "        \n",
    "        results['Overall_Score'] = {\n",
    "            'mean': float(overall_mean),\n",
    "            'std': float(overall_std),\n",
    "            'mean_pct': float(overall_pct)\n",
    "        }\n",
    "        \n",
    "        self.analysis_report['overall_metrics'] = results\n",
    "        return results\n",
    "    \n",
    "    def compute_overall_score_distribution(self):\n",
    "        \"\"\"2. Compute overall score distribution.\"\"\"\n",
    "        overall_scores = self.df['Overall_Score']\n",
    "        \n",
    "        distribution = {\n",
    "            'statistics': {\n",
    "                'mean': float(overall_scores.mean()),\n",
    "                'std': float(overall_scores.std()),\n",
    "                'min': float(overall_scores.min()),\n",
    "                'max': float(overall_scores.max())\n",
    "            },\n",
    "            'ranges': {}\n",
    "        }\n",
    "        \n",
    "        # Distribution by ranges\n",
    "        ranges = [(1.0, 2.0), (2.0, 3.0), (3.0, 4.0), (4.0, 5.0)]\n",
    "        \n",
    "        for start, end in ranges:\n",
    "            if start == 4.0:  # Include 5.0 in the last range\n",
    "                count = ((overall_scores >= start) & (overall_scores <= end)).sum()\n",
    "            else:\n",
    "                count = ((overall_scores >= start) & (overall_scores < end)).sum()\n",
    "            pct = (count / len(overall_scores)) * 100\n",
    "            \n",
    "            distribution['ranges'][f'{start:.1f}-{end:.1f}'] = {\n",
    "                'count': int(count),\n",
    "                'percentage': float(pct)\n",
    "            }\n",
    "        \n",
    "        self.analysis_report['score_distribution'] = distribution\n",
    "        return distribution\n",
    "    \n",
    "    def compute_breakdown_by_shot_type(self):\n",
    "        \"\"\"3. Breakdown by shot type.\"\"\"\n",
    "        shot_type_results = {}\n",
    "        \n",
    "        for shot_type in sorted([s for s in self.df['shot_type'].dropna().unique() if str(s).strip()]):\n",
    "            shot_df = self.df[self.df['shot_type'] == shot_type]\n",
    "            count = len(shot_df)\n",
    "            unique_qa = shot_df['qa_id'].nunique()\n",
    "            \n",
    "            results = {}\n",
    "            for metric in METRICS:\n",
    "                mean_score = shot_df[metric].mean()\n",
    "                std_score = shot_df[metric].std()\n",
    "                mean_pct = self.normalize_to_percentage(mean_score)\n",
    "                \n",
    "                results[metric] = {\n",
    "                    'mean': float(mean_score), \n",
    "                    'std': float(std_score), \n",
    "                    'mean_pct': float(mean_pct)\n",
    "                }\n",
    "            \n",
    "            # Overall score\n",
    "            overall_mean = shot_df['Overall_Score'].mean()\n",
    "            overall_std = shot_df['Overall_Score'].std()\n",
    "            overall_pct = self.normalize_to_percentage(overall_mean)\n",
    "            \n",
    "            shot_type_results[shot_type] = {\n",
    "                'count': count,\n",
    "                'unique_qa': unique_qa,\n",
    "                'metrics': results,\n",
    "                'overall': {\n",
    "                    'mean': float(overall_mean), \n",
    "                    'std': float(overall_std), \n",
    "                    'pct': float(overall_pct)\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        self.analysis_report['shot_type_analysis'] = shot_type_results\n",
    "        return shot_type_results\n",
    "    \n",
    "    def compute_breakdown_by_question_type(self):\n",
    "        \"\"\"4. Breakdown by question type.\"\"\"\n",
    "        question_type_results = {}\n",
    "        \n",
    "        for q_type in sorted([q for q in self.df['question_type'].dropna().unique() if str(q).strip()]):\n",
    "            q_df = self.df[self.df['question_type'] == q_type]\n",
    "            count = len(q_df)\n",
    "            unique_qa = q_df['qa_id'].nunique()\n",
    "            \n",
    "            results = {}\n",
    "            for metric in METRICS:\n",
    "                mean_score = q_df[metric].mean()\n",
    "                std_score = q_df[metric].std()\n",
    "                mean_pct = self.normalize_to_percentage(mean_score)\n",
    "                \n",
    "                results[metric] = {\n",
    "                    'mean': float(mean_score), \n",
    "                    'std': float(std_score), \n",
    "                    'mean_pct': float(mean_pct)\n",
    "                }\n",
    "            \n",
    "            # Overall score\n",
    "            overall_mean = q_df['Overall_Score'].mean()\n",
    "            overall_std = q_df['Overall_Score'].std()\n",
    "            overall_pct = self.normalize_to_percentage(overall_mean)\n",
    "            \n",
    "            question_type_results[q_type] = {\n",
    "                'count': count,\n",
    "                'unique_qa': unique_qa,\n",
    "                'metrics': results,\n",
    "                'overall': {\n",
    "                    'mean': float(overall_mean), \n",
    "                    'std': float(overall_std), \n",
    "                    'pct': float(overall_pct)\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        self.analysis_report['question_type_analysis'] = question_type_results\n",
    "        return question_type_results\n",
    "    \n",
    "    def compute_shot_type_x_question_type_matrix(self):\n",
    "        \"\"\"5. Create 2D matrix of shot type × question type.\"\"\"\n",
    "        # Create pivot table\n",
    "        pivot_df = self.df.pivot_table(\n",
    "            values='Overall_Score',\n",
    "            index='question_type',\n",
    "            columns='shot_type',\n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        \n",
    "        # Convert to dictionary format for JSON serialization\n",
    "        matrix_data = {}\n",
    "        shot_types = sorted([s for s in self.df['shot_type'].dropna().unique() if str(s).strip()])\n",
    "        q_types = sorted([q for q in self.df['question_type'].dropna().unique() if str(q).strip()])\n",
    "        \n",
    "        for q_type in q_types:\n",
    "            matrix_data[q_type] = {}\n",
    "            for shot_type in shot_types:\n",
    "                if q_type in pivot_df.index and shot_type in pivot_df.columns:\n",
    "                    score = pivot_df.loc[q_type, shot_type]\n",
    "                    if pd.notna(score):\n",
    "                        pct = self.normalize_to_percentage(score)\n",
    "                        matrix_data[q_type][shot_type] = {\n",
    "                            'score': float(score),\n",
    "                            'percentage': float(pct)\n",
    "                        }\n",
    "                    else:\n",
    "                        matrix_data[q_type][shot_type] = None\n",
    "                else:\n",
    "                    matrix_data[q_type][shot_type] = None\n",
    "        \n",
    "        self.analysis_report['interaction_matrix'] = matrix_data\n",
    "        return matrix_data\n",
    "    \n",
    "    def compute_individual_evaluator_analysis(self):\n",
    "        \"\"\"6. Individual evaluator analysis by question type and shot type.\"\"\"\n",
    "        evaluators = self.df['evaluator'].unique()\n",
    "        evaluator_analysis = {}\n",
    "        \n",
    "        for evaluator in sorted(evaluators):\n",
    "            evaluator_df = self.df[self.df['evaluator'] == evaluator]\n",
    "            \n",
    "            # 6.1 Overall performance for this evaluator\n",
    "            overall_performance = {}\n",
    "            for metric in METRICS:\n",
    "                mean_score = evaluator_df[metric].mean()\n",
    "                std_score = evaluator_df[metric].std()\n",
    "                mean_pct = self.normalize_to_percentage(mean_score)\n",
    "                overall_performance[metric] = {\n",
    "                    'mean': float(mean_score),\n",
    "                    'std': float(std_score),\n",
    "                    'mean_pct': float(mean_pct)\n",
    "                }\n",
    "            \n",
    "            overall_mean = evaluator_df['Overall_Score'].mean()\n",
    "            overall_std = evaluator_df['Overall_Score'].std()\n",
    "            overall_pct = self.normalize_to_percentage(overall_mean)\n",
    "            overall_performance['Overall_Score'] = {\n",
    "                'mean': float(overall_mean),\n",
    "                'std': float(overall_std),\n",
    "                'mean_pct': float(overall_pct)\n",
    "            }\n",
    "            \n",
    "            # 6.2 By Question Type\n",
    "            by_question_type = {}\n",
    "            for q_type in sorted([q for q in evaluator_df['question_type'].dropna().unique() if str(q).strip()]):\n",
    "                q_evaluator_df = evaluator_df[evaluator_df['question_type'] == q_type]\n",
    "                count = len(q_evaluator_df)\n",
    "                \n",
    "                overall_mean = q_evaluator_df['Overall_Score'].mean()\n",
    "                overall_pct = self.normalize_to_percentage(overall_mean)\n",
    "                \n",
    "                by_question_type[q_type] = {\n",
    "                    'mean': float(overall_mean),\n",
    "                    'percentage': float(overall_pct),\n",
    "                    'count': count\n",
    "                }\n",
    "            \n",
    "            # 6.3 By Shot Type\n",
    "            by_shot_type = {}\n",
    "            for shot_type in sorted([s for s in evaluator_df['shot_type'].dropna().unique() if str(s).strip()]):\n",
    "                shot_evaluator_df = evaluator_df[evaluator_df['shot_type'] == shot_type]\n",
    "                count = len(shot_evaluator_df)\n",
    "                \n",
    "                overall_mean = shot_evaluator_df['Overall_Score'].mean()\n",
    "                overall_pct = self.normalize_to_percentage(overall_mean)\n",
    "                \n",
    "                by_shot_type[shot_type] = {\n",
    "                    'mean': float(overall_mean),\n",
    "                    'percentage': float(overall_pct),\n",
    "                    'count': count\n",
    "                }\n",
    "            \n",
    "            # 6.4 Shot Type × Question Type Matrix for this evaluator\n",
    "            pivot_df = evaluator_df.pivot_table(\n",
    "                values='Overall_Score',\n",
    "                index='question_type',\n",
    "                columns='shot_type',\n",
    "                aggfunc='mean'\n",
    "            )\n",
    "            \n",
    "            matrix = {}\n",
    "            if not pivot_df.empty:\n",
    "                shot_types = sorted([s for s in evaluator_df['shot_type'].dropna().unique() if str(s).strip()])\n",
    "                q_types = sorted([q for q in evaluator_df['question_type'].dropna().unique() if str(q).strip()])\n",
    "                \n",
    "                for q_type in q_types:\n",
    "                    matrix[q_type] = {}\n",
    "                    for shot_type in shot_types:\n",
    "                        if q_type in pivot_df.index and shot_type in pivot_df.columns:\n",
    "                            score = pivot_df.loc[q_type, shot_type]\n",
    "                            if pd.notna(score):\n",
    "                                pct = self.normalize_to_percentage(score)\n",
    "                                matrix[q_type][shot_type] = {\n",
    "                                    'score': float(score),\n",
    "                                    'percentage': float(pct)\n",
    "                                }\n",
    "                            else:\n",
    "                                matrix[q_type][shot_type] = None\n",
    "                        else:\n",
    "                            matrix[q_type][shot_type] = None\n",
    "            \n",
    "            evaluator_analysis[evaluator] = {\n",
    "                'overall_performance': overall_performance,\n",
    "                'by_question_type': by_question_type,\n",
    "                'by_shot_type': by_shot_type,\n",
    "                'interaction_matrix': matrix\n",
    "            }\n",
    "        \n",
    "        self.analysis_report['individual_evaluator_analysis'] = evaluator_analysis\n",
    "        return evaluator_analysis\n",
    "        \n",
    "    def compute_inter_evaluator_variance(self):\n",
    "        \"\"\"7. Compute inter-evaluator variance for each metric.\"\"\"\n",
    "        # For each qa_id, compute the variance across evaluators\n",
    "        variance_data = []\n",
    "        \n",
    "        qa_ids = self.df['qa_id'].unique()\n",
    "        for qa_id in qa_ids:\n",
    "            qa_data = self.df[self.df['qa_id'] == qa_id]\n",
    "            \n",
    "            if len(qa_data) < 2:  # Need at least 2 evaluators for variance\n",
    "                continue\n",
    "            \n",
    "            variance_row = {'qa_id': qa_id}\n",
    "            for metric in METRICS:\n",
    "                scores = qa_data[metric].values\n",
    "                variance_row[f'{metric}_variance'] = np.var(scores)\n",
    "                variance_row[f'{metric}_std'] = np.std(scores)\n",
    "            \n",
    "            variance_row['Overall_Score_variance'] = np.var(qa_data['Overall_Score'].values)\n",
    "            variance_row['Overall_Score_std'] = np.std(qa_data['Overall_Score'].values)\n",
    "            \n",
    "            variance_data.append(variance_row)\n",
    "        \n",
    "        variance_df = pd.DataFrame(variance_data)\n",
    "        \n",
    "        # Calculate average variance and std dev for each metric\n",
    "        variance_analysis = {}\n",
    "        for metric in METRICS:\n",
    "            if f'{metric}_variance' in variance_df.columns:\n",
    "                avg_var = variance_df[f'{metric}_variance'].mean()\n",
    "                avg_std = variance_df[f'{metric}_std'].mean()\n",
    "                variance_analysis[metric] = {\n",
    "                    'average_variance': float(avg_var),\n",
    "                    'average_std': float(avg_std)\n",
    "                }\n",
    "        \n",
    "        if 'Overall_Score_variance' in variance_df.columns:\n",
    "            avg_overall_var = variance_df['Overall_Score_variance'].mean()\n",
    "            avg_overall_std = variance_df['Overall_Score_std'].mean()\n",
    "            variance_analysis['Overall_Score'] = {\n",
    "                'average_variance': float(avg_overall_var),\n",
    "                'average_std': float(avg_overall_std)\n",
    "            }\n",
    "        \n",
    "        self.analysis_report['inter_evaluator_variance'] = variance_analysis\n",
    "        return variance_analysis\n",
    "    \n",
    "    def generate_analysis_report(self):\n",
    "        \"\"\"Generate and save comprehensive analysis report.\"\"\"\n",
    "        # Add metadata\n",
    "        self.analysis_report['metadata'] = {\n",
    "            'analysis_timestamp': datetime.now().isoformat(),\n",
    "            'analysis_type': 'Human Evaluation',\n",
    "            'total_evaluators': len(self.df['evaluator'].unique()) if self.df is not None else 0,\n",
    "            'total_shot_types': len([s for s in self.df['shot_type'].dropna().unique() if str(s).strip()]) if self.df is not None else 0,\n",
    "            'total_question_types': len([q for q in self.df['question_type'].dropna().unique() if str(q).strip()]) if self.df is not None else 0,\n",
    "            'total_records': len(self.df) if self.df is not None else 0,\n",
    "            'expected_qa_pairs': self.total_qa_pairs,\n",
    "            'metrics_evaluated': METRICS,\n",
    "            'num_evaluators': NUM_EVALUATORS\n",
    "        }\n",
    "        \n",
    "        # Save as JSON\n",
    "        with open('human_evaluation_analysis_report.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.analysis_report, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        return self.analysis_report\n",
    "    \n",
    "    def print_summary_report(self):\n",
    "        \"\"\"Print a formatted summary of the analysis.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"COMPREHENSIVE HUMAN-BASED QA EVALUATION ANALYSIS REPORT\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Metadata\n",
    "        metadata = self.analysis_report['metadata']\n",
    "        print(f\"\\nANALYSIS METADATA:\")\n",
    "        print(f\"  Timestamp: {metadata['analysis_timestamp']}\")\n",
    "        print(f\"  Total Evaluators: {metadata['total_evaluators']}\")\n",
    "        print(f\"  Total Records: {metadata['total_records']}\")\n",
    "        print(f\"  Expected QA Pairs: {metadata['expected_qa_pairs']}\")\n",
    "        print(f\"  Metrics: {', '.join(metadata['metrics_evaluated'])}\")\n",
    "        \n",
    "        # Data Loading Summary\n",
    "        loading = self.analysis_report['data_loading']\n",
    "        print(f\"\\nDATA LOADING SUMMARY:\")\n",
    "        print(f\"  Files Successfully Loaded: {len(loading['files_loaded'])}\")\n",
    "        print(f\"  Files Failed: {len(loading['files_failed'])}\")\n",
    "        print(f\"  Total Records: {loading['total_records']}\")\n",
    "        print(f\"  Expected Records: {loading['expected_records']}\")\n",
    "        \n",
    "        # Overall Metrics\n",
    "        overall = self.analysis_report['overall_metrics']\n",
    "        print(f\"\\nOVERALL PERFORMANCE:\")\n",
    "        for metric, stats in overall.items():\n",
    "            print(f\"  {metric:15s}: {stats['mean']:.3f} ± {stats['std']:.3f} ({stats['mean_pct']:.1f}%)\")\n",
    "        \n",
    "        # Best Performing Evaluators (by overall score)\n",
    "        evaluator_scores = []\n",
    "        for evaluator, data in self.analysis_report['individual_evaluator_analysis'].items():\n",
    "            overall_score = data['overall_performance']['Overall_Score']['mean']\n",
    "            evaluator_scores.append((evaluator, overall_score))\n",
    "        \n",
    "        evaluator_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        print(f\"\\nEVALUATOR RANKING (by Overall Score):\")\n",
    "        for i, (evaluator, score) in enumerate(evaluator_scores, 1):\n",
    "            pct = self.normalize_to_percentage(score)\n",
    "            print(f\"  {i}. {evaluator:30s}: {score:.3f} ({pct:.1f}%)\")\n",
    "        \n",
    "        # Majority Vote Summary\n",
    "        if 'majority_vote_analysis' in self.analysis_report:\n",
    "            majority = self.analysis_report['majority_vote_analysis']\n",
    "            if 'evaluator_agreement_stats' in majority:\n",
    "                print(f\"\\nEVALUATOR AGREEMENT WITH MAJORITY VOTE:\")\n",
    "                for evaluator, stats in majority['evaluator_agreement_stats'].items():\n",
    "                    print(f\"  {evaluator:30s}: {stats['percentage']:.1f}% ({stats['agreements']}/{stats['total']})\")\n",
    "        \n",
    "        # Inter-evaluator Variance\n",
    "        if 'inter_evaluator_variance' in self.analysis_report:\n",
    "            variance = self.analysis_report['inter_evaluator_variance']\n",
    "            print(f\"\\nINTER-EVALUATOR VARIANCE:\")\n",
    "            for metric, stats in variance.items():\n",
    "                print(f\"  {metric:15s}: Avg Variance={stats['average_variance']:.3f}, Avg Std={stats['average_std']:.3f}\")\n",
    "        \n",
    "        print(f\"\\nDETAILED REPORT SAVED TO: human_evaluation_analysis_report.json\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    def run_all_analyses(self):\n",
    "        \"\"\"Main function to run all analyses and generate single report.\"\"\"\n",
    "        # Load all data (this also computes majority vote)\n",
    "        self.load_all_data()\n",
    "        \n",
    "        # Run all analyses\n",
    "        self.compute_overall_metrics()\n",
    "        self.compute_overall_score_distribution()\n",
    "        self.compute_breakdown_by_shot_type()\n",
    "        self.compute_breakdown_by_question_type()\n",
    "        self.compute_shot_type_x_question_type_matrix()\n",
    "        self.compute_individual_evaluator_analysis()\n",
    "        self.compute_inter_evaluator_variance()\n",
    "        \n",
    "        # Generate comprehensive report\n",
    "        self.generate_analysis_report()\n",
    "        \n",
    "        # Print summary\n",
    "        self.print_summary_report()\n",
    "        \n",
    "        return self.analysis_report\n",
    "\n",
    "def main():\n",
    "    analyzer = HumanEvaluationAnalyzer()\n",
    "    report = analyzer.run_all_analyses()\n",
    "    return report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
