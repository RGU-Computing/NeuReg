{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0c0dd3-b4e5-4349-8b24-225df4303af3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "                                                 # Ontology Guided KG evaluation\n",
    "# To assess KG quality, we implemented a two-layered LLM-based  validation framework. The first layer, , utilises\n",
    "# DeepSeek R1 to compare triples to the source text, identifying false positives and false negatives. The second layer applies\n",
    "# the KGValidator method to verify ontology conformity, assigning Pass/not pass labels and confidence scores. These layers yield\n",
    "# precision, recall, F1-score, and semantic validity metrics.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Any, Optional\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class EvaluationResult:\n",
    "    \"\"\"Data class to store evaluation results for a chunk\"\"\"\n",
    "    chunk_id: str\n",
    "    precision: float\n",
    "    recall: float\n",
    "    f1_score: float\n",
    "    true_positives: int\n",
    "    false_positives: int\n",
    "    false_negatives: int\n",
    "    total_extracted: int\n",
    "    total_ground_truth: int\n",
    "    false_positive_triples: List[str]\n",
    "    false_negative_facts: List[str]\n",
    "    ontology_alignment: str\n",
    "    ontology_confidence: float\n",
    "\n",
    "class DeepSeekEvaluator:\n",
    "    \"\"\"Main evaluator class using DeepSeek API\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, api_url: str):\n",
    "        self.api_key = api_key\n",
    "        self.api_url = api_url\n",
    "        self.model = \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\"\n",
    "        self.headers = {\n",
    "            \"Authorization\": f\"Bearer {api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "    \n",
    "    def call_deepseek_api(self, prompt: str, max_retries: int = 3) -> str:\n",
    "        \"\"\"Make API call to DeepSeek with retry logic\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                payload = {\n",
    "                    \"model\": self.model,\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": \"You are an expert knowledge graph evaluator. Provide precise, structured responses following the exact format requested.\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": prompt\n",
    "                        }\n",
    "                    ],\n",
    "                    \"temperature\": 0.1,\n",
    "                    \"max_tokens\": 2000\n",
    "                }\n",
    "                \n",
    "                response = requests.post(self.api_url, headers=self.headers, json=payload, timeout=60)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                result = response.json()\n",
    "                return result['choices'][0]['message']['content'].strip()\n",
    "                \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logger.warning(f\"API call attempt {attempt + 1} failed: {str(e)}\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    raise\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "        \n",
    "        raise Exception(\"Max retries exceeded\")\n",
    "    \n",
    "    def identify_false_positives(self, original_text: str, extracted_triples: List[Dict]) -> List[str]:\n",
    "        \"\"\"Identify incorrectly extracted triples (false positives)\"\"\"\n",
    "        if not extracted_triples:\n",
    "            return []\n",
    "            \n",
    "        triples_str = \"\\n\".join([f\"{t['subject']} -> {t['predicate']} -> {t['object']}\" \n",
    "                                for t in extracted_triples])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "TASK: Identify FALSE POSITIVE triples - triples that are incorrectly extracted from the text.\n",
    "\n",
    "ORIGINAL TEXT:\n",
    "{original_text}\n",
    "\n",
    "EXTRACTED TRIPLES:\n",
    "{triples_str}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Carefully read the original text\n",
    "2. For each extracted triple, determine if it accurately represents information from the text\n",
    "3. A triple is a FALSE POSITIVE if:\n",
    "   - The relationship doesn't exist in the text\n",
    "   - The entities are not mentioned in the text\n",
    "   - The relationship is misrepresented or incorrect\n",
    "   - The triple makes assumptions not supported by the text\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "List each false positive triple exactly as shown above, one per line.\n",
    "If no false positives, respond with \"NONE\"\n",
    "\n",
    "FALSE POSITIVES:\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.call_deepseek_api(prompt)\n",
    "            logger.debug(f\"False positives response: {response[:200]}...\")\n",
    "            \n",
    "            if \"NONE\" in response.upper():\n",
    "                return []\n",
    "            \n",
    "            # Extract false positive triples from response\n",
    "            false_positives = []\n",
    "            lines = response.split('\\n')\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if '->' in line and not line.startswith('FALSE POSITIVES:'):\n",
    "                    false_positives.append(line)\n",
    "            \n",
    "            # Validate that we don't have more FPs than total triples\n",
    "            if len(false_positives) > len(extracted_triples):\n",
    "                logger.warning(f\"Found {len(false_positives)} FPs but only {len(extracted_triples)} triples exist. Limiting to actual triples.\")\n",
    "                # Only keep FPs that match actual extracted triples\n",
    "                validated_fps = []\n",
    "                for fp in false_positives:\n",
    "                    for triple in extracted_triples:\n",
    "                        triple_str = f\"{triple['subject']} -> {triple['predicate']} -> {triple['object']}\"\n",
    "                        if fp.strip() == triple_str.strip():\n",
    "                            validated_fps.append(fp)\n",
    "                            break\n",
    "                false_positives = validated_fps\n",
    "            \n",
    "            logger.info(f\"Identified {len(false_positives)} false positives\")\n",
    "            return false_positives\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error identifying false positives: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def identify_false_negatives(self, original_text: str, extracted_triples: List[Dict]) -> List[str]:\n",
    "        \"\"\"Identify missing facts (false negatives)\"\"\"\n",
    "        triples_str = \"\\n\".join([f\"{t['subject']} -> {t['predicate']} -> {t['object']}\" \n",
    "                                for t in extracted_triples]) if extracted_triples else \"No triples extracted\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "TASK: Identify FALSE NEGATIVE facts - important facts present in the original text but missing from extracted triples.\n",
    "\n",
    "ORIGINAL TEXT:\n",
    "{original_text}\n",
    "\n",
    "EXTRACTED TRIPLES:\n",
    "{triples_str}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Carefully read the original text and identify all factual relationships\n",
    "2. Compare with the extracted triples\n",
    "3. List facts from the text that are NOT captured by any extracted triple\n",
    "4. Focus on important relationships, entities, and attributes mentioned in the text\n",
    "5. Express missing facts in the format: \"Entity -> Relationship -> Entity/Value\"\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "List each missing fact as a potential triple, one per line.\n",
    "If no missing facts, respond with \"NONE\"\n",
    "\n",
    "MISSING FACTS:\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.call_deepseek_api(prompt)\n",
    "            logger.debug(f\"False negatives response: {response[:200]}...\")\n",
    "            \n",
    "            if \"NONE\" in response.upper():\n",
    "                return []\n",
    "            \n",
    "            # Extract missing facts from response\n",
    "            missing_facts = []\n",
    "            lines = response.split('\\n')\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if '->' in line and not line.startswith('MISSING FACTS:'):\n",
    "                    missing_facts.append(line)\n",
    "            \n",
    "            logger.info(f\"Identified {len(missing_facts)} false negatives\")\n",
    "            return missing_facts\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error identifying false negatives: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def evaluate_ontology_alignment(self, original_text: str, extracted_triples: List[Dict], \n",
    "                                  ontology_content: str) -> Tuple[str, float]:\n",
    "        \"\"\"Evaluate if triples align with the ontology\"\"\"\n",
    "        if not extracted_triples:\n",
    "            return \"Not Pass\", 0.0\n",
    "            \n",
    "        triples_str = \"\\n\".join([f\"{t['subject']} -> {t['predicate']} -> {t['object']}\" \n",
    "                                for t in extracted_triples])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "TASK: Judge if the QUESTION (original text) and ANSWER (extracted triples) align well with the ONTOLOGY.\n",
    "\n",
    "ONTOLOGY:\n",
    "{ontology_content[:2000]}  # Truncate if too long\n",
    "\n",
    "QUESTION (Original Text):\n",
    "{original_text}\n",
    "\n",
    "ANSWER (Extracted Triples):\n",
    "{triples_str}\n",
    "\n",
    "EVALUATION CRITERIA:\n",
    "The QUESTION and ANSWER align well with the ONTOLOGY if:\n",
    "1. They are in the same knowledge domain as the ONTOLOGY\n",
    "2. The ANSWER follows the relationships defined in the ONTOLOGY\n",
    "3. The entities and predicates used are consistent with the ontology structure\n",
    "4. The semantic relationships respect the ontology constraints\n",
    "\n",
    "OUTPUT FORMAT (exactly as specified):\n",
    "(your judgment: Pass/Not Pass, confidence score)\n",
    "\n",
    "Where confidence score is between 0.0 and 1.0\n",
    "\n",
    "JUDGMENT:\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.call_deepseek_api(prompt)\n",
    "            logger.debug(f\"Ontology alignment response: {response}\")\n",
    "            \n",
    "            # Parse the response to extract judgment and confidence\n",
    "            # Look for pattern like \"(Pass, 0.85)\" or \"(Not Pass, 0.6)\"\n",
    "            pattern = r'\\((.*?),\\s*([\\d.]+)\\)'\n",
    "            match = re.search(pattern, response)\n",
    "            \n",
    "            if match:\n",
    "                judgment = match.group(1).strip()\n",
    "                confidence = float(match.group(2))\n",
    "                # Validate confidence is in range\n",
    "                confidence = max(0.0, min(1.0, confidence))\n",
    "                return judgment, confidence\n",
    "            else:\n",
    "                # Fallback parsing\n",
    "                if \"Pass\" in response and \"Not Pass\" not in response:\n",
    "                    judgment = \"Pass\"\n",
    "                else:\n",
    "                    judgment = \"Not Pass\"\n",
    "                \n",
    "                # Try to extract confidence score\n",
    "                conf_pattern = r'[\\d.]+(?=\\s*\\)|\\s*$)'\n",
    "                conf_match = re.search(conf_pattern, response)\n",
    "                confidence = float(conf_match.group()) if conf_match else 0.5\n",
    "                confidence = max(0.0, min(1.0, confidence))\n",
    "                \n",
    "                return judgment, confidence\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error parsing ontology alignment response: {e}\")\n",
    "            return \"Not Pass\", 0.0\n",
    "\n",
    "class KnowledgeGraphEvaluator:\n",
    "    \"\"\"Main evaluation orchestrator\"\"\"\n",
    "    \n",
    "    def __init__(self, chunks_file: str, triples_file: str, ontology_file: str, api_key: str):\n",
    "        self.chunks_file = chunks_file\n",
    "        self.triples_file = triples_file\n",
    "        self.ontology_file = ontology_file\n",
    "        self.evaluator = DeepSeekEvaluator(api_key, \"https://inference.api.nscale.com/v1/chat/completions\")\n",
    "        \n",
    "        # Load data\n",
    "        self.chunks_df = pd.read_csv(chunks_file)\n",
    "        self.triples_df = pd.read_csv(triples_file)\n",
    "        self.ontology_content = self.load_ontology()\n",
    "        \n",
    "        logger.info(f\"Loaded {len(self.chunks_df)} chunks and {len(self.triples_df)} triples\")\n",
    "    \n",
    "    def load_ontology(self) -> str:\n",
    "        \"\"\"Load RDF ontology file\"\"\"\n",
    "        try:\n",
    "            with open(self.ontology_file, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                logger.info(f\"Loaded ontology file: {len(content)} characters\")\n",
    "                return content\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading ontology: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def get_triples_for_chunk(self, chunk_id: str) -> List[Dict]:\n",
    "        \"\"\"Get all triples associated with a chunk ID\"\"\"\n",
    "        chunk_triples = self.triples_df[self.triples_df['chunk_id'] == chunk_id]\n",
    "        triples = [\n",
    "            {\n",
    "                'subject': str(row['subject']),\n",
    "                'predicate': str(row['predicate']),\n",
    "                'object': str(row['object'])\n",
    "            }\n",
    "            for _, row in chunk_triples.iterrows()\n",
    "        ]\n",
    "        logger.debug(f\"Found {len(triples)} triples for chunk {chunk_id}\")\n",
    "        return triples\n",
    "    \n",
    "    def calculate_metrics(self, total_extracted: int, false_positives: int, \n",
    "                         false_negatives: int) -> Tuple[float, float, float]:\n",
    "        \"\"\"Calculate precision, recall, and F1 score with proper validation\"\"\"\n",
    "        \n",
    "        # Validate inputs\n",
    "        if false_positives > total_extracted:\n",
    "            logger.error(f\"Invalid metrics: {false_positives} FPs > {total_extracted} total extracted\")\n",
    "            false_positives = total_extracted\n",
    "        \n",
    "        # Calculate true positives (with protection against negative values)\n",
    "        true_positives = max(0, total_extracted - false_positives)\n",
    "        \n",
    "        # Calculate precision\n",
    "        if total_extracted == 0:\n",
    "            precision = 0.0\n",
    "        else:\n",
    "            precision = true_positives / total_extracted\n",
    "        \n",
    "        # Calculate ground truth (what should have been extracted)\n",
    "        # Ground truth = True Positives + False Negatives\n",
    "        total_ground_truth = true_positives + false_negatives\n",
    "        \n",
    "        # Calculate recall\n",
    "        if total_ground_truth == 0:\n",
    "            recall = 0.0\n",
    "        else:\n",
    "            recall = true_positives / total_ground_truth\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        if precision + recall == 0:\n",
    "            f1_score = 0.0\n",
    "        else:\n",
    "            f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "        # Log the calculation for debugging\n",
    "        logger.debug(f\"Metrics calculation: TP={true_positives}, FP={false_positives}, FN={false_negatives}, \"\n",
    "                    f\"Precision={precision:.3f}, Recall={recall:.3f}, F1={f1_score:.3f}\")\n",
    "        \n",
    "        return precision, recall, f1_score\n",
    "    \n",
    "    def evaluate_chunk(self, chunk_id: str) -> Optional[EvaluationResult]:\n",
    "        \"\"\"Evaluate a single chunk\"\"\"\n",
    "        logger.info(f\"Evaluating chunk: {chunk_id}\")\n",
    "        \n",
    "        try:\n",
    "            # Get chunk text\n",
    "            chunk_row = self.chunks_df[self.chunks_df['chunk_id'] == chunk_id]\n",
    "            if chunk_row.empty:\n",
    "                logger.error(f\"Chunk ID {chunk_id} not found\")\n",
    "                return None\n",
    "            \n",
    "            original_text = str(chunk_row.iloc[0]['text'])\n",
    "            \n",
    "            # Get associated triples\n",
    "            extracted_triples = self.get_triples_for_chunk(chunk_id)\n",
    "            \n",
    "            if not extracted_triples:\n",
    "                logger.warning(f\"No triples found for chunk {chunk_id}\")\n",
    "                return EvaluationResult(\n",
    "                    chunk_id=chunk_id,\n",
    "                    precision=0.0, recall=0.0, f1_score=0.0,\n",
    "                    true_positives=0, false_positives=0, false_negatives=0,\n",
    "                    total_extracted=0, total_ground_truth=0,\n",
    "                    false_positive_triples=[], false_negative_facts=[],\n",
    "                    ontology_alignment=\"Not Pass\", ontology_confidence=0.0\n",
    "                )\n",
    "            \n",
    "            # Identify false positives\n",
    "            false_positive_triples = self.evaluator.identify_false_positives(original_text, extracted_triples)\n",
    "            \n",
    "            # Identify false negatives\n",
    "            false_negative_facts = self.evaluator.identify_false_negatives(original_text, extracted_triples)\n",
    "            \n",
    "            # Evaluate ontology alignment\n",
    "            ontology_judgment, ontology_confidence = self.evaluator.evaluate_ontology_alignment(\n",
    "                original_text, extracted_triples, self.ontology_content\n",
    "            )\n",
    "            \n",
    "            # Calculate metrics\n",
    "            total_extracted = len(extracted_triples)\n",
    "            false_positives = len(false_positive_triples)\n",
    "            false_negatives = len(false_negative_facts)\n",
    "            \n",
    "            # Validate false positives\n",
    "            if false_positives > total_extracted:\n",
    "                logger.warning(f\"Capping FPs: {false_positives} > {total_extracted}\")\n",
    "                false_positives = total_extracted\n",
    "                false_positive_triples = false_positive_triples[:total_extracted]\n",
    "            \n",
    "            precision, recall, f1_score = self.calculate_metrics(\n",
    "                total_extracted, false_positives, false_negatives\n",
    "            )\n",
    "            \n",
    "            true_positives = max(0, total_extracted - false_positives)\n",
    "            total_ground_truth = true_positives + false_negatives\n",
    "            \n",
    "            result = EvaluationResult(\n",
    "                chunk_id=chunk_id,\n",
    "                precision=precision,\n",
    "                recall=recall,\n",
    "                f1_score=f1_score,\n",
    "                true_positives=true_positives,\n",
    "                false_positives=false_positives,\n",
    "                false_negatives=false_negatives,\n",
    "                total_extracted=total_extracted,\n",
    "                total_ground_truth=total_ground_truth,\n",
    "                false_positive_triples=false_positive_triples,\n",
    "                false_negative_facts=false_negative_facts,\n",
    "                ontology_alignment=ontology_judgment,\n",
    "                ontology_confidence=ontology_confidence\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Chunk {chunk_id} evaluated: F1={f1_score:.3f}, P={precision:.3f}, R={recall:.3f}, \"\n",
    "                       f\"TP={true_positives}, FP={false_positives}, FN={false_negatives}\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error evaluating chunk {chunk_id}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "    \n",
    "    def evaluate_all(self, max_chunks: int = None) -> List[EvaluationResult]:\n",
    "        # iterate over every chunk, even if it has no triples\n",
    "        unique_chunk_ids = self.triples_df['chunk_id'].unique()\n",
    "        total_chunks    = len(unique_chunk_ids)\n",
    "\n",
    "        \n",
    "        results = []\n",
    "        total_chunks = len(unique_chunk_ids)\n",
    "        \n",
    "        for i, chunk_id in enumerate(unique_chunk_ids):\n",
    "            try:\n",
    "                logger.info(f\"Processing chunk {i+1}/{total_chunks}: {chunk_id}\")\n",
    "                result = self.evaluate_chunk(chunk_id)\n",
    "                if result is not None:\n",
    "                    results.append(result)\n",
    "                \n",
    "                # Add delay to avoid rate limiting\n",
    "                if i < total_chunks - 1:  # Don't sleep after last chunk\n",
    "                    time.sleep(1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error evaluating chunk {chunk_id}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        logger.info(f\"Successfully evaluated {len(results)} out of {total_chunks} chunks\")\n",
    "        return results\n",
    "    \n",
    "    def generate_report(self, results: List[EvaluationResult]) -> Dict[str, Any]:\n",
    "        \"\"\"Generate comprehensive evaluation report\"\"\"\n",
    "        if not results:\n",
    "            return {\"error\": \"No results to report\"}\n",
    "        \n",
    "        # Calculate overall metrics\n",
    "        total_tp = sum(r.true_positives for r in results)\n",
    "        total_fp = sum(r.false_positives for r in results)\n",
    "        total_fn = sum(r.false_negatives for r in results)\n",
    "        total_extracted = sum(r.total_extracted for r in results)\n",
    "        \n",
    "        # Use the same validated calculation method\n",
    "        overall_precision, overall_recall, overall_f1 = self.calculate_metrics(\n",
    "            total_extracted, total_fp, total_fn\n",
    "        )\n",
    "        \n",
    "        # Ground truth is TP + FN\n",
    "        total_ground_truth = total_tp + total_fn\n",
    "        \n",
    "        # Ontology alignment statistics\n",
    "        pass_count = sum(1 for r in results if r.ontology_alignment == \"Pass\")\n",
    "        avg_confidence = sum(r.ontology_confidence for r in results) / len(results) if results else 0\n",
    "        \n",
    "        # Calculate per-chunk statistics\n",
    "        chunk_metrics = []\n",
    "        for result in results:\n",
    "            chunk_metrics.append({\n",
    "                'chunk_id': result.chunk_id,\n",
    "                'precision': result.precision,\n",
    "                'recall': result.recall,\n",
    "                'f1_score': result.f1_score,\n",
    "                'extracted_triples': result.total_extracted,\n",
    "                'true_positives': result.true_positives,\n",
    "                'false_positives': result.false_positives,\n",
    "                'false_negatives': result.false_negatives,\n",
    "                'ground_truth': result.total_ground_truth,\n",
    "                'ontology_alignment': result.ontology_alignment,\n",
    "                'ontology_confidence': result.ontology_confidence\n",
    "            })\n",
    "        \n",
    "        # Sort chunks by F1 score\n",
    "        chunk_metrics_sorted = sorted(chunk_metrics, key=lambda x: x['f1_score'], reverse=True)\n",
    "        \n",
    "        report = {\n",
    "            'summary': {\n",
    "                'total_chunks_evaluated': len(results),\n",
    "                'average_f1_score': overall_f1,\n",
    "                'average_precision': overall_precision,\n",
    "                'average_recall': overall_recall,\n",
    "                'ontology_pass_rate': pass_count / len(results) if results else 0\n",
    "            },\n",
    "            'overall_metrics': {\n",
    "                'precision': overall_precision,\n",
    "                'recall': overall_recall,\n",
    "                'f1_score': overall_f1,\n",
    "                'total_extracted_triples': total_extracted,\n",
    "                'total_true_positives': total_tp,\n",
    "                'total_false_positives': total_fp,\n",
    "                'total_false_negatives': total_fn,\n",
    "                'total_ground_truth_facts': total_ground_truth\n",
    "            },\n",
    "            'ontology_alignment': {\n",
    "                'pass_rate': pass_count / len(results) if results else 0,\n",
    "                'pass_count': pass_count,\n",
    "                'total_chunks': len(results),\n",
    "                'average_confidence': avg_confidence\n",
    "            },\n",
    "            'best_performing_chunks': chunk_metrics_sorted[:5],\n",
    "            'worst_performing_chunks': chunk_metrics_sorted[-5:],\n",
    "            'chunk_level_metrics': chunk_metrics,\n",
    "            'detailed_results': results\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def save_results(self, results: List[EvaluationResult], output_file: str):\n",
    "        \"\"\"Save detailed results to CSV\"\"\"\n",
    "        data = []\n",
    "        for result in results:\n",
    "            data.append({\n",
    "                'chunk_id': result.chunk_id,\n",
    "                'precision': round(result.precision, 4),\n",
    "                'recall': round(result.recall, 4),\n",
    "                'f1_score': round(result.f1_score, 4),\n",
    "                'true_positives': result.true_positives,\n",
    "                'false_positives': result.false_positives,\n",
    "                'false_negatives': result.false_negatives,\n",
    "                'total_extracted': result.total_extracted,\n",
    "                'total_ground_truth': result.total_ground_truth,\n",
    "                'ontology_alignment': result.ontology_alignment,\n",
    "                'ontology_confidence': round(result.ontology_confidence, 4),\n",
    "                'false_positive_triples': '; '.join(result.false_positive_triples),\n",
    "                'false_negative_facts': '; '.join(result.false_negative_facts)\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(output_file, index=False)\n",
    "        logger.info(f\"Results saved to {output_file}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Configuration\n",
    "    API_KEY = \"Put your api key here\" \n",
    "    CHUNKS_FILE = \"chunks.csv\"\n",
    "    TRIPLES_FILE = \"Ontology_Guided_Triples.csv\"\n",
    "    ONTOLOGY_FILE = \"EFRO.rdf\"\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    try:\n",
    "        evaluator = KnowledgeGraphEvaluator(\n",
    "            chunks_file=CHUNKS_FILE,\n",
    "            triples_file=TRIPLES_FILE,\n",
    "            ontology_file=ONTOLOGY_FILE,\n",
    "            api_key=API_KEY\n",
    "        )\n",
    "        \n",
    "        # Run evaluation\n",
    "        logger.info(\"Starting evaluation...\")\n",
    "        results = evaluator.evaluate_all()  \n",
    "        \n",
    "        if not results:\n",
    "            logger.error(\"No results generated\")\n",
    "            return\n",
    "        \n",
    "        # Generate and print report\n",
    "        report = evaluator.generate_report(results)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"KNOWLEDGE GRAPH EVALUATION REPORT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\\nSUMMARY:\")\n",
    "        print(f\"Chunks Evaluated: {report['summary']['total_chunks_evaluated']}\")\n",
    "        print(f\"Average F1 Score: {report['summary']['average_f1_score']:.3f}\")\n",
    "        print(f\"Average Precision: {report['summary']['average_precision']:.3f}\")\n",
    "        print(f\"Average Recall: {report['summary']['average_recall']:.3f}\")\n",
    "        print(f\"Ontology Pass Rate: {report['summary']['ontology_pass_rate']:.1%}\")\n",
    "        \n",
    "        print(f\"\\nOVERALL METRICS:\")\n",
    "        print(f\"Total Extracted Triples: {report['overall_metrics']['total_extracted_triples']}\")\n",
    "        print(f\"True Positives: {report['overall_metrics']['total_true_positives']}\")\n",
    "        print(f\"False Positives: {report['overall_metrics']['total_false_positives']}\")\n",
    "        print(f\"False Negatives: {report['overall_metrics']['total_false_negatives']}\")\n",
    "        print(f\"Ground Truth Facts: {report['overall_metrics']['total_ground_truth_facts']}\")\n",
    "        \n",
    "        print(f\"\\nONTOLOGY ALIGNMENT:\")\n",
    "        print(f\"Pass Rate: {report['ontology_alignment']['pass_rate']:.1%}\")\n",
    "        print(f\"Passed Chunks: {report['ontology_alignment']['pass_count']}/{report['ontology_alignment']['total_chunks']}\")\n",
    "        print(f\"Average Confidence: {report['ontology_alignment']['average_confidence']:.3f}\")\n",
    "        \n",
    "        print(f\"\\nTOP 5 BEST PERFORMING CHUNKS:\")\n",
    "        for i, chunk in enumerate(report['best_performing_chunks'][:5], 1):\n",
    "            print(f\"{i}. {chunk['chunk_id']}: F1={chunk['f1_score']:.3f}, P={chunk['precision']:.3f}, R={chunk['recall']:.3f}\")\n",
    "        \n",
    "        print(f\"\\nBOTTOM 5 WORST PERFORMING CHUNKS:\")\n",
    "        for i, chunk in enumerate(report['worst_performing_chunks'][:5], 1):\n",
    "            print(f\"{i}. {chunk['chunk_id']}: F1={chunk['f1_score']:.3f}, P={chunk['precision']:.3f}, R={chunk['recall']:.3f}\")\n",
    "        \n",
    "        # Save detailed results\n",
    "        evaluator.save_results(results, \"evaluation_results.csv\")\n",
    "        \n",
    "        # Save report as JSON\n",
    "        with open(\"evaluation_report.json\", \"w\") as f:\n",
    "            # Convert EvaluationResult objects to dicts for JSON serialization\n",
    "            report_copy = report.copy()\n",
    "            report_copy['detailed_results'] = [\n",
    "                {\n",
    "                    'chunk_id': r.chunk_id,\n",
    "                    'precision': r.precision,\n",
    "                    'recall': r.recall,\n",
    "                    'f1_score': r.f1_score,\n",
    "                    'true_positives': r.true_positives,\n",
    "                    'false_positives': r.false_positives,\n",
    "                    'false_negatives': r.false_negatives,\n",
    "                    'ontology_alignment': r.ontology_alignment,\n",
    "                    'ontology_confidence': r.ontology_confidence,\n",
    "                    'false_positive_triples': r.false_positive_triples,\n",
    "                    'false_negative_facts': r.false_negative_facts\n",
    "                }\n",
    "                for r in results\n",
    "            ]\n",
    "            json.dump(report_copy, f, indent=2)\n",
    "        \n",
    "        logger.info(\"\\nEvaluation completed successfully!\")\n",
    "        logger.info(\"Output files:\")\n",
    "        logger.info(\"- evaluation_results.csv (detailed metrics)\")\n",
    "        logger.info(\"- evaluation_report.json (complete report)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Evaluation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af5a625-8474-4af4-9365-90a14b797e25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
