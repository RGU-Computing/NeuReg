## 🧠 Ontology-Guided Knowledge Graph Evaluation
This module evaluates the quality and semantic validity of knowledge graph (KG) triples generated by the NeuReg framework using a two-layered LLM-based validation system. It combines text-grounded validation using DeepSeek-R1 with ontology conformity checking, enabling a comprehensive assessment of both factual accuracy and structural alignment.

----

### 📊 Summary of Evaluation Results

| Metric                   | Value |
| ------------------------ | ----- |
| **Average F1 Score**     | 0.880 |
| **Average Precision**    | 0.936 |
| **Average Recall**       | 0.829 |
| Total Extracted Triples  | 1891  |
| True Positives           | 1770  |
| False Positives          | 121   |
| False Negatives          | 364   |
| Total Ground Truth Facts | 2134  |

---
### 🧬 Ontology Alignment Results

| Metric              | Value     |
| ------------------- | --------- |
| **Pass Rate**       | 98.0%     |
| Pass Count          | 150 / 153 |
| **Avg. Confidence** | 0.913     |

---

### ⚙️ Evaluation Pipeline
The evaluation process is divided into two validation layers:

#### 🔹 Layer 1: Triple Validation via LLM (DeepSeek-R1)
This layer compares extracted triples with the source chunk text to identify:

✅ True Positives (TP): Correctly extracted triples

❌ False Positives (FP): Triples that do not exist in the original text

⚠️ False Negatives (FN): Facts from the text not captured in the triples

##### 🔸 Prompt for False Positives
```text
TASK: Identify FALSE POSITIVE triples - triples that are incorrectly extracted from the text.

ORIGINAL TEXT:
{original_text}

EXTRACTED TRIPLES:
{subject} -> {predicate} -> {object}
...

INSTRUCTIONS:
1. Carefully read the original text.
2. For each extracted triple, determine if it accurately represents information from the text.
3. A triple is a FALSE POSITIVE if:
   - The relationship doesn't exist in the text.
   - The entities are not mentioned in the text.
   - The relationship is misrepresented or incorrect.
   - The triple makes assumptions not supported by the text.

OUTPUT FORMAT:
List each false positive triple exactly as shown above, one per line.
If no false positives, respond with "NONE"
```text

##### 🔸 Prompt for False Negatives
```text
TASK: Identify FALSE NEGATIVE facts - important facts present in the original text but missing from extracted triples.

ORIGINAL TEXT:
{original_text}

EXTRACTED TRIPLES:
{subject} -> {predicate} -> {object}
...

INSTRUCTIONS:
1. Carefully read the original text and identify all factual relationships.
2. Compare with the extracted triples.
3. List facts from the text that are NOT captured by any extracted triple.
4. Focus on important relationships, entities, and attributes mentioned in the text.
5. Express missing facts in the format: "Entity -> Relationship -> Entity/Value".

OUTPUT FORMAT:
List each missing fact as a potential triple, one per line.
If no missing facts, respond with "NONE"
```text

#### 🔹 Layer 2: Ontology Alignment Check

This layer checks whether extracted triples align with the domain ontology using LLM reasoning.

##### 🔸 Prompt for Ontology Alignment
```text
TASK: Judge if the QUESTION (original text) and ANSWER (extracted triples) align well with the ONTOLOGY.

ONTOLOGY:
{ontology_content}

QUESTION (Original Text):
{original_text}

ANSWER (Extracted Triples):
{subject} -> {predicate} -> {object}
...

EVALUATION CRITERIA:
The QUESTION and ANSWER align well with the ONTOLOGY if:
1. They are in the same knowledge domain as the ONTOLOGY.
2. The ANSWER follows the relationships defined in the ONTOLOGY.
3. The entities and predicates used are consistent with the ontology structure.
4. The semantic relationships respect the ontology constraints.

OUTPUT FORMAT (exactly as specified):
(your judgment: Pass/Not Pass, confidence score)

Where confidence score is between 0.0 and 1.0
```text

---

### 📂 Folder Contents

| File                          | Description                                                                          |
| ----------------------------- | ------------------------------------------------------------------------------------ |
| `Evaluation.ipynb`            | Main notebook to run the full KG evaluation                                          |
| `evaluation_results.csv`      | Output file with detailed chunk-level metrics                                        |
| `evaluation_report.json`      | Summary report including precision, recall, F1, alignment, and chunk-level breakdown |

**Note** You must add your DeepSeek API key in the configuration section before running the main script:

API_KEY = "Put your api key here"

----

### 📈 Outputs
After running the script, you will obtain:

✅ evaluation_results.csv: F1, Precision, Recall, TP, FP, FN, alignment results for each chunk

✅ evaluation_report.json: Full summary and top/bottom performing chunks
