{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "265340a8-e90b-4e7d-8626-2542fbfcf4ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MULTI-DATASET EVALUATION WITH Mixtral-8x22b-instruct-v0.1\n",
      "================================================================================\n",
      "Model: mistralai/mixtral-8x22b-instruct-v0.1\n",
      "Datasets to evaluate: 3\n",
      "  1. Zero-Shot: Zero-Shot_qa_dataset.json\n",
      "  2. One-Shot: One-Shot_qa_dataset.json\n",
      "  3. Few-Shot: Few-Shot_qa_dataset.json\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Proceed with multi-dataset evaluation? (y/n):  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 1127 QA pairs from Zero-Shot_qa_dataset.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROCESSING DATASET 1/3: Zero-Shot\n",
      "============================================================\n",
      "Input: Zero-Shot_qa_dataset.json\n",
      "Output: mixtral-8x22b-instruct-v0.1_zeroshot_evaluation_results.csv\n",
      "Model: mistralai/mixtral-8x22b-instruct-v0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating QA pairs:   4%| | 49/1127 [06:35<2:08:36,  7.16s/items, Completed=50,INFO:__main__:Checkpoint saved: 50 results to mixtral-8x22b-instruct-v0.1_zeroshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:   9%| | 99/1127 [13:11<2:45:56,  9.69s/items, Completed=100INFO:__main__:Checkpoint saved: 100 results to mixtral-8x22b-instruct-v0.1_zeroshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  13%|â–| 149/1127 [18:51<1:54:37,  7.03s/items, Completed=15INFO:__main__:Checkpoint saved: 150 results to mixtral-8x22b-instruct-v0.1_zeroshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  18%|â–| 199/1127 [24:28<2:04:17,  8.04s/items, Completed=20INFO:__main__:Checkpoint saved: 200 results to mixtral-8x22b-instruct-v0.1_zeroshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  22%|â–| 249/1127 [30:03<2:06:59,  8.68s/items, Completed=25INFO:__main__:Checkpoint saved: 250 results to mixtral-8x22b-instruct-v0.1_zeroshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  27%|â–Ž| 299/1127 [35:41<1:21:30,  5.91s/items, Completed=30INFO:__main__:Checkpoint saved: 300 results to mixtral-8x22b-instruct-v0.1_zeroshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  31%|â–Ž| 349/1127 [42:16<1:51:18,  8.58s/items, Completed=35INFO:__main__:Checkpoint saved: 350 results to mixtral-8x22b-instruct-v0.1_zeroshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  35%|â–Ž| 399/1127 [47:37<53:03,  4.37s/items, Completed=400,INFO:__main__:Checkpoint saved: 400 results to mixtral-8x22b-instruct-v0.1_zeroshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  40%|â–| 449/1127 [54:17<1:21:24,  7.20s/items, Completed=45INFO:__main__:Checkpoint saved: 450 results to mixtral-8x22b-instruct-v0.1_zeroshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  44%|â–| 499/1127 [1:01:43<1:40:25,  9.59s/items, Completed=INFO:__main__:Checkpoint saved: 500 results to mixtral-8x22b-instruct-v0.1_zeroshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  49%|â–| 549/1127 [1:08:56<1:44:36, 10.86s/items, Completed=INFO:__main__:Checkpoint saved: 550 results to mixtral-8x22b-instruct-v0.1_zeroshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  53%|â–Œ| 599/1127 [1:16:05<1:21:30,  9.26s/items, Completed=INFO:__main__:Checkpoint saved: 600 results to mixtral-8x22b-instruct-v0.1_zeroshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  58%|â–Œ| 649/1127 [1:22:53<1:05:39,  8.24s/items, Completed=INFO:__main__:Checkpoint saved: 650 results to mixtral-8x22b-instruct-v0.1_zeroshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  62%|â–Œ| 699/1127 [1:29:24<56:10,  7.87s/items, Completed=70INFO:__main__:Checkpoint saved: 700 results to mixtral-8x22b-instruct-v0.1_zeroshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  66%|â–‹| 749/1127 [1:34:56<45:21,  7.20s/items, Completed=75INFO:__main__:Checkpoint saved: 750 results to mixtral-8x22b-instruct-v0.1_zeroshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  71%|â–‹| 799/1127 [1:40:22<31:15,  5.72s/items, Completed=80INFO:__main__:Checkpoint saved: 800 results to mixtral-8x22b-instruct-v0.1_zeroshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  75%|â–Š| 849/1127 [1:47:25<45:25,  9.80s/items, Completed=85INFO:__main__:Checkpoint saved: 850 results to mixtral-8x22b-instruct-v0.1_zeroshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  80%|â–Š| 899/1127 [1:53:18<37:46,  9.94s/items, Completed=90INFO:__main__:Checkpoint saved: 900 results to mixtral-8x22b-instruct-v0.1_zeroshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  84%|â–Š| 949/1127 [1:59:51<26:56,  9.08s/items, Completed=95INFO:__main__:Checkpoint saved: 950 results to mixtral-8x22b-instruct-v0.1_zeroshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  89%|â–‰| 999/1127 [2:04:53<09:12,  4.32s/items, Completed=10INFO:__main__:Checkpoint saved: 1000 results to mixtral-8x22b-instruct-v0.1_zeroshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  93%|â–‰| 1049/1127 [2:09:59<05:41,  4.38s/items, Completed=1INFO:__main__:Checkpoint saved: 1050 results to mixtral-8x22b-instruct-v0.1_zeroshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  98%|â–‰| 1099/1127 [2:15:48<02:19,  4.98s/items, Completed=1INFO:__main__:Checkpoint saved: 1100 results to mixtral-8x22b-instruct-v0.1_zeroshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs: 100%|â–ˆ| 1127/1127 [2:19:09<00:00,  7.41s/items, Completed=1\n",
      "INFO:__main__:Loaded 1080 QA pairs from One-Shot_qa_dataset.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      " IMPROVED LLM JUDGE EVALUATION SUMMARY\n",
      "======================================================================\n",
      " Evaluation Statistics:\n",
      "   Total Evaluated: 1127\n",
      "   Failed Evaluations: 0\n",
      "   Success Rate: 100.0%\n",
      "   Evaluation Time: 2025-07-17 00:27:58\n",
      "\n",
      " Average Scores by Metric:\n",
      "   Relevance: 4.98 Â± 0.15 (range: 3-5)\n",
      "   Accuracy: 4.97 Â± 0.18 (range: 3-5)\n",
      "   Completeness: 4.88 Â± 0.34 (range: 3-5)\n",
      "   Fluency: 5.00 Â± 0.00 (range: 5-5)\n",
      "   KG_Alignment: 4.93 Â± 0.30 (range: 1-5)\n",
      "\n",
      " Overall Performance:\n",
      "   Mean Overall Score: 4.95\n",
      "   Median Overall Score: 5.00\n",
      "   Best Score: 5.00\n",
      "   Worst Score: 3.20\n",
      "\n",
      " Score Distribution:\n",
      "   1.0-2.0: 0 (0.0%)\n",
      "   2.0-3.0: 0 (0.0%)\n",
      "   3.0-4.0: 3 (0.3%)\n",
      "   4.0-5.0: 149 (13.2%)\n",
      "   Perfect (5.0): 975 (86.5%)\n",
      "\n",
      " Performance by Question Type:\n",
      "   comparative: 4.92 Â± 0.20 (n=270.0)\n",
      "   factual: 4.97 Â± 0.08 (n=305.0)\n",
      "   inferential: 4.95 Â± 0.16 (n=275.0)\n",
      "   relationship: 4.95 Â± 0.17 (n=277.0)\n",
      "\n",
      " Zero-Shot EVALUATION COMPLETED!\n",
      "   Duration: 139.2 minutes\n",
      "   Evaluated: 1127 QA pairs\n",
      "   Results saved to: mixtral-8x22b-instruct-v0.1_zeroshot_evaluation_results.csv\n",
      "   Average Score: 4.95\n",
      "\n",
      "============================================================\n",
      "PROCESSING DATASET 2/3: One-Shot\n",
      "============================================================\n",
      "Input: One-Shot_qa_dataset.json\n",
      "Output: mixtral-8x22b-instruct-v0.1_oneshot_evaluation_results.csv\n",
      "Model: mistralai/mixtral-8x22b-instruct-v0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating QA pairs:   5%| | 49/1080 [05:59<2:04:37,  7.25s/items, Completed=50,INFO:__main__:Checkpoint saved: 50 results to mixtral-8x22b-instruct-v0.1_oneshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:   9%| | 99/1080 [12:21<2:12:20,  8.09s/items, Completed=100INFO:__main__:Checkpoint saved: 100 results to mixtral-8x22b-instruct-v0.1_oneshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  14%|â–| 149/1080 [17:31<1:16:35,  4.94s/items, Completed=15INFO:__main__:Checkpoint saved: 150 results to mixtral-8x22b-instruct-v0.1_oneshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  18%|â–| 199/1080 [24:07<2:25:42,  9.92s/items, Completed=20INFO:__main__:Checkpoint saved: 200 results to mixtral-8x22b-instruct-v0.1_oneshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  23%|â–| 249/1080 [30:59<2:04:02,  8.96s/items, Completed=25INFO:__main__:Checkpoint saved: 250 results to mixtral-8x22b-instruct-v0.1_oneshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  28%|â–Ž| 299/1080 [36:44<1:51:48,  8.59s/items, Completed=30INFO:__main__:Checkpoint saved: 300 results to mixtral-8x22b-instruct-v0.1_oneshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  32%|â–Ž| 349/1080 [43:40<1:37:38,  8.01s/items, Completed=35INFO:__main__:Checkpoint saved: 350 results to mixtral-8x22b-instruct-v0.1_oneshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  37%|â–Ž| 399/1080 [49:18<1:00:58,  5.37s/items, Completed=40INFO:__main__:Checkpoint saved: 400 results to mixtral-8x22b-instruct-v0.1_oneshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  42%|â–| 449/1080 [56:21<1:50:57, 10.55s/items, Completed=45INFO:__main__:Checkpoint saved: 450 results to mixtral-8x22b-instruct-v0.1_oneshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  46%|â–| 499/1080 [1:03:28<1:33:21,  9.64s/items, Completed=INFO:__main__:Checkpoint saved: 500 results to mixtral-8x22b-instruct-v0.1_oneshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  51%|â–Œ| 549/1080 [1:09:39<42:08,  4.76s/items, Completed=55INFO:__main__:Checkpoint saved: 550 results to mixtral-8x22b-instruct-v0.1_oneshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  55%|â–Œ| 599/1080 [1:16:27<1:15:37,  9.43s/items, Completed=INFO:__main__:Checkpoint saved: 600 results to mixtral-8x22b-instruct-v0.1_oneshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  60%|â–Œ| 649/1080 [1:22:34<44:46,  6.23s/items, Completed=65INFO:__main__:Checkpoint saved: 650 results to mixtral-8x22b-instruct-v0.1_oneshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  65%|â–‹| 699/1080 [1:29:33<1:03:46, 10.04s/items, Completed=INFO:__main__:Checkpoint saved: 700 results to mixtral-8x22b-instruct-v0.1_oneshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  69%|â–‹| 749/1080 [1:34:30<34:23,  6.23s/items, Completed=75INFO:__main__:Checkpoint saved: 750 results to mixtral-8x22b-instruct-v0.1_oneshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  74%|â–‹| 799/1080 [1:40:54<23:45,  5.07s/items, Completed=80INFO:__main__:Checkpoint saved: 800 results to mixtral-8x22b-instruct-v0.1_oneshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  79%|â–Š| 849/1080 [1:47:44<31:05,  8.08s/items, Completed=85INFO:__main__:Checkpoint saved: 850 results to mixtral-8x22b-instruct-v0.1_oneshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  83%|â–Š| 899/1080 [1:54:43<24:00,  7.96s/items, Completed=90INFO:__main__:Checkpoint saved: 900 results to mixtral-8x22b-instruct-v0.1_oneshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  88%|â–‰| 949/1080 [2:00:32<17:32,  8.04s/items, Completed=95INFO:__main__:Checkpoint saved: 950 results to mixtral-8x22b-instruct-v0.1_oneshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  92%|â–‰| 999/1080 [2:06:13<10:34,  7.83s/items, Completed=10INFO:__main__:Checkpoint saved: 1000 results to mixtral-8x22b-instruct-v0.1_oneshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  97%|â–‰| 1049/1080 [2:12:29<02:20,  4.54s/items, Completed=1INFO:__main__:Checkpoint saved: 1050 results to mixtral-8x22b-instruct-v0.1_oneshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs: 100%|â–ˆ| 1080/1080 [2:16:26<00:00,  7.58s/items, Completed=1\n",
      "INFO:__main__:Loaded 1107 QA pairs from Few-Shot_qa_dataset.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      " IMPROVED LLM JUDGE EVALUATION SUMMARY\n",
      "======================================================================\n",
      " Evaluation Statistics:\n",
      "   Total Evaluated: 1080\n",
      "   Failed Evaluations: 0\n",
      "   Success Rate: 100.0%\n",
      "   Evaluation Time: 2025-07-17 02:44:25\n",
      "\n",
      " Average Scores by Metric:\n",
      "   Relevance: 4.98 Â± 0.14 (range: 3-5)\n",
      "   Accuracy: 4.96 Â± 0.21 (range: 3-5)\n",
      "   Completeness: 4.84 Â± 0.39 (range: 3-5)\n",
      "   Fluency: 5.00 Â± 0.00 (range: 5-5)\n",
      "   KG_Alignment: 4.92 Â± 0.29 (range: 3-5)\n",
      "\n",
      " Overall Performance:\n",
      "   Mean Overall Score: 4.94\n",
      "   Median Overall Score: 5.00\n",
      "   Best Score: 5.00\n",
      "   Worst Score: 3.40\n",
      "\n",
      " Score Distribution:\n",
      "   1.0-2.0: 0 (0.0%)\n",
      "   2.0-3.0: 0 (0.0%)\n",
      "   3.0-4.0: 6 (0.6%)\n",
      "   4.0-5.0: 199 (18.4%)\n",
      "   Perfect (5.0): 875 (81.0%)\n",
      "\n",
      " Performance by Question Type:\n",
      "   comparative: 4.93 Â± 0.17 (n=275.0)\n",
      "   factual: 4.97 Â± 0.08 (n=281.0)\n",
      "   inferential: 4.92 Â± 0.23 (n=269.0)\n",
      "   relationship: 4.93 Â± 0.13 (n=255.0)\n",
      "\n",
      " One-Shot EVALUATION COMPLETED!\n",
      "   Duration: 136.4 minutes\n",
      "   Evaluated: 1080 QA pairs\n",
      "   Results saved to: mixtral-8x22b-instruct-v0.1_oneshot_evaluation_results.csv\n",
      "   Average Score: 4.94\n",
      "\n",
      "============================================================\n",
      "PROCESSING DATASET 3/3: Few-Shot\n",
      "============================================================\n",
      "Input: Few-Shot_qa_dataset.json\n",
      "Output: mixtral-8x22b-instruct-v0.1_fewshot_evaluation_results.csv\n",
      "Model: mistralai/mixtral-8x22b-instruct-v0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating QA pairs:   4%| | 49/1107 [06:10<2:32:13,  8.63s/items, Completed=50,INFO:__main__:Checkpoint saved: 50 results to mixtral-8x22b-instruct-v0.1_fewshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:   9%| | 99/1107 [13:05<2:40:25,  9.55s/items, Completed=100INFO:__main__:Checkpoint saved: 100 results to mixtral-8x22b-instruct-v0.1_fewshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  13%|â–| 149/1107 [19:07<1:51:49,  7.00s/items, Completed=15INFO:__main__:Checkpoint saved: 150 results to mixtral-8x22b-instruct-v0.1_fewshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  18%|â–| 199/1107 [25:30<2:14:06,  8.86s/items, Completed=20INFO:__main__:Checkpoint saved: 200 results to mixtral-8x22b-instruct-v0.1_fewshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  22%|â–| 249/1107 [32:35<2:15:38,  9.49s/items, Completed=25INFO:__main__:Checkpoint saved: 250 results to mixtral-8x22b-instruct-v0.1_fewshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  27%|â–Ž| 299/1107 [38:49<1:55:42,  8.59s/items, Completed=30INFO:__main__:Checkpoint saved: 300 results to mixtral-8x22b-instruct-v0.1_fewshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  32%|â–Ž| 349/1107 [44:52<1:09:50,  5.53s/items, Completed=35INFO:__main__:Checkpoint saved: 350 results to mixtral-8x22b-instruct-v0.1_fewshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  36%|â–Ž| 399/1107 [50:16<1:37:33,  8.27s/items, Completed=40INFO:__main__:Checkpoint saved: 400 results to mixtral-8x22b-instruct-v0.1_fewshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  41%|â–| 449/1107 [57:20<1:36:49,  8.83s/items, Completed=45INFO:__main__:Checkpoint saved: 450 results to mixtral-8x22b-instruct-v0.1_fewshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  45%|â–| 499/1107 [1:04:59<46:45,  4.61s/items, Completed=50INFO:__main__:Checkpoint saved: 500 results to mixtral-8x22b-instruct-v0.1_fewshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  50%|â–| 549/1107 [1:12:31<1:31:40,  9.86s/items, Completed=INFO:__main__:Checkpoint saved: 550 results to mixtral-8x22b-instruct-v0.1_fewshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  54%|â–Œ| 599/1107 [1:18:53<54:30,  6.44s/items, Completed=60INFO:__main__:Checkpoint saved: 600 results to mixtral-8x22b-instruct-v0.1_fewshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  59%|â–Œ| 649/1107 [1:25:55<1:07:56,  8.90s/items, Completed=INFO:__main__:Checkpoint saved: 650 results to mixtral-8x22b-instruct-v0.1_fewshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  63%|â–‹| 699/1107 [1:31:43<41:02,  6.04s/items, Completed=70INFO:__main__:Checkpoint saved: 700 results to mixtral-8x22b-instruct-v0.1_fewshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  68%|â–‹| 749/1107 [1:37:12<39:53,  6.69s/items, Completed=75INFO:__main__:Checkpoint saved: 750 results to mixtral-8x22b-instruct-v0.1_fewshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  72%|â–‹| 799/1107 [1:43:20<47:45,  9.30s/items, Completed=80INFO:__main__:Checkpoint saved: 800 results to mixtral-8x22b-instruct-v0.1_fewshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  77%|â–Š| 849/1107 [1:50:11<19:53,  4.62s/items, Completed=85INFO:__main__:Checkpoint saved: 850 results to mixtral-8x22b-instruct-v0.1_fewshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  81%|â–Š| 899/1107 [1:56:46<29:24,  8.48s/items, Completed=90INFO:__main__:Checkpoint saved: 900 results to mixtral-8x22b-instruct-v0.1_fewshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  86%|â–Š| 949/1107 [2:02:46<21:34,  8.19s/items, Completed=95INFO:__main__:Checkpoint saved: 950 results to mixtral-8x22b-instruct-v0.1_fewshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  90%|â–‰| 999/1107 [2:08:15<12:27,  6.92s/items, Completed=10INFO:__main__:Checkpoint saved: 1000 results to mixtral-8x22b-instruct-v0.1_fewshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  95%|â–‰| 1049/1107 [2:13:40<03:28,  3.60s/items, Completed=1INFO:__main__:Checkpoint saved: 1050 results to mixtral-8x22b-instruct-v0.1_fewshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs:  99%|â–‰| 1099/1107 [2:20:21<01:17,  9.64s/items, Completed=1INFO:__main__:Checkpoint saved: 1100 results to mixtral-8x22b-instruct-v0.1_fewshot_evaluation_results.csv.checkpoint\n",
      "Evaluating QA pairs: 100%|â–ˆ| 1107/1107 [2:21:18<00:00,  7.66s/items, Completed=1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      " IMPROVED LLM JUDGE EVALUATION SUMMARY\n",
      "======================================================================\n",
      " Evaluation Statistics:\n",
      "   Total Evaluated: 1107\n",
      "   Failed Evaluations: 0\n",
      "   Success Rate: 100.0%\n",
      "   Evaluation Time: 2025-07-17 05:05:43\n",
      "\n",
      " Average Scores by Metric:\n",
      "   Relevance: 4.98 Â± 0.17 (range: 2-5)\n",
      "   Accuracy: 4.96 Â± 0.24 (range: 1-5)\n",
      "   Completeness: 4.86 Â± 0.40 (range: 1-5)\n",
      "   Fluency: 5.00 Â± 0.00 (range: 5-5)\n",
      "   KG_Alignment: 4.89 Â± 0.36 (range: 1-5)\n",
      "\n",
      " Overall Performance:\n",
      "   Mean Overall Score: 4.94\n",
      "   Median Overall Score: 5.00\n",
      "   Best Score: 5.00\n",
      "   Worst Score: 2.00\n",
      "\n",
      " Score Distribution:\n",
      "   1.0-2.0: 0 (0.0%)\n",
      "   2.0-3.0: 2 (0.2%)\n",
      "   3.0-4.0: 0 (0.0%)\n",
      "   4.0-5.0: 195 (17.6%)\n",
      "   Perfect (5.0): 910 (82.2%)\n",
      "\n",
      " Performance by Question Type:\n",
      "   comparative: 4.94 Â± 0.16 (n=265.0)\n",
      "   factual: 4.95 Â± 0.13 (n=281.0)\n",
      "   inferential: 4.91 Â± 0.29 (n=271.0)\n",
      "   relationship: 4.95 Â± 0.14 (n=290.0)\n",
      "\n",
      " Few-Shot EVALUATION COMPLETED!\n",
      "   Duration: 141.3 minutes\n",
      "   Evaluated: 1107 QA pairs\n",
      "   Results saved to: mixtral-8x22b-instruct-v0.1_fewshot_evaluation_results.csv\n",
      "   Average Score: 4.94\n",
      "\n",
      "================================================================================\n",
      "MULTI-DATASET EVALUATION SUMMARY\n",
      "================================================================================\n",
      "Total Duration: 416.9 minutes\n",
      "Model Used: mistralai/mixtral-8x22b-instruct-v0.1\n",
      "Datasets Processed: 3/3\n",
      "\n",
      "ðŸ“Š COMPARATIVE ANALYSIS:\n",
      "Combined results saved to: Mixtral-8x22b-instruct-v0.1_combined_evaluation_results.csv\n",
      "Total QA pairs evaluated: 3314\n",
      "\n",
      "ðŸ“ˆ PERFORMANCE BY DATASET:\n",
      "   Few-Shot: 4.94 Â± 0.19 (n=1107.0)\n",
      "   One-Shot: 4.94 Â± 0.16 (n=1080.0)\n",
      "   Zero-Shot: 4.95 Â± 0.16 (n=1127.0)\n",
      "\n",
      " OVERALL PERFORMANCE:\n",
      "   Mean Overall Score: 4.94\n",
      "   Best Dataset: Zero-Shot\n",
      "   Score Range: 2.00 - 5.00\n",
      "\n",
      "âš¡ PROCESSING EFFICIENCY:\n",
      "   Items per minute: 7.9\n",
      "   Average time per item: 7.55 seconds\n",
      "\n",
      "================================================================================\n",
      "EVALUATION COMPLETE\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from typing import Dict, List, Optional\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "class ImprovedLLMJudgeEvaluator:\n",
    "    \"\"\"\n",
    "     LLM Judge Evaluation System with detailed scoring criteria \n",
    "    aligned with human evaluation guidelines.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, model_name: str = \"mistralai/mixtral-8x22b-instruct-v0.1\"):\n",
    "        \"\"\"\n",
    "        Initialize the LLM Judge Evaluator.\n",
    "        \n",
    "        Args:\n",
    "            api_key: NScale API key\n",
    "            model_name: Model identifier for the LLM judge\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.model_name = model_name\n",
    "        self.base_url = \"https://inference.api.nscale.com/v1/chat/completions\"\n",
    "        self.headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {api_key}\"\n",
    "        }\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # evaluation prompt with detailed criteria\n",
    "        self.evaluation_prompt = \"\"\"You are a STRICT QA evaluator following precise scoring guidelines.\n",
    "\n",
    "You will evaluate a model-generated question-answer pair on FIVE metrics using a 1-5 scale where:\n",
    "5 = Excellent  \n",
    "4 = Good  \n",
    "3 = Fair  \n",
    "2 = Poor  \n",
    "1 = Very Poor\n",
    "\n",
    "==============================\n",
    "DETAILED SCORING CRITERIA\n",
    "==============================\n",
    "\n",
    "1. **RELEVANCE (1â€“5):** Does the question appropriately relate to the source text?  \n",
    "   5: Perfectly relevant to the source, clearly grounded in the text  \n",
    "   4: Mostly relevant, with minor off-topic elements  \n",
    "   3: Addresses the main question but misses some important points  \n",
    "   2: Loosely related, with significant tangents or irrelevance  \n",
    "   1: Entirely irrelevant or unrelated to the source content\n",
    "\n",
    "2. **ACCURACY (1â€“5):** Is the answer factually correct based on the source text?  \n",
    "   5: All facts are accurate and fully verifiable in the context  \n",
    "   4: Mostly accurate; contains only minor factual issues  \n",
    "   3: Some factual inconsistencies or assumptions  \n",
    "   2: Several factual errors that affect reliability  \n",
    "   1: Mostly inaccurate or misleading information\n",
    "\n",
    "3. **COMPLETENESS (1â€“5):** Does the answer fully address the question?  \n",
    "   5: Thorough and complete response  \n",
    "   4: Covers most parts but misses minor aspects  \n",
    "   3: Addresses main part, omits some key details  \n",
    "   2: Partial answer with significant gaps  \n",
    "   1: Severely incomplete or off-topic\n",
    "\n",
    "4. **FLUENCY (1â€“5):** Is the answer well-written and grammatically correct?  \n",
    "   5: Excellent grammar and clarity; highly readable  \n",
    "   4: Minor grammatical or structural issues  \n",
    "   3: Understandable, but contains noticeable language errors  \n",
    "   2: Somewhat unclear due to poor grammar or phrasing  \n",
    "   1: Difficult to read or understand\n",
    "\n",
    "5. **KG ALIGNMENT (1â€“5):** How well does the answer reflect the KG triples?  \n",
    "   5: Effectively uses KG relationships; no contradictions; may include additional relevant info  \n",
    "   4: Uses most relevant KG information correctly; may omit minor details, no contradictions  \n",
    "   3: Uses some KG information; may miss important relationships but generally consistent  \n",
    "   2: Limited use of KG information; may contain minor contradictions or misinterpretations  \n",
    "   1: Ignores KG information entirely or includes clear contradictions\n",
    "\n",
    "==============================\n",
    "INPUT\n",
    "==============================\n",
    "**Question:** {question}  \n",
    "**Answer:** {answer}  \n",
    "**Source Context:** {source_context}  \n",
    "**Knowledge Graph Triples:** {kg_triples}\n",
    "\n",
    "==============================\n",
    "RESPONSE FORMAT\n",
    "==============================\n",
    "\n",
    "Relevance: X  \n",
    "Accuracy: X  \n",
    "Completeness: X  \n",
    "Fluency: X  \n",
    "KG_Alignment: X\n",
    "\n",
    "Where X is a number from 1 to 5 \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    def extract_triples_from_item(self, item: Dict) -> str:\n",
    "        \"\"\"Extract and format KG triples from a QA item.\"\"\"\n",
    "        def extract_all_triples(obj):\n",
    "            triples = []\n",
    "            if isinstance(obj, dict):\n",
    "                if set(obj.keys()) >= {\"subject\", \"predicate\", \"object\"}:\n",
    "                    triple = f\"{obj['subject']} â†’ {obj['predicate']} â†’ {obj['object']}\"\n",
    "                    triples.append(triple)\n",
    "                for value in obj.values():\n",
    "                    triples.extend(extract_all_triples(value))\n",
    "            elif isinstance(obj, list):\n",
    "                for item in obj:\n",
    "                    triples.extend(extract_all_triples(item))\n",
    "            return triples\n",
    "        \n",
    "        triples = extract_all_triples(item)\n",
    "        if triples:\n",
    "            formatted_triples = []\n",
    "            for i, triple in enumerate(triples, 1):\n",
    "                formatted_triples.append(f\"{i}. {triple}\")\n",
    "            return \"\\n\".join(formatted_triples)\n",
    "        return \"No KG triples available\"\n",
    "\n",
    "    def get_source_context(self, item: Dict) -> str:\n",
    "        \"\"\"Extract source context from QA item.\"\"\"\n",
    "        # Check if source_context is nested under ground_truth\n",
    "        if \"ground_truth\" in item and isinstance(item[\"ground_truth\"], dict):\n",
    "            return item[\"ground_truth\"].get(\"source_context\", \"\")\n",
    "        \n",
    "        # Fallback to root level\n",
    "        return item.get(\"source_context\", \"No source context available\")\n",
    "\n",
    "    def call_llm_judge(self, question: str, answer: str, source_context: str, kg_triples: str, \n",
    "                      max_retries: int = 3) -> Optional[Dict[str, int]]:\n",
    "        \"\"\"\n",
    "        Call the LLM judge to evaluate a QA pair with retry logic.\n",
    "        \"\"\"\n",
    "        # Format the prompt\n",
    "        prompt = self.evaluation_prompt.format(\n",
    "            question=question,\n",
    "            answer=answer,\n",
    "            source_context=source_context,\n",
    "            kg_triples=kg_triples\n",
    "        )\n",
    "        \n",
    "        # Prepare payload\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"temperature\": 0.0,  \n",
    "            \"max_tokens\": 800,\n",
    "            \"top_p\": 0.8\n",
    "        }\n",
    "    \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # API call\n",
    "                response = requests.post(\n",
    "                    self.base_url,\n",
    "                    headers=self.headers,\n",
    "                    json=payload,\n",
    "                    timeout=45\n",
    "                )\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    result = response.json()\n",
    "                    content = result['choices'][0]['message']['content']\n",
    "                    \n",
    "                    if content:\n",
    "                        parsed_result = self.parse_evaluation_response(content)\n",
    "                        if parsed_result:\n",
    "                            return parsed_result\n",
    "                    else:\n",
    "                        self.logger.warning(f\"Empty response content from Mixtral\")\n",
    "                \n",
    "                elif response.status_code == 429:\n",
    "                    wait_time = 2 ** attempt\n",
    "                    self.logger.warning(f\"Rate limit hit, waiting {wait_time}s before retry {attempt + 1}/{max_retries}\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    self.logger.error(f\"API Error {response.status_code}: {response.text}\")\n",
    "            \n",
    "            except requests.exceptions.Timeout:\n",
    "                self.logger.warning(f\"Timeout on attempt {attempt + 1}/{max_retries}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(2 ** attempt)\n",
    "            \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Request failed on attempt {attempt + 1}: {str(e)}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(2 ** attempt)\n",
    "    \n",
    "        return None\n",
    "\n",
    "    def parse_evaluation_response(self, response: str) -> Optional[Dict[str, int]]:\n",
    "        \"\"\"\n",
    "       Enhanced parser that can handle DeepSeek reasoning + extract scores\n",
    "        \"\"\"\n",
    "        try:\n",
    "            scores = {}\n",
    "            \n",
    "            # Method 1: Standard format parsing (existing logic)\n",
    "            lines = response.strip().split('\\n')\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if ':' in line:\n",
    "                    parts = line.split(':', 1)\n",
    "                    if len(parts) == 2:\n",
    "                        metric = parts[0].strip()\n",
    "                        score_text = parts[1].strip()\n",
    "                        \n",
    "                        score_match = re.search(r':\\s*([1-5])(?:/5)?', line)\n",
    "                        if score_match:\n",
    "                            score = int(score_match.group(1))\n",
    "                            \n",
    "                            metric_lower = metric.lower()\n",
    "                            if 'relevance' in metric_lower:\n",
    "                                scores['Relevance'] = score\n",
    "                            elif 'accuracy' in metric_lower:\n",
    "                                scores['Accuracy'] = score\n",
    "                            elif 'completeness' in metric_lower:\n",
    "                                scores['Completeness'] = score\n",
    "                            elif 'fluency' in metric_lower:\n",
    "                                scores['Fluency'] = score\n",
    "                            elif 'kg' in metric_lower or 'alignment' in metric_lower:\n",
    "                                scores['KG_Alignment'] = score\n",
    "            \n",
    "            # If standard parsing fails, extract from reasoning text\n",
    "            expected_metrics = ['Relevance', 'Accuracy', 'Completeness', 'Fluency', 'KG_Alignment']\n",
    "            if len(scores) < 5:\n",
    "                # Extract from reasoning patterns like \"Relevance should be a 5\"\n",
    "                text_lower = response.lower()\n",
    "                \n",
    "                if 'Relevance' not in scores:\n",
    "                    match = re.search(r'relevance.*?(?:should be|is).*?([1-5])', text_lower)\n",
    "                    if match:\n",
    "                        scores['Relevance'] = int(match.group(1))\n",
    "                \n",
    "                if 'Accuracy' not in scores:\n",
    "                    match = re.search(r'accuracy.*?(?:should be|is).*?([1-5])', text_lower)\n",
    "                    if match:\n",
    "                        scores['Accuracy'] = int(match.group(1))\n",
    "                \n",
    "                if 'Completeness' not in scores:\n",
    "                    match = re.search(r'completeness.*?(?:should be|is).*?([1-5])', text_lower)\n",
    "                    if match:\n",
    "                        scores['Completeness'] = int(match.group(1))\n",
    "                \n",
    "                if 'Fluency' not in scores:\n",
    "                    match = re.search(r'fluency.*?(?:should be|is).*?([1-5])', text_lower)\n",
    "                    if match:\n",
    "                        scores['Fluency'] = int(match.group(1))\n",
    "                \n",
    "                if 'KG_Alignment' not in scores:\n",
    "                    match = re.search(r'(?:kg|alignment).*?(?:should be|is).*?([1-5])', text_lower)\n",
    "                    if match:\n",
    "                        scores['KG_Alignment'] = int(match.group(1))\n",
    "            \n",
    "            \n",
    "           \n",
    "            if len(scores) > 0 and len(scores) < 5:\n",
    "                self.logger.warning(f\"Got partial response with {len(scores)} scores: {scores}\")\n",
    "                # Fill missing scores with average of existing ones\n",
    "                if len(scores) >= 2:\n",
    "                    avg_score = round(sum(scores.values()) / len(scores))\n",
    "                    expected_metrics = ['Relevance', 'Accuracy', 'Completeness', 'Fluency', 'KG_Alignment']\n",
    "                    for metric in expected_metrics:\n",
    "                        if metric not in scores:\n",
    "                            scores[metric] = avg_score\n",
    "                            self.logger.info(f\"Filled missing {metric} with average {avg_score}\")\n",
    "                    return scores\n",
    "            \n",
    "            # Final validation (existing code)\n",
    "            if all(metric in scores for metric in expected_metrics):\n",
    "                return scores\n",
    "            else:\n",
    "                self.logger.warning(f\"Missing metrics in response: {response}\")\n",
    "                return self._parse_with_fallback(response)\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to parse response: {response}. Error: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _parse_with_fallback(self, response: str) -> Optional[Dict[str, int]]:\n",
    "        \"\"\"Fallback parsing with multiple regex patterns.\"\"\"\n",
    "        patterns = [\n",
    "            r':\\s*([1-5])(?:/5)?',          #  pattern\n",
    "            r'\\b([1-5])\\b',                 \n",
    "            r'([1-5])\\s*(?:out of 5|/5)?'   \n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            try:\n",
    "                scores = {}\n",
    "                lines = response.strip().split('\\n')\n",
    "                \n",
    "                for line in lines:\n",
    "                    if ':' in line:\n",
    "                        parts = line.split(':', 1)\n",
    "                        if len(parts) == 2:\n",
    "                            metric = parts[0].strip()\n",
    "                            score_text = parts[1].strip()\n",
    "                            \n",
    "                            score_match = re.search(pattern, score_text)\n",
    "                            if score_match:\n",
    "                                score = int(score_match.group(1))\n",
    "                                \n",
    "                                # Normalize metric names\n",
    "                                metric_lower = metric.lower()\n",
    "                                if 'relevance' in metric_lower:\n",
    "                                    scores['Relevance'] = score\n",
    "                                elif 'accuracy' in metric_lower:\n",
    "                                    scores['Accuracy'] = score\n",
    "                                elif 'completeness' in metric_lower:\n",
    "                                    scores['Completeness'] = score\n",
    "                                elif 'fluency' in metric_lower:\n",
    "                                    scores['Fluency'] = score\n",
    "                                elif 'kg' in metric_lower or 'alignment' in metric_lower:\n",
    "                                    scores['KG_Alignment'] = score\n",
    "                \n",
    "                # Check if this pattern worked\n",
    "                expected_metrics = ['Relevance', 'Accuracy', 'Completeness', 'Fluency', 'KG_Alignment']\n",
    "                if all(metric in scores for metric in expected_metrics):\n",
    "                    return scores\n",
    "                    \n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def validate_evaluation_scores(self, evaluation: Dict[str, int], qa_id: str) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Validate and clamp evaluation scores to valid range [1, 5].\n",
    "        \"\"\"\n",
    "        validated = {}\n",
    "        issues = []\n",
    "        \n",
    "        for metric, score in evaluation.items():\n",
    "            original_score = score\n",
    "            \n",
    "            # Handle non-numeric values\n",
    "            if not isinstance(score, (int, float)):\n",
    "                try:\n",
    "                    score = int(score)\n",
    "                except (ValueError, TypeError):\n",
    "                    score = 3  # Default to middle score\n",
    "                    issues.append(f\"{metric}: non-numeric '{original_score}' â†’ {score}\")\n",
    "            \n",
    "            # Clamp to valid range\n",
    "            clamped_score = min(max(int(score), 1), 5)\n",
    "            validated[metric] = clamped_score\n",
    "            \n",
    "            # Log if clamped\n",
    "            if clamped_score != original_score:\n",
    "                issues.append(f\"{metric}: {original_score} â†’ {clamped_score}\")\n",
    "        \n",
    "        # Log any issues\n",
    "        if issues:\n",
    "            self.logger.warning(f\"Score validation issues for {qa_id}: {'; '.join(issues)}\")\n",
    "        \n",
    "        return validated\n",
    "\n",
    "    def load_qa_dataset(self, file_path: str) -> List[Dict]:\n",
    "        \"\"\"Load QA dataset from JSON file.\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            queries = data.get('queries', [])\n",
    "            self.logger.info(f\"Loaded {len(queries)} QA pairs from {file_path}\")\n",
    "            return queries\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load dataset: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def save_checkpoint(self, results: List[Dict], checkpoint_path: str):\n",
    "        \"\"\"Save evaluation results as checkpoint.\"\"\"\n",
    "        try:\n",
    "            df = pd.DataFrame(results)\n",
    "            df.to_csv(checkpoint_path, index=False)\n",
    "            self.logger.info(f\"Checkpoint saved: {len(results)} results to {checkpoint_path}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to save checkpoint: {str(e)}\")\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_path: str) -> List[Dict]:\n",
    "        \"\"\"Load evaluation results from checkpoint.\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(checkpoint_path):\n",
    "                df = pd.read_csv(checkpoint_path)\n",
    "                results = df.to_dict('records')\n",
    "                self.logger.info(f\"Loaded checkpoint: {len(results)} results from {checkpoint_path}\")\n",
    "                return results\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load checkpoint: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def evaluate_dataset(self, dataset_path: str, output_path: str, \n",
    "                        sample_size: Optional[int] = None, \n",
    "                        delay_seconds: float = 1.0,\n",
    "                        checkpoint_interval: int = 50) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Evaluate QA dataset using LLM judge with checkpointing.\n",
    "        \n",
    "        Args:\n",
    "            dataset_path: Path to the QA dataset JSON file\n",
    "            output_path: Path to save evaluation results\n",
    "            sample_size: Number of samples to evaluate (None for all)\n",
    "            delay_seconds: Delay between API calls to avoid rate limits\n",
    "            checkpoint_interval: Save checkpoint every N evaluations\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with evaluation results\n",
    "        \"\"\"\n",
    "        # Load dataset\n",
    "        qa_items = self.load_qa_dataset(dataset_path)\n",
    "        if not qa_items:\n",
    "            self.logger.error(\"No QA items loaded. Exiting.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Sample subset if requested\n",
    "        if sample_size and sample_size < len(qa_items):\n",
    "            import random\n",
    "            random.seed(42)  # For reproducibility\n",
    "            qa_items = random.sample(qa_items, sample_size)\n",
    "            self.logger.info(f\"Sampling {sample_size} items for evaluation\")\n",
    "        \n",
    "        # Setup checkpoint\n",
    "        checkpoint_path = f\"{output_path}.checkpoint\"\n",
    "        results = self.load_checkpoint(checkpoint_path)\n",
    "        \n",
    "        # Track processed items\n",
    "        processed_ids = {result['qa_id'] for result in results} if results else set()\n",
    "        \n",
    "        # Filter unprocessed items\n",
    "        remaining_items = [item for item in qa_items \n",
    "                          if item.get('id', f'item_{qa_items.index(item)}') not in processed_ids]\n",
    "        \n",
    "        if processed_ids:\n",
    "            self.logger.info(f\"Resuming from checkpoint: {len(results)} completed, {len(remaining_items)} remaining\")\n",
    "        \n",
    "        failed_evaluations = 0\n",
    "        \n",
    "        # Progress bar with correct total and initial values\n",
    "        total_items = len(qa_items)\n",
    "        completed_items = len(results)\n",
    "        pbar = tqdm(\n",
    "            remaining_items, \n",
    "            desc=\"Evaluating QA pairs\",\n",
    "            total=total_items,\n",
    "            initial=completed_items,\n",
    "            unit=\"items\"\n",
    "        )\n",
    "        \n",
    "        for i, item in enumerate(remaining_items):\n",
    "            try:\n",
    "                # Extract data from item\n",
    "                qa_id = item.get('id', f'item_{qa_items.index(item)}')\n",
    "                question = item.get('question', '')\n",
    "                answer = item.get('answer', '')\n",
    "                question_type = item.get('question_type', 'unknown')\n",
    "                \n",
    "                # Extract source context and KG triples\n",
    "                source_context = self.get_source_context(item)\n",
    "                kg_triples = self.extract_triples_from_item(item)\n",
    "                \n",
    "                # Skip if essential data is missing\n",
    "                if not question or not answer:\n",
    "                    self.logger.warning(f\"Skipping item {qa_id}: missing question or answer\")\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                \n",
    "                # Call LLM judge\n",
    "                evaluation = self.call_llm_judge(question, answer, source_context, kg_triples)\n",
    "                \n",
    "                if evaluation:\n",
    "                    evaluation = self.validate_evaluation_scores(evaluation, qa_id)   \n",
    "\n",
    "                    # Store results\n",
    "                    result = {\n",
    "                        'qa_id': qa_id,\n",
    "                        'question_type': question_type,\n",
    "                        'question': question,\n",
    "                        'answer': answer,\n",
    "                        'Relevance': evaluation['Relevance'],\n",
    "                        'Accuracy': evaluation['Accuracy'],\n",
    "                        'Completeness': evaluation['Completeness'],\n",
    "                        'Fluency': evaluation['Fluency'],\n",
    "                        'KG_Alignment': evaluation['KG_Alignment'],\n",
    "                        'Overall_Score': sum(evaluation.values()) / len(evaluation)\n",
    "                    }\n",
    "                    results.append(result)\n",
    "                    \n",
    "                    # Update progress bar with detailed status\n",
    "                    pbar.set_postfix({\n",
    "                        'Completed': len(results),\n",
    "                        'Failed': failed_evaluations,\n",
    "                        'Success_Rate': f\"{len(results)/(len(results)+failed_evaluations)*100:.1f}%\",\n",
    "                        'Last_Score': f\"{result['Overall_Score']:.1f}\"\n",
    "                    })\n",
    "                    \n",
    "                    # Save checkpoint\n",
    "                    if len(results) % checkpoint_interval == 0:\n",
    "                        self.save_checkpoint(results, checkpoint_path)\n",
    "                    \n",
    "                else:\n",
    "                    failed_evaluations += 1\n",
    "                    self.logger.warning(f\"Failed to evaluate item {qa_id}\")\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Rate limiting\n",
    "                if delay_seconds > 0:\n",
    "                    time.sleep(delay_seconds)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                failed_evaluations += 1\n",
    "                self.logger.error(f\"Error processing item {qa_id}: {str(e)}\")\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        # Final save\n",
    "        if results:\n",
    "            df = pd.DataFrame(results)\n",
    "            df.to_csv(output_path, index=False)\n",
    "            \n",
    "            # Clean up checkpoint\n",
    "            if os.path.exists(checkpoint_path):\n",
    "                os.remove(checkpoint_path)\n",
    "            \n",
    "            # Print summary statistics\n",
    "            self.print_evaluation_summary(df, failed_evaluations)\n",
    "            \n",
    "            return df\n",
    "        else:\n",
    "            self.logger.error(\"No successful evaluations completed\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def print_evaluation_summary(self, df: pd.DataFrame, failed_count: int):\n",
    "        \"\"\"Print comprehensive summary statistics of the evaluation.\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\" LLM JUDGE EVALUATION SUMMARY\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        print(f\" Evaluation Statistics:\")\n",
    "        print(f\"   Total Evaluated: {len(df)}\")\n",
    "        print(f\"   Failed Evaluations: {failed_count}\")\n",
    "        print(f\"   Success Rate: {len(df)/(len(df)+failed_count)*100:.1f}%\")\n",
    "        print(f\"   Evaluation Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        print(f\"\\n Average Scores by Metric:\")\n",
    "        metrics = ['Relevance', 'Accuracy', 'Completeness', 'Fluency', 'KG_Alignment']\n",
    "        for metric in metrics:\n",
    "            mean_score = df[metric].mean()\n",
    "            std_score = df[metric].std()\n",
    "            min_score = df[metric].min()\n",
    "            max_score = df[metric].max()\n",
    "            print(f\"   {metric}: {mean_score:.2f} Â± {std_score:.2f} (range: {min_score}-{max_score})\")\n",
    "        \n",
    "        print(f\"\\n Overall Performance:\")\n",
    "        print(f\"   Mean Overall Score: {df['Overall_Score'].mean():.2f}\")\n",
    "        print(f\"   Median Overall Score: {df['Overall_Score'].median():.2f}\")\n",
    "        print(f\"   Best Score: {df['Overall_Score'].max():.2f}\")\n",
    "        print(f\"   Worst Score: {df['Overall_Score'].min():.2f}\")\n",
    "        \n",
    "        # Score distribution\n",
    "        print(f\"\\n Score Distribution:\")\n",
    "        score_ranges = [(1.0, 2.0), (2.0, 3.0), (3.0, 4.0), (4.0, 5.0)]\n",
    "        for min_score, max_score in score_ranges:\n",
    "            count = len(df[(df['Overall_Score'] >= min_score) & (df['Overall_Score'] < max_score)])\n",
    "            percentage = (count / len(df)) * 100\n",
    "            print(f\"   {min_score:.1f}-{max_score:.1f}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Perfect scores\n",
    "        perfect_scores = len(df[df['Overall_Score'] == 5.0])\n",
    "        print(f\"   Perfect (5.0): {perfect_scores} ({(perfect_scores/len(df))*100:.1f}%)\")\n",
    "        \n",
    "        if 'question_type' in df.columns:\n",
    "            print(f\"\\n Performance by Question Type:\")\n",
    "            type_summary = df.groupby('question_type').agg({\n",
    "                'Overall_Score': ['mean', 'std', 'count'],\n",
    "                'Relevance': 'mean',\n",
    "                'Accuracy': 'mean',\n",
    "                'Completeness': 'mean',\n",
    "                'Fluency': 'mean',\n",
    "                'KG_Alignment': 'mean'\n",
    "            }).round(3)\n",
    "            \n",
    "            for qtype in type_summary.index:\n",
    "                stats = type_summary.loc[qtype]\n",
    "                mean_score = stats[('Overall_Score', 'mean')]\n",
    "                std_score = stats[('Overall_Score', 'std')]\n",
    "                count = stats[('Overall_Score', 'count')]\n",
    "                print(f\"   {qtype}: {mean_score:.2f} Â± {std_score:.2f} (n={count})\")\n",
    "\n",
    "def run_multi_dataset_evaluation():\n",
    "    \"\"\"Run evaluation on multiple datasets sequentially.\"\"\"\n",
    "    \n",
    "    # Configuration https://console.nscale.com/\n",
    "    \n",
    "    API_KEY = \"\"  # your NScale api key\n",
    "    \n",
    "    # Dataset configurations\n",
    "    datasets = [\n",
    "        {\n",
    "            'name': 'Zero-Shot',\n",
    "            'path': 'Zero-Shot_qa_dataset.json',\n",
    "            'output': 'mixtral_zeroshot_evaluation_results.csv'\n",
    "        },\n",
    "        {\n",
    "            'name': 'One-Shot',\n",
    "            'path': 'One-Shot_qa_dataset.json',\n",
    "            'output': 'mixtral_oneshot_evaluation_results.csv'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Few-Shot',\n",
    "            'path': 'Few-Shot_qa_dataset.json',\n",
    "            'output': 'mixtral_fewshot_evaluation_results.csv'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Initialize evaluator with DeepSeek model\n",
    "    evaluator = ImprovedLLMJudgeEvaluator(\n",
    "        api_key=API_KEY,\n",
    "        model_name=\"mistralai/mixtral-8x22b-instruct-v0.1\"\n",
    "    )\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"MULTI-DATASET EVALUATION WITH Mixtral-8x22b-instruct-v0.1\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Model: {evaluator.model_name}\")\n",
    "    print(f\"Datasets to evaluate: {len(datasets)}\")\n",
    "    for i, dataset in enumerate(datasets, 1):\n",
    "        print(f\"  {i}. {dataset['name']}: {dataset['path']}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Confirm before starting\n",
    "    response = input(\"\\nProceed with multi-dataset evaluation? (y/n): \").strip().lower()\n",
    "    if response != 'y':\n",
    "        print(\"Evaluation cancelled.\")\n",
    "        return\n",
    "    \n",
    "    # Track overall statistics\n",
    "    overall_start_time = time.time()\n",
    "    all_results = []\n",
    "    \n",
    "    # Process each dataset\n",
    "    for i, dataset in enumerate(datasets, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"PROCESSING DATASET {i}/{len(datasets)}: {dataset['name']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Input: {dataset['path']}\")\n",
    "        print(f\"Output: {dataset['output']}\")\n",
    "        print(f\"Model: {evaluator.model_name}\")\n",
    "        \n",
    "        # Check if dataset file exists\n",
    "        if not os.path.exists(dataset['path']):\n",
    "            print(f\" ERROR: Dataset file not found: {dataset['path']}\")\n",
    "            print(f\"Skipping {dataset['name']} dataset...\")\n",
    "            continue\n",
    "        \n",
    "        dataset_start_time = time.time()\n",
    "        \n",
    "        # Run evaluation\n",
    "        try:\n",
    "            results_df = evaluator.evaluate_dataset(\n",
    "                dataset_path=dataset['path'],\n",
    "                output_path=dataset['output'],\n",
    "                sample_size=None,  # Process all items\n",
    "                delay_seconds=1.0,  # 0.5 second delay between requests\n",
    "                checkpoint_interval=50  # Save every 50 evaluations\n",
    "            )\n",
    "            \n",
    "            dataset_end_time = time.time()\n",
    "            dataset_duration = dataset_end_time - dataset_start_time\n",
    "            \n",
    "            if not results_df.empty:\n",
    "                # Add dataset type to results\n",
    "                results_df['dataset_type'] = dataset['name']\n",
    "                all_results.append(results_df)\n",
    "                \n",
    "                print(f\"\\n {dataset['name']} EVALUATION COMPLETED!\")\n",
    "                print(f\"   Duration: {dataset_duration/60:.1f} minutes\")\n",
    "                print(f\"   Evaluated: {len(results_df)} QA pairs\")\n",
    "                print(f\"   Results saved to: {dataset['output']}\")\n",
    "                print(f\"   Average Score: {results_df['Overall_Score'].mean():.2f}\")\n",
    "                \n",
    "            else:\n",
    "                print(f\" {dataset['name']} evaluation failed. Check logs for details.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\" Error processing {dataset['name']}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Overall summary\n",
    "    overall_end_time = time.time()\n",
    "    overall_duration = overall_end_time - overall_start_time\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"MULTI-DATASET EVALUATION SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Duration: {overall_duration/60:.1f} minutes\")\n",
    "    print(f\"Model Used: {evaluator.model_name}\")\n",
    "    print(f\"Datasets Processed: {len(all_results)}/{len(datasets)}\")\n",
    "    \n",
    "    if all_results:\n",
    "        # Combine all results for comparative analysis\n",
    "        combined_df = pd.concat(all_results, ignore_index=True)\n",
    "        combined_output = \"Mixtral_combined_evaluation_results.csv\"\n",
    "        combined_df.to_csv(combined_output, index=False)\n",
    "        \n",
    "        print(f\"\\n COMPARATIVE ANALYSIS:\")\n",
    "        print(f\"Combined results saved to: {combined_output}\")\n",
    "        print(f\"Total QA pairs evaluated: {len(combined_df)}\")\n",
    "        \n",
    "        # Performance by dataset\n",
    "        print(f\"\\n PERFORMANCE BY DATASET:\")\n",
    "        dataset_summary = combined_df.groupby('dataset_type').agg({\n",
    "            'Overall_Score': ['mean', 'std', 'count'],\n",
    "            'Relevance': 'mean',\n",
    "            'Accuracy': 'mean',\n",
    "            'Completeness': 'mean',\n",
    "            'Fluency': 'mean',\n",
    "            'KG_Alignment': 'mean'\n",
    "        }).round(3)\n",
    "        \n",
    "        for dataset_type in dataset_summary.index:\n",
    "            stats = dataset_summary.loc[dataset_type]\n",
    "            mean_score = stats[('Overall_Score', 'mean')]\n",
    "            std_score = stats[('Overall_Score', 'std')]\n",
    "            count = stats[('Overall_Score', 'count')]\n",
    "            print(f\"   {dataset_type}: {mean_score:.2f} Â± {std_score:.2f} (n={count})\")\n",
    "        \n",
    "        # Overall statistics\n",
    "        print(f\"\\n OVERALL PERFORMANCE:\")\n",
    "        print(f\"   Mean Overall Score: {combined_df['Overall_Score'].mean():.2f}\")\n",
    "        print(f\"   Best Dataset: {dataset_summary[('Overall_Score', 'mean')].idxmax()}\")\n",
    "        print(f\"   Score Range: {combined_df['Overall_Score'].min():.2f} - {combined_df['Overall_Score'].max():.2f}\")\n",
    "        \n",
    "        # Processing efficiency\n",
    "        total_items = len(combined_df)\n",
    "        items_per_minute = total_items / (overall_duration / 60)\n",
    "        print(f\"\\nâš¡ PROCESSING EFFICIENCY:\")\n",
    "        print(f\"   Items per minute: {items_per_minute:.1f}\")\n",
    "        print(f\"   Average time per item: {overall_duration/total_items:.2f} seconds\")\n",
    "        \n",
    "    else:\n",
    "        print(\" No datasets were successfully processed.\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EVALUATION COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_multi_dataset_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fcc52f-8e92-4a70-87c0-223bc0497301",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
