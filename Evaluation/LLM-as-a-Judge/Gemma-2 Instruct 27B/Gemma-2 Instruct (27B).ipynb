{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b02e40-6eef-4e57-bb0f-57a4ac0278cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from typing import Dict, List, Optional\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "from together import Together\n",
    "import random\n",
    "\n",
    "class ImprovedLLMJudgeEvaluator:\n",
    "    \"\"\"\n",
    "    LLM Judge Evaluation System with detailed scoring criteria \n",
    "    aligned with human evaluation guidelines - Together AI Version.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, model_name: str = \"google/gemma-2-27b-it\"):\n",
    "        \"\"\"\n",
    "        Initialize the LLM Judge Evaluator.\n",
    "        \n",
    "        Args:\n",
    "            api_key: Together AI API key\n",
    "            model_name: Model identifier for the LLM judge\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # Initialize Together AI client\n",
    "        self.client = Together(api_key=api_key)\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # evaluation prompt with detailed criteria\n",
    "        self.evaluation_prompt = \"\"\"You are a STRICT QA evaluator following precise scoring guidelines.\n",
    "\n",
    "You will evaluate a model-generated question-answer pair on FIVE metrics using a 1-5 scale where:\n",
    "5 = Excellent  \n",
    "4 = Good  \n",
    "3 = Fair  \n",
    "2 = Poor  \n",
    "1 = Very Poor\n",
    "\n",
    "==============================\n",
    "DETAILED SCORING CRITERIA\n",
    "==============================\n",
    "\n",
    "1. **RELEVANCE (1–5):** Does the question appropriately relate to the source text?  \n",
    "   5: Perfectly relevant to the source, clearly grounded in the text  \n",
    "   4: Mostly relevant, with minor off-topic elements  \n",
    "   3: Addresses the main question but misses some important points  \n",
    "   2: Loosely related, with significant tangents or irrelevance  \n",
    "   1: Entirely irrelevant or unrelated to the source content\n",
    "\n",
    "2. **ACCURACY (1–5):** Is the answer factually correct based on the source text?  \n",
    "   5: All facts are accurate and fully verifiable in the context  \n",
    "   4: Mostly accurate; contains only minor factual issues  \n",
    "   3: Some factual inconsistencies or assumptions  \n",
    "   2: Several factual errors that affect reliability  \n",
    "   1: Mostly inaccurate or misleading information\n",
    "\n",
    "3. **COMPLETENESS (1–5):** Does the answer fully address the question?  \n",
    "   5: Thorough and complete response  \n",
    "   4: Covers most parts but misses minor aspects  \n",
    "   3: Addresses main part, omits some key details  \n",
    "   2: Partial answer with significant gaps  \n",
    "   1: Severely incomplete or off-topic\n",
    "\n",
    "4. **FLUENCY (1–5):** Is the answer well-written and grammatically correct?  \n",
    "   5: Excellent grammar and clarity; highly readable  \n",
    "   4: Minor grammatical or structural issues  \n",
    "   3: Understandable, but contains noticeable language errors  \n",
    "   2: Somewhat unclear due to poor grammar or phrasing  \n",
    "   1: Difficult to read or understand\n",
    "\n",
    "5. **KG ALIGNMENT (1–5):** How well does the answer reflect the KG triples?  \n",
    "   5: Effectively uses KG relationships; no contradictions; may include additional relevant info  \n",
    "   4: Uses most relevant KG information correctly; may omit minor details, no contradictions  \n",
    "   3: Uses some KG information; may miss important relationships but generally consistent  \n",
    "   2: Limited use of KG information; may contain minor contradictions or misinterpretations  \n",
    "   1: Ignores KG information entirely or includes clear contradictions\n",
    "\n",
    "\n",
    "==============================\n",
    "INPUT\n",
    "==============================\n",
    "**Question:** {question}  \n",
    "**Answer:** {answer}  \n",
    "**Source Context:** {source_context}  \n",
    "**Knowledge Graph Triples:** {kg_triples}\n",
    "\n",
    "==============================\n",
    "RESPONSE FORMAT\n",
    "==============================\n",
    "Provide ONLY the numerical scores in this exact format (no explanation):\n",
    "\n",
    "Relevance: X  \n",
    "Accuracy: X  \n",
    "Completeness: X  \n",
    "Fluency: X  \n",
    "KG_Alignment: X\n",
    "\n",
    "Where X is a number from 1 to 5 \"\"\"\n",
    "\n",
    "    def extract_triples_from_item(self, item: Dict) -> str:\n",
    "        \"\"\"Extract and format KG triples from a QA item.\"\"\"\n",
    "        def extract_all_triples(obj):\n",
    "            triples = []\n",
    "            if isinstance(obj, dict):\n",
    "                if set(obj.keys()) >= {\"subject\", \"predicate\", \"object\"}:\n",
    "                    triple = f\"{obj['subject']} → {obj['predicate']} → {obj['object']}\"\n",
    "                    triples.append(triple)\n",
    "                for value in obj.values():\n",
    "                    triples.extend(extract_all_triples(value))\n",
    "            elif isinstance(obj, list):\n",
    "                for item in obj:\n",
    "                    triples.extend(extract_all_triples(item))\n",
    "            return triples\n",
    "        \n",
    "        triples = extract_all_triples(item)\n",
    "        if triples:\n",
    "            formatted_triples = []\n",
    "            for i, triple in enumerate(triples, 1):\n",
    "                formatted_triples.append(f\"{i}. {triple}\")\n",
    "            return \"\\n\".join(formatted_triples)\n",
    "        return \"No KG triples available\"\n",
    "\n",
    "    def get_source_context(self, item: Dict) -> str:\n",
    "        \"\"\"Extract source context from QA item.\"\"\"\n",
    "        # Check if source_context is nested under ground_truth\n",
    "        if \"ground_truth\" in item and isinstance(item[\"ground_truth\"], dict):\n",
    "            return item[\"ground_truth\"].get(\"source_context\", \"\")\n",
    "        \n",
    "        # Fallback to root level\n",
    "        return item.get(\"source_context\", \"No source context available\")\n",
    "\n",
    "    def call_llm_judge(self, question: str, answer: str, source_context: str, kg_triples: str, \n",
    "                      max_retries: int = 3) -> Optional[Dict[str, int]]:\n",
    "        \"\"\"\n",
    "        Call the LLM judge to evaluate a QA pair with retry logic using Together AI.\n",
    "        \"\"\"\n",
    "        # Format the prompt\n",
    "        prompt = self.evaluation_prompt.format(\n",
    "            question=question,\n",
    "            answer=answer,\n",
    "            source_context=source_context,\n",
    "            kg_triples=kg_triples\n",
    "        )\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Together AI API call\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model_name,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=0.0,  # This should fix most score inflation\n",
    "                    max_tokens=800,\n",
    "                    top_p=0.8\n",
    "                )\n",
    "                \n",
    "                if response and response.choices:\n",
    "                    content = response.choices[0].message.content\n",
    "                    \n",
    "                    if content:\n",
    "                        parsed_result = self.parse_evaluation_response(content)\n",
    "                        if parsed_result:\n",
    "                            return parsed_result\n",
    "                    else:\n",
    "                        self.logger.warning(f\"Empty response content from {self.model_name}\")\n",
    "                else:\n",
    "                    self.logger.warning(f\"No response or choices from {self.model_name}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                # Handle rate limiting and other errors\n",
    "                if \"rate limit\" in str(e).lower() or \"429\" in str(e):\n",
    "                    wait_time = 2 ** attempt\n",
    "                    self.logger.warning(f\"Rate limit hit, waiting {wait_time}s before retry {attempt + 1}/{max_retries}\")\n",
    "                    time.sleep(wait_time)\n",
    "                elif \"timeout\" in str(e).lower():\n",
    "                    self.logger.warning(f\"Timeout on attempt {attempt + 1}/{max_retries}\")\n",
    "                    if attempt < max_retries - 1:\n",
    "                        time.sleep(2 ** attempt)\n",
    "                else:\n",
    "                    self.logger.error(f\"API Error on attempt {attempt + 1}: {str(e)}\")\n",
    "                    if attempt < max_retries - 1:\n",
    "                        time.sleep(2 ** attempt)\n",
    "    \n",
    "        return None\n",
    "\n",
    "    def parse_evaluation_response(self, response: str) -> Optional[Dict[str, int]]:\n",
    "        \"\"\"\n",
    "        parser that can handle reasoning + extract scores\n",
    "        \"\"\"\n",
    "        try:\n",
    "            scores = {}\n",
    "            \n",
    "            # Method 1: Standard format parsing (existing logic)\n",
    "            lines = response.strip().split('\\n')\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if ':' in line:\n",
    "                    parts = line.split(':', 1)\n",
    "                    if len(parts) == 2:\n",
    "                        metric = parts[0].strip()\n",
    "                        score_text = parts[1].strip()\n",
    "                        \n",
    "                        score_match = re.search(r':\\s*([1-5])(?:/5)?', line)\n",
    "                        if score_match:\n",
    "                            score = int(score_match.group(1))\n",
    "                            \n",
    "                            metric_lower = metric.lower()\n",
    "                            if 'relevance' in metric_lower:\n",
    "                                scores['Relevance'] = score\n",
    "                            elif 'accuracy' in metric_lower:\n",
    "                                scores['Accuracy'] = score\n",
    "                            elif 'completeness' in metric_lower:\n",
    "                                scores['Completeness'] = score\n",
    "                            elif 'fluency' in metric_lower:\n",
    "                                scores['Fluency'] = score\n",
    "                            elif 'kg' in metric_lower or 'alignment' in metric_lower:\n",
    "                                scores['KG_Alignment'] = score\n",
    "            \n",
    "            #  If standard parsing fails, extract from reasoning text\n",
    "            expected_metrics = ['Relevance', 'Accuracy', 'Completeness', 'Fluency', 'KG_Alignment']\n",
    "            if len(scores) < 5:\n",
    "                # Extract from reasoning patterns like \"Relevance should be a 5\"\n",
    "                text_lower = response.lower()\n",
    "                \n",
    "                if 'Relevance' not in scores:\n",
    "                    match = re.search(r'relevance.*?(?:should be|is).*?([1-5])', text_lower)\n",
    "                    if match:\n",
    "                        scores['Relevance'] = int(match.group(1))\n",
    "                \n",
    "                if 'Accuracy' not in scores:\n",
    "                    match = re.search(r'accuracy.*?(?:should be|is).*?([1-5])', text_lower)\n",
    "                    if match:\n",
    "                        scores['Accuracy'] = int(match.group(1))\n",
    "                \n",
    "                if 'Completeness' not in scores:\n",
    "                    match = re.search(r'completeness.*?(?:should be|is).*?([1-5])', text_lower)\n",
    "                    if match:\n",
    "                        scores['Completeness'] = int(match.group(1))\n",
    "                \n",
    "                if 'Fluency' not in scores:\n",
    "                    match = re.search(r'fluency.*?(?:should be|is).*?([1-5])', text_lower)\n",
    "                    if match:\n",
    "                        scores['Fluency'] = int(match.group(1))\n",
    "                \n",
    "                if 'KG_Alignment' not in scores:\n",
    "                    match = re.search(r'(?:kg|alignment).*?(?:should be|is).*?([1-5])', text_lower)\n",
    "                    if match:\n",
    "                        scores['KG_Alignment'] = int(match.group(1))\n",
    "            \n",
    "            # If we have partial scores, try to get the rest\n",
    "            if len(scores) > 0 and len(scores) < 5:\n",
    "                self.logger.warning(f\"Got partial response with {len(scores)} scores: {scores}\")\n",
    "                # Fill missing scores with average of existing ones\n",
    "                if len(scores) >= 2:\n",
    "                    avg_score = round(sum(scores.values()) / len(scores))\n",
    "                    expected_metrics = ['Relevance', 'Accuracy', 'Completeness', 'Fluency', 'KG_Alignment']\n",
    "                    for metric in expected_metrics:\n",
    "                        if metric not in scores:\n",
    "                            scores[metric] = avg_score\n",
    "                            self.logger.info(f\"Filled missing {metric} with average {avg_score}\")\n",
    "                    return scores\n",
    "            \n",
    "            # Final validation\n",
    "            if all(metric in scores for metric in expected_metrics):\n",
    "                return scores\n",
    "            else:\n",
    "                self.logger.warning(f\"Missing metrics in response: {response}\")\n",
    "                return self._parse_with_fallback(response)\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to parse response: {response}. Error: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _parse_with_fallback(self, response: str) -> Optional[Dict[str, int]]:\n",
    "        \"\"\"Fallback parsing with multiple regex patterns.\"\"\"\n",
    "        patterns = [\n",
    "            r':\\s*([1-5])(?:/5)?',          #  pattern\n",
    "            r'\\b([1-5])\\b',                 \n",
    "            r'([1-5])\\s*(?:out of 5|/5)?'   \n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            try:\n",
    "                scores = {}\n",
    "                lines = response.strip().split('\\n')\n",
    "                \n",
    "                for line in lines:\n",
    "                    if ':' in line:\n",
    "                        parts = line.split(':', 1)\n",
    "                        if len(parts) == 2:\n",
    "                            metric = parts[0].strip()\n",
    "                            score_text = parts[1].strip()\n",
    "                            \n",
    "                            score_match = re.search(pattern, score_text)\n",
    "                            if score_match:\n",
    "                                score = int(score_match.group(1))\n",
    "                                \n",
    "                                # Normalize metric names\n",
    "                                metric_lower = metric.lower()\n",
    "                                if 'relevance' in metric_lower:\n",
    "                                    scores['Relevance'] = score\n",
    "                                elif 'accuracy' in metric_lower:\n",
    "                                    scores['Accuracy'] = score\n",
    "                                elif 'completeness' in metric_lower:\n",
    "                                    scores['Completeness'] = score\n",
    "                                elif 'fluency' in metric_lower:\n",
    "                                    scores['Fluency'] = score\n",
    "                                elif 'kg' in metric_lower or 'alignment' in metric_lower:\n",
    "                                    scores['KG_Alignment'] = score\n",
    "                \n",
    "                # Check if this pattern worked\n",
    "                expected_metrics = ['Relevance', 'Accuracy', 'Completeness', 'Fluency', 'KG_Alignment']\n",
    "                if all(metric in scores for metric in expected_metrics):\n",
    "                    return scores\n",
    "                    \n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def validate_evaluation_scores(self, evaluation: Dict[str, int], qa_id: str) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Validate and clamp evaluation scores to valid range [1, 5].\n",
    "        \"\"\"\n",
    "        validated = {}\n",
    "        issues = []\n",
    "        \n",
    "        for metric, score in evaluation.items():\n",
    "            original_score = score\n",
    "            \n",
    "            # Handle non-numeric values\n",
    "            if not isinstance(score, (int, float)):\n",
    "                try:\n",
    "                    score = int(score)\n",
    "                except (ValueError, TypeError):\n",
    "                    score = 3  # Default to middle score\n",
    "                    issues.append(f\"{metric}: non-numeric '{original_score}' → {score}\")\n",
    "            \n",
    "            # Clamp to valid range\n",
    "            clamped_score = min(max(int(score), 1), 5)\n",
    "            validated[metric] = clamped_score\n",
    "            \n",
    "            # Log if clamped\n",
    "            if clamped_score != original_score:\n",
    "                issues.append(f\"{metric}: {original_score} → {clamped_score}\")\n",
    "        \n",
    "        # Log any issues\n",
    "        if issues:\n",
    "            self.logger.warning(f\"Score validation issues for {qa_id}: {'; '.join(issues)}\")\n",
    "        \n",
    "        return validated\n",
    "\n",
    "    def load_qa_dataset(self, file_path: str) -> List[Dict]:\n",
    "        \"\"\"Load QA dataset from JSON file.\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            queries = data.get('queries', [])\n",
    "            self.logger.info(f\"Loaded {len(queries)} QA pairs from {file_path}\")\n",
    "            return queries\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load dataset: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def save_checkpoint(self, results: List[Dict], checkpoint_path: str):\n",
    "        \"\"\"Save evaluation results as checkpoint.\"\"\"\n",
    "        try:\n",
    "            df = pd.DataFrame(results)\n",
    "            df.to_csv(checkpoint_path, index=False)\n",
    "            self.logger.info(f\"Checkpoint saved: {len(results)} results to {checkpoint_path}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to save checkpoint: {str(e)}\")\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_path: str) -> List[Dict]:\n",
    "        \"\"\"Load evaluation results from checkpoint.\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(checkpoint_path):\n",
    "                df = pd.read_csv(checkpoint_path)\n",
    "                results = df.to_dict('records')\n",
    "                self.logger.info(f\"Loaded checkpoint: {len(results)} results from {checkpoint_path}\")\n",
    "                return results\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load checkpoint: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def evaluate_dataset(self, dataset_path: str, output_path: str, \n",
    "                        sample_size: Optional[int] = None, \n",
    "                        delay_seconds: float = 1.0,\n",
    "                        checkpoint_interval: int = 50) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Evaluate QA dataset using LLM judge with checkpointing.\n",
    "        \n",
    "        Args:\n",
    "            dataset_path: Path to the QA dataset JSON file\n",
    "            output_path: Path to save evaluation results\n",
    "            sample_size: Number of samples to evaluate (None for all)\n",
    "            delay_seconds: Delay between API calls to avoid rate limits\n",
    "            checkpoint_interval: Save checkpoint every N evaluations\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with evaluation results\n",
    "        \"\"\"\n",
    "        # Load dataset\n",
    "        qa_items = self.load_qa_dataset(dataset_path)\n",
    "        if not qa_items:\n",
    "            self.logger.error(\"No QA items loaded. Exiting.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Sample subset if requested\n",
    "        if sample_size and sample_size < len(qa_items):\n",
    "            random.seed(42)  # For reproducibility\n",
    "            qa_items = random.sample(qa_items, sample_size)\n",
    "            self.logger.info(f\"Sampling {sample_size} items for evaluation\")\n",
    "        \n",
    "        # Setup checkpoint\n",
    "        checkpoint_path = f\"{output_path}.checkpoint\"\n",
    "        results = self.load_checkpoint(checkpoint_path)\n",
    "        \n",
    "        # Track processed items\n",
    "        processed_ids = {result['qa_id'] for result in results} if results else set()\n",
    "        \n",
    "        # Filter unprocessed items \n",
    "        remaining_items = []\n",
    "        for i, item in enumerate(qa_items):\n",
    "            item_id = item.get('id', f'item_{i}')\n",
    "            if item_id not in processed_ids:\n",
    "                remaining_items.append(item)\n",
    "        \n",
    "        if processed_ids:\n",
    "            self.logger.info(f\"Resuming from checkpoint: {len(results)} completed, {len(remaining_items)} remaining\")\n",
    "        \n",
    "        failed_evaluations = 0\n",
    "        \n",
    "        # Progress bar with correct total and initial values\n",
    "        total_items = len(qa_items)\n",
    "        completed_items = len(results)\n",
    "        pbar = tqdm(\n",
    "            remaining_items, \n",
    "            desc=\"Evaluating QA pairs\",\n",
    "            total=total_items,\n",
    "            initial=completed_items,\n",
    "            unit=\"items\"\n",
    "        )\n",
    "        \n",
    "        for i, item in enumerate(remaining_items):\n",
    "            try:\n",
    "                # Extract data from item -  ID generation\n",
    "                original_index = next((i for i, orig_item in enumerate(qa_items) if orig_item is item), len(qa_items))\n",
    "                qa_id = item.get('id', f'item_{original_index}')\n",
    "                question = item.get('question', '')\n",
    "                answer = item.get('answer', '')\n",
    "                question_type = item.get('question_type', 'unknown')\n",
    "                \n",
    "                # Extract source context and KG triples\n",
    "                source_context = self.get_source_context(item)\n",
    "                kg_triples = self.extract_triples_from_item(item)\n",
    "                \n",
    "                # Skip if essential data is missing\n",
    "                if not question or not answer:\n",
    "                    self.logger.warning(f\"Skipping item {qa_id}: missing question or answer\")\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                \n",
    "                # Call LLM judge\n",
    "                evaluation = self.call_llm_judge(question, answer, source_context, kg_triples)\n",
    "                \n",
    "                if evaluation:\n",
    "                    evaluation = self.validate_evaluation_scores(evaluation, qa_id)   \n",
    "\n",
    "                    # Store results\n",
    "                    result = {\n",
    "                        'qa_id': qa_id,\n",
    "                        'question_type': question_type,\n",
    "                        'question': question,\n",
    "                        'answer': answer,\n",
    "                        'Relevance': evaluation['Relevance'],\n",
    "                        'Accuracy': evaluation['Accuracy'],\n",
    "                        'Completeness': evaluation['Completeness'],\n",
    "                        'Fluency': evaluation['Fluency'],\n",
    "                        'KG_Alignment': evaluation['KG_Alignment'],\n",
    "                        'Overall_Score': sum(evaluation.values()) / len(evaluation)\n",
    "                    }\n",
    "                    results.append(result)\n",
    "                    \n",
    "                    # Update progress bar with detailed status\n",
    "                    pbar.set_postfix({\n",
    "                        'Completed': len(results),\n",
    "                        'Failed': failed_evaluations,\n",
    "                        'Success_Rate': f\"{len(results)/(len(results)+failed_evaluations)*100:.1f}%\",\n",
    "                        'Last_Score': f\"{result['Overall_Score']:.1f}\"\n",
    "                    })\n",
    "                    \n",
    "                    # Save checkpoint\n",
    "                    if len(results) % checkpoint_interval == 0:\n",
    "                        self.save_checkpoint(results, checkpoint_path)\n",
    "                    \n",
    "                else:\n",
    "                    failed_evaluations += 1\n",
    "                    self.logger.warning(f\"Failed to evaluate item {qa_id}\")\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Rate limiting\n",
    "                if delay_seconds > 0:\n",
    "                    time.sleep(delay_seconds)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                failed_evaluations += 1\n",
    "                self.logger.error(f\"Error processing item {qa_id}: {str(e)}\")\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        # Final save\n",
    "        if results:\n",
    "            df = pd.DataFrame(results)\n",
    "            df.to_csv(output_path, index=False)\n",
    "            \n",
    "            # Clean up checkpoint\n",
    "            if os.path.exists(checkpoint_path):\n",
    "                os.remove(checkpoint_path)\n",
    "            \n",
    "            # Print summary statistics\n",
    "            self.print_evaluation_summary(df, failed_evaluations)\n",
    "            \n",
    "            return df\n",
    "        else:\n",
    "            self.logger.error(\"No successful evaluations completed\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def print_evaluation_summary(self, df: pd.DataFrame, failed_count: int):\n",
    "        \"\"\"Print comprehensive summary statistics of the evaluation.\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\" LLM JUDGE EVALUATION SUMMARY\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        print(f\" Evaluation Statistics:\")\n",
    "        print(f\"   Total Evaluated: {len(df)}\")\n",
    "        print(f\"   Failed Evaluations: {failed_count}\")\n",
    "        print(f\"   Success Rate: {len(df)/(len(df)+failed_count)*100:.1f}%\")\n",
    "        print(f\"   Evaluation Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        print(f\"\\n Average Scores by Metric:\")\n",
    "        metrics = ['Relevance', 'Accuracy', 'Completeness', 'Fluency', 'KG_Alignment']\n",
    "        for metric in metrics:\n",
    "            mean_score = df[metric].mean()\n",
    "            std_score = df[metric].std()\n",
    "            min_score = df[metric].min()\n",
    "            max_score = df[metric].max()\n",
    "            print(f\"   {metric}: {mean_score:.2f} ± {std_score:.2f} (range: {min_score}-{max_score})\")\n",
    "        \n",
    "        print(f\"\\n Overall Performance:\")\n",
    "        print(f\"   Mean Overall Score: {df['Overall_Score'].mean():.2f}\")\n",
    "        print(f\"   Median Overall Score: {df['Overall_Score'].median():.2f}\")\n",
    "        print(f\"   Best Score: {df['Overall_Score'].max():.2f}\")\n",
    "        print(f\"   Worst Score: {df['Overall_Score'].min():.2f}\")\n",
    "        \n",
    "        # Score distribution\n",
    "        print(f\"\\n Score Distribution:\")\n",
    "        score_ranges = [(1.0, 2.0), (2.0, 3.0), (3.0, 4.0), (4.0, 5.0)]\n",
    "        for min_score, max_score in score_ranges:\n",
    "            count = len(df[(df['Overall_Score'] >= min_score) & (df['Overall_Score'] < max_score)])\n",
    "            percentage = (count / len(df)) * 100\n",
    "            print(f\"   {min_score:.1f}-{max_score:.1f}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Perfect scores\n",
    "        perfect_scores = len(df[df['Overall_Score'] == 5.0])\n",
    "        print(f\"   Perfect (5.0): {perfect_scores} ({(perfect_scores/len(df))*100:.1f}%)\")\n",
    "        \n",
    "        if 'question_type' in df.columns:\n",
    "            print(f\"\\n Performance by Question Type:\")\n",
    "            type_summary = df.groupby('question_type').agg({\n",
    "                'Overall_Score': ['mean', 'std', 'count'],\n",
    "                'Relevance': 'mean',\n",
    "                'Accuracy': 'mean',\n",
    "                'Completeness': 'mean',\n",
    "                'Fluency': 'mean',\n",
    "                'KG_Alignment': 'mean'\n",
    "            }).round(3)\n",
    "            \n",
    "            for qtype in type_summary.index:\n",
    "                stats = type_summary.loc[qtype]\n",
    "                mean_score = stats[('Overall_Score', 'mean')]\n",
    "                std_score = stats[('Overall_Score', 'std')]\n",
    "                count = stats[('Overall_Score', 'count')]\n",
    "                print(f\"   {qtype}: {mean_score:.2f} ± {std_score:.2f} (n={count})\")\n",
    "\n",
    "def run_multi_dataset_evaluation():\n",
    "    \"\"\"Run evaluation on multiple datasets sequentially.\"\"\"\n",
    "    \n",
    "    # Configuration - (https://api.together.ai/)    Together AI\n",
    "    API_KEY = \"\"  # Your Together AI API key\n",
    "    \n",
    "    # Dataset configurations\n",
    "    datasets = [\n",
    "        {\n",
    "            'name': 'Zero-Shot',\n",
    "            'path': 'Zero-Shot_qa_dataset.json',\n",
    "            'output': 'gemma_zeroshot_evaluation_results.csv'\n",
    "        },\n",
    "        {\n",
    "            'name': 'One-Shot',\n",
    "            'path': 'One-Shot_qa_dataset.json',\n",
    "            'output': 'gemma_oneshot_evaluation_results.csv'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Few-Shot',\n",
    "            'path': 'Few-Shot_qa_dataset.json',\n",
    "            'output': 'gemma_fewshot_evaluation_results.csv'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Initialize evaluator with google/gemma-2-27b-it model via Together AI\n",
    "    evaluator = ImprovedLLMJudgeEvaluator(\n",
    "        api_key=API_KEY,\n",
    "        model_name=\"google/gemma-2-27b-it\"\n",
    "    )\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"MULTI-DATASET EVALUATION WITH google/gemma-2-27b-it\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Model: {evaluator.model_name}\")\n",
    "    print(f\"API Provider: Together AI\")\n",
    "    print(f\"Datasets to evaluate: {len(datasets)}\")\n",
    "    for i, dataset in enumerate(datasets, 1):\n",
    "        print(f\"  {i}. {dataset['name']}: {dataset['path']}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Confirm before starting\n",
    "    response = input(\"\\nProceed with multi-dataset evaluation? (y/n): \").strip().lower()\n",
    "    if response != 'y':\n",
    "        print(\"Evaluation cancelled.\")\n",
    "        return\n",
    "    \n",
    "    # Track overall statistics\n",
    "    overall_start_time = time.time()\n",
    "    all_results = []\n",
    "    \n",
    "    # Process each dataset\n",
    "    for i, dataset in enumerate(datasets, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"PROCESSING DATASET {i}/{len(datasets)}: {dataset['name']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Input: {dataset['path']}\")\n",
    "        print(f\"Output: {dataset['output']}\")\n",
    "        print(f\"Model: {evaluator.model_name}\")\n",
    "        print(f\"API Provider: Together AI\")\n",
    "        \n",
    "        # Check if dataset file exists\n",
    "        if not os.path.exists(dataset['path']):\n",
    "            print(f\" ERROR: Dataset file not found: {dataset['path']}\")\n",
    "            print(f\"Skipping {dataset['name']} dataset...\")\n",
    "            continue\n",
    "        \n",
    "        dataset_start_time = time.time()\n",
    "        \n",
    "        # Run evaluation\n",
    "        try:\n",
    "            results_df = evaluator.evaluate_dataset(\n",
    "                dataset_path=dataset['path'],\n",
    "                output_path=dataset['output'],\n",
    "                sample_size=None,  # Process all items\n",
    "                delay_seconds=1.0,  # 1 second delay between requests\n",
    "                checkpoint_interval=50  # Save every 50 evaluations\n",
    "            )\n",
    "            \n",
    "            dataset_end_time = time.time()\n",
    "            dataset_duration = dataset_end_time - dataset_start_time\n",
    "            \n",
    "            if not results_df.empty:\n",
    "                # Add dataset type to results\n",
    "                results_df['dataset_type'] = dataset['name']\n",
    "                all_results.append(results_df)\n",
    "                \n",
    "                print(f\"\\n {dataset['name']} EVALUATION COMPLETED!\")\n",
    "                print(f\"   Duration: {dataset_duration/60:.1f} minutes\")\n",
    "                print(f\"   Evaluated: {len(results_df)} QA pairs\")\n",
    "                print(f\"   Results saved to: {dataset['output']}\")\n",
    "                print(f\"   Average Score: {results_df['Overall_Score'].mean():.2f}\")\n",
    "                \n",
    "            else:\n",
    "                print(f\" {dataset['name']} evaluation failed. Check logs for details.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\" Error processing {dataset['name']}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Overall summary\n",
    "    overall_end_time = time.time()\n",
    "    overall_duration = overall_end_time - overall_start_time\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"MULTI-DATASET EVALUATION SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Duration: {overall_duration/60:.1f} minutes\")\n",
    "    print(f\"Model Used: {evaluator.model_name}\")\n",
    "    print(f\"API Provider: Together AI\")\n",
    "    print(f\"Datasets Processed: {len(all_results)}/{len(datasets)}\")\n",
    "    \n",
    "    if all_results:\n",
    "        # Combine all results for comparative analysis\n",
    "        combined_df = pd.concat(all_results, ignore_index=True)\n",
    "        combined_output = \"gemma-2-27b-it_combined_evaluation_results.csv\"\n",
    "        combined_df.to_csv(combined_output, index=False)\n",
    "        \n",
    "        print(f\"\\n COMPARATIVE ANALYSIS:\")\n",
    "        print(f\"Combined results saved to: {combined_output}\")\n",
    "        print(f\"Total QA pairs evaluated: {len(combined_df)}\")\n",
    "        \n",
    "        # Performance by dataset\n",
    "        print(f\"\\n PERFORMANCE BY DATASET:\")\n",
    "        dataset_summary = combined_df.groupby('dataset_type').agg({\n",
    "            'Overall_Score': ['mean', 'std', 'count'],\n",
    "            'Relevance': 'mean',\n",
    "            'Accuracy': 'mean',\n",
    "            'Completeness': 'mean',\n",
    "            'Fluency': 'mean',\n",
    "            'KG_Alignment': 'mean'\n",
    "        }).round(3)\n",
    "        \n",
    "        for dataset_type in dataset_summary.index:\n",
    "            stats = dataset_summary.loc[dataset_type]\n",
    "            mean_score = stats[('Overall_Score', 'mean')]\n",
    "            std_score = stats[('Overall_Score', 'std')]\n",
    "            count = stats[('Overall_Score', 'count')]\n",
    "            print(f\"   {dataset_type}: {mean_score:.2f} ± {std_score:.2f} (n={count})\")\n",
    "        \n",
    "        # Overall statistics\n",
    "        print(f\"\\n OVERALL PERFORMANCE:\")\n",
    "        print(f\"   Mean Overall Score: {combined_df['Overall_Score'].mean():.2f}\")\n",
    "        print(f\"   Best Dataset: {dataset_summary[('Overall_Score', 'mean')].idxmax()}\")\n",
    "        print(f\"   Score Range: {combined_df['Overall_Score'].min():.2f} - {combined_df['Overall_Score'].max():.2f}\")\n",
    "        \n",
    "        # Processing efficiency\n",
    "        total_items = len(combined_df)\n",
    "        items_per_minute = total_items / (overall_duration / 60)\n",
    "        print(f\"\\n⚡ PROCESSING EFFICIENCY:\")\n",
    "        print(f\"   Items per minute: {items_per_minute:.1f}\")\n",
    "        print(f\"   Average time per item: {overall_duration/total_items:.2f} seconds\")\n",
    "        \n",
    "    else:\n",
    "        print(\" No datasets were successfully processed.\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EVALUATION COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_multi_dataset_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80d280b-6449-4094-9669-c1209e5e7188",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
