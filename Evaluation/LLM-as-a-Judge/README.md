## 🤖 LLM-as-a-Judge: QA Evaluation by Language Models

This module automates the evaluation of QA pairs generated by the NeuReg framework using large language models (LLMs) as strict graders. It applies a uniform evaluation prompt to five leading LLMs to assess the quality of each question–answer (QA) pair across five well-defined metrics.

----

### 📝 Evaluation Prompt

All LLMs follow this standardised scoring prompt to ensure consistency and fairness:

You are a STRICT QA evaluator following precise scoring guidelines.

You will evaluate a model-generated question-answer pair on FIVE metrics using a 1–5 scale where:
5 = Excellent  
4 = Good  
3 = Fair  
2 = Poor  
1 = Very Poor

==============================
DETAILED SCORING CRITERIA
==============================

1. **RELEVANCE (1–5):** Does the question appropriately relate to the source text?  
   5: Perfectly relevant to the source, clearly grounded in the text  
   4: Mostly relevant, with minor off-topic elements  
   3: Addresses the main question but misses some important points  
   2: Loosely related, with significant tangents or irrelevance  
   1: Entirely irrelevant or unrelated to the source content

2. **ACCURACY (1–5):** Is the answer factually correct based on the source text?  
   5: All facts are accurate and fully verifiable in the context  
   4: Mostly accurate; contains only minor factual issues  
   3: Some factual inconsistencies or assumptions  
   2: Several factual errors that affect reliability  
   1: Mostly inaccurate or misleading information

3. **COMPLETENESS (1–5):** Does the answer fully address the question?  
   5: Thorough and complete response  
   4: Covers most parts but misses minor aspects  
   3: Addresses main part, omits some key details  
   2: Partial answer with significant gaps  
   1: Severely incomplete or off-topic

4. **FLUENCY (1–5):** Is the answer well-written and grammatically correct?  
   5: Excellent grammar and clarity; highly readable  
   4: Minor grammatical or structural issues  
   3: Understandable, but contains noticeable language errors  
   2: Somewhat unclear due to poor grammar or phrasing  
   1: Difficult to read or understand

5. **KG ALIGNMENT (1–5):** How well does the answer reflect the KG triples?  
   5: Effectively uses KG relationships; no contradictions; may include additional relevant info  
   4: Uses most relevant KG information correctly; may omit minor details, no contradictions  
   3: Uses some KG information; may miss important relationships but generally consistent  
   2: Limited use of KG information; may contain minor contradictions or misinterpretations  
   1: Ignores KG information entirely or includes clear contradictions

==============================
INPUT
==============================
**Question:** {question}  
**Answer:** {answer}  
**Source Context:** {source_context}  
**Knowledge Graph Triples:** {kg_triples}

==============================
RESPONSE FORMAT
==============================
Relevance: X  
Accuracy: X  
Completeness: X  
Fluency: X  
KG_Alignment: X

Where X is a number from 1 to 5

---

### 🧪 Evaluation Metrics

Each QA pair is scored on:

Relevance – Link between question and context

Accuracy – Factual correctness of the answer

Completeness – Coverage of the expected response

Fluency – Grammar, clarity, and structure

KG Alignment – Alignment with knowledge graph triples

---

###  🧠 Models Used

The evaluation was conducted using the following five LLMs, each with its own folder:

| Model Folder                     | Description                               |
| -------------------------------- | ----------------------------------------- |
| `DeepSeek-R1-Distill-Llama-70B/` | Scored by DeepSeek R1 distilled LLaMA 70B |
| `LLaMA 3.3 70B/`                 | Meta's LLaMA 3.3 70B Instruct             |
| `Mixtral-8x22B/`                 | Mixtral 8x22B Instruct v0.1               |
| `Gemma-2 Instruct 27B/`          | Google's Gemma 2 27B Instruction-Tuned    |
| `Qwen3-32B/`                     | Qwen3-32B Instruct from Alibaba           |


Each model folder contains:

🔹 A .ipynb notebook for scoring

🔹 CSVs with results for Zero-Shot, One-Shot, and Few-Shot QA


---

###  ⚙️ API Configuration
To run the model-specific notebooks, you need valid API keys.

For Gemma-2 Instruct 27B, use Together AI:

#### Configuration - Together AI (https://api.together.ai/)
API_KEY = "your_together_ai_key"

For all other models (DeepSeek, LLaMA 3.3, Mixtral, Qwen3), use NScale:

#### Configuration - NScale (https://console.nscale.com/)
API_KEY = "your_nscale_api_key"



### 📂 Folder Structure

LLM-as-a-Judge/
│
├── README.md                         # Overview (this file)
│
├── DeepSeek-R1-Distill-Llama-70B/
│   ├── DeepSeek-R1-Distill-Llama-70B.ipynb
│   ├── DeepSeek_zeroshot_evaluation_results.csv
│   ├── DeepSeek_oneshot_evaluation_results.csv
│   └── DeepSeek_fewshot_evaluation_results.csv
│
├── LLaMA 3.3 70B/
│   ├── LLaMA_3.3_70B.ipynb
│   ├── LLaMA_zeroshot_evaluation_results.csv
│   ├── LLaMA_oneshot_evaluation_results.csv
│   └── LLaMA_fewshot_evaluation_results.csv
│
├── Mixtral-8x22B/
│   ├── Mixtral-8x22B.ipynb
│   ├── Mixtral_zeroshot_evaluation_results.csv
│   ├── Mixtral_oneshot_evaluation_results.csv
│   └── Mixtral_fewshot_evaluation_results.csv
│
├── Gemma-2 Instruct 27B/
│   ├── Gemma-2 Instruct 27B.ipynb
│   ├── Gemma_zeroshot_evaluation_results.csv
│   ├── Gemma_oneshot_evaluation_results.csv
│   └── Gemma_fewshot_evaluation_results.csv
│
└── Qwen3-32B/
    ├── Qwen3-32B.ipynb
    ├── Qwen_zeroshot_evaluation_results.csv
    ├── Qwen_oneshot_evaluation_results.csv
    └── Qwen_fewshot_evaluation_results.csv

---
   ###  📈 Output

Each model produces numeric scores (1–5) for each QA pair and metric. These results are used for:

Cross-model comparison

Aggregated analysis (see llms results analysis/)

Majority voting agreement  (see llms results analysis)

Agreement with human evaluators (see LLM vs Human/)

---

📘 For full QA generation methodology and dataset details, see the main project README.

