## ğŸ¤– LLM-as-a-Judge: QA Evaluation by Language Models

This module automates the evaluation of QA pairs generated by the NeuReg framework using large language models (LLMs) as strict graders. It applies a uniform evaluation prompt to five leading LLMs to assess the quality of each questionâ€“answer (QA) pair across five well-defined metrics.

----

### ğŸ“ Evaluation Prompt

All LLMs follow this standardised scoring prompt to ensure consistency and fairness:

You are a STRICT QA evaluator following precise scoring guidelines.

You will evaluate a model-generated question-answer pair on FIVE metrics using a 1â€“5 scale where:
5 = Excellent  
4 = Good  
3 = Fair  
2 = Poor  
1 = Very Poor

==============================
DETAILED SCORING CRITERIA
==============================

1. **RELEVANCE (1â€“5):** Does the question appropriately relate to the source text?  
   5: Perfectly relevant to the source, clearly grounded in the text  
   4: Mostly relevant, with minor off-topic elements  
   3: Addresses the main question but misses some important points  
   2: Loosely related, with significant tangents or irrelevance  
   1: Entirely irrelevant or unrelated to the source content

2. **ACCURACY (1â€“5):** Is the answer factually correct based on the source text?  
   5: All facts are accurate and fully verifiable in the context  
   4: Mostly accurate; contains only minor factual issues  
   3: Some factual inconsistencies or assumptions  
   2: Several factual errors that affect reliability  
   1: Mostly inaccurate or misleading information

3. **COMPLETENESS (1â€“5):** Does the answer fully address the question?  
   5: Thorough and complete response  
   4: Covers most parts but misses minor aspects  
   3: Addresses main part, omits some key details  
   2: Partial answer with significant gaps  
   1: Severely incomplete or off-topic

4. **FLUENCY (1â€“5):** Is the answer well-written and grammatically correct?  
   5: Excellent grammar and clarity; highly readable  
   4: Minor grammatical or structural issues  
   3: Understandable, but contains noticeable language errors  
   2: Somewhat unclear due to poor grammar or phrasing  
   1: Difficult to read or understand

5. **KG ALIGNMENT (1â€“5):** How well does the answer reflect the KG triples?  
   5: Effectively uses KG relationships; no contradictions; may include additional relevant info  
   4: Uses most relevant KG information correctly; may omit minor details, no contradictions  
   3: Uses some KG information; may miss important relationships but generally consistent  
   2: Limited use of KG information; may contain minor contradictions or misinterpretations  
   1: Ignores KG information entirely or includes clear contradictions

==============================
INPUT
==============================
**Question:** {question}  
**Answer:** {answer}  
**Source Context:** {source_context}  
**Knowledge Graph Triples:** {kg_triples}

==============================
RESPONSE FORMAT
==============================
Relevance: X  
Accuracy: X  
Completeness: X  
Fluency: X  
KG_Alignment: X

Where X is a number from 1 to 5

---

### ğŸ§ª Evaluation Metrics

Each QA pair is scored on:

Relevance â€“ Link between question and context

Accuracy â€“ Factual correctness of the answer

Completeness â€“ Coverage of the expected response

Fluency â€“ Grammar, clarity, and structure

KG Alignment â€“ Alignment with knowledge graph triples

---

###  ğŸ§  Models Used

The evaluation was conducted using the following five LLMs, each with its own folder:

| Model Folder                     | Description                               |
| -------------------------------- | ----------------------------------------- |
| `DeepSeek-R1-Distill-Llama-70B/` | Scored by DeepSeek R1 distilled LLaMA 70B |
| `LLaMA 3.3 70B/`                 | Meta's LLaMA 3.3 70B Instruct             |
| `Mixtral-8x22B/`                 | Mixtral 8x22B Instruct v0.1               |
| `Gemma-2 Instruct 27B/`          | Google's Gemma 2 27B Instruction-Tuned    |
| `Qwen3-32B/`                     | Qwen3-32B Instruct from Alibaba           |


Each model folder contains:

ğŸ”¹ A .ipynb notebook for scoring

ğŸ”¹ CSVs with results for Zero-Shot, One-Shot, and Few-Shot QA


---

###  âš™ï¸ API Configuration
To run the model-specific notebooks, you need valid API keys.

For Gemma-2 Instruct 27B, use Together AI:

#### Configuration - Together AI (https://api.together.ai/)
API_KEY = "your_together_ai_key"

For all other models (DeepSeek, LLaMA 3.3, Mixtral, Qwen3), use NScale:

#### Configuration - NScale (https://console.nscale.com/)
API_KEY = "your_nscale_api_key"



### ğŸ“‚ Folder Structure

LLM-as-a-Judge/
â”‚
â”œâ”€â”€ README.md                         # Overview (this file)
â”‚
â”œâ”€â”€ DeepSeek-R1-Distill-Llama-70B/
â”‚   â”œâ”€â”€ DeepSeek-R1-Distill-Llama-70B.ipynb
â”‚   â”œâ”€â”€ DeepSeek_zeroshot_evaluation_results.csv
â”‚   â”œâ”€â”€ DeepSeek_oneshot_evaluation_results.csv
â”‚   â””â”€â”€ DeepSeek_fewshot_evaluation_results.csv
â”‚
â”œâ”€â”€ LLaMA 3.3 70B/
â”‚   â”œâ”€â”€ LLaMA_3.3_70B.ipynb
â”‚   â”œâ”€â”€ LLaMA_zeroshot_evaluation_results.csv
â”‚   â”œâ”€â”€ LLaMA_oneshot_evaluation_results.csv
â”‚   â””â”€â”€ LLaMA_fewshot_evaluation_results.csv
â”‚
â”œâ”€â”€ Mixtral-8x22B/
â”‚   â”œâ”€â”€ Mixtral-8x22B.ipynb
â”‚   â”œâ”€â”€ Mixtral_zeroshot_evaluation_results.csv
â”‚   â”œâ”€â”€ Mixtral_oneshot_evaluation_results.csv
â”‚   â””â”€â”€ Mixtral_fewshot_evaluation_results.csv
â”‚
â”œâ”€â”€ Gemma-2 Instruct 27B/
â”‚   â”œâ”€â”€ Gemma-2 Instruct 27B.ipynb
â”‚   â”œâ”€â”€ Gemma_zeroshot_evaluation_results.csv
â”‚   â”œâ”€â”€ Gemma_oneshot_evaluation_results.csv
â”‚   â””â”€â”€ Gemma_fewshot_evaluation_results.csv
â”‚
â””â”€â”€ Qwen3-32B/
    â”œâ”€â”€ Qwen3-32B.ipynb
    â”œâ”€â”€ Qwen_zeroshot_evaluation_results.csv
    â”œâ”€â”€ Qwen_oneshot_evaluation_results.csv
    â””â”€â”€ Qwen_fewshot_evaluation_results.csv

---
   ###  ğŸ“ˆ Output

Each model produces numeric scores (1â€“5) for each QA pair and metric. These results are used for:

Cross-model comparison

Aggregated analysis (see llms results analysis/)

Majority voting agreement  (see llms results analysis)

Agreement with human evaluators (see LLM vs Human/)

---

ğŸ“˜ For full QA generation methodology and dataset details, see the main project README.

